{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Plan -> Do -> Verify')\n",
        "print('Plan: Configure inputs, harmonize geometries, bin tweets temporally, fuse multi-scale signals, and build rasters.')\n",
        "print('Do: Each section ingests data, constructs grids, fuses signals, assembles time-aware arrays, and writes exports.')\n",
        "print('Verify: Assertions validate schemas, CRS alignment, time coverage, and quick-look diagnostics.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tweet Project \u2014 Time-Aware Raster Builder\n",
        "\n",
        "This notebook constructs a fused spatiotemporal raster of tweet intensity by combining evidence from state, county, and city signals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    'event': 'francine',             # Options: 'francine', 'helene'\n",
        "    'cell_size_km': 25,              # Spatial resolution of output cells (kilometers)\n",
        "    'time_bin_hours': 6,             # Temporal aggregation window\n",
        "    'layer_weights': {               # Relative influence of each administrative layer\n",
        "        'state': 0.5,\n",
        "        'county': 0.35,\n",
        "        'city': 0.15\n",
        "    }\n",
        "}\n",
        "\n",
        "MAX_CITY_DISTANCE_KM = 50  # upper bound for associating tweets with a nearby city\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from datetime import timedelta\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import xarray as xr\n",
        "from shapely.geometry import Point\n",
        "from shapely.ops import unary_union\n",
        "from shapely.errors import TopologicalError\n",
        "from pyproj import CRS\n",
        "import rasterio\n",
        "from rasterio import features\n",
        "from rasterio.enums import MergeAlg\n",
        "from rasterio.transform import from_origin\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "DATA_ROOT = Path('data')\n",
        "OUTPUT_ROOT = Path('rasters_output') / 'fused_time_intensity'\n",
        "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EVENT_PATHS = {\n",
        "    'francine': DATA_ROOT / 'geojson' / 'francine.geojson',\n",
        "    'helene': DATA_ROOT / 'geojson' / 'helene.geojson'\n",
        "}\n",
        "\n",
        "SHAPE_PATHS = {\n",
        "    'states': DATA_ROOT / 'shape_files' / 'cb_2023_us_state_20m.shp',\n",
        "    'counties': DATA_ROOT / 'shape_files' / 'cb_2023_us_county_20m.shp'\n",
        "}\n",
        "\n",
        "CITIES_PATH = DATA_ROOT / 'tables' / 'cities1000.csv'\n",
        "TARGET_CRS = CRS.from_epsg(5070)  # NAD83 / Conus Albers \u2014 equal-area for conterminous US\n",
        "\n",
        "assert CONFIG['event'].lower() in EVENT_PATHS, 'Config event must be francine or helene.'\n",
        "for key, path in {**EVENT_PATHS, **SHAPE_PATHS, 'cities': CITIES_PATH}.items():\n",
        "    assert Path(path).exists(), f'Missing input: {path}'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data ingestion and harmonization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "event_key = CONFIG['event'].lower()\n",
        "raw_tweets = gpd.read_file(EVENT_PATHS[event_key])\n",
        "states = gpd.read_file(SHAPE_PATHS['states'])\n",
        "counties = gpd.read_file(SHAPE_PATHS['counties'])\n",
        "cities = pd.read_csv(CITIES_PATH)\n",
        "\n",
        "assert {'time', 'geometry'}.issubset(raw_tweets.columns), 'Tweet GeoJSON missing required columns.'\n",
        "assert 'geometry' in states.columns and 'geometry' in counties.columns, 'Shapefiles lack geometry.'\n",
        "assert {'latitude', 'longitude', 'country_code', 'geonameid', 'name'}.issubset(cities.columns), 'City table missing expected columns.'\n",
        "\n",
        "raw_tweets['timestamp'] = pd.to_datetime(raw_tweets['time'], errors='coerce', utc=True)\n",
        "valid_tweets = raw_tweets.dropna(subset=['timestamp']).copy()\n",
        "valid_tweets = valid_tweets[~valid_tweets.geometry.is_empty].copy()\n",
        "\n",
        "if valid_tweets.empty:\n",
        "    raise ValueError('No tweets with valid timestamps and geometries after preprocessing.')\n",
        "\n",
        "us_cities = cities.loc[cities['country_code'] == 'US'].copy()\n",
        "us_cities = us_cities.drop_duplicates(subset=['geonameid'])\n",
        "city_geometries = gpd.GeoDataFrame(us_cities, geometry=gpd.points_from_xy(us_cities['longitude'], us_cities['latitude']), crs='EPSG:4326')\n",
        "\n",
        "valid_tweets = valid_tweets.to_crs(TARGET_CRS)\n",
        "states = states.to_crs(TARGET_CRS)\n",
        "counties = counties.to_crs(TARGET_CRS)\n",
        "city_geometries = city_geometries.to_crs(TARGET_CRS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Administrative enrichment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets = valid_tweets.copy()\n",
        "\n",
        "state_cols = ['STATEFP', 'NAME', 'GEOID']\n",
        "county_cols = ['STATEFP', 'COUNTYFP', 'GEOID', 'NAME']\n",
        "\n",
        "state_join = gpd.sjoin(tweets, states[state_cols + ['geometry']], how='left', predicate='within')\n",
        "tweets['state_fips'] = state_join['STATEFP'].values\n",
        "tweets['state_name'] = state_join['NAME'].values\n",
        "tweets['state_geoid'] = state_join['GEOID'].values\n",
        "\n",
        "county_join = gpd.sjoin(tweets, counties[county_cols + ['geometry']], how='left', predicate='within')\n",
        "tweets['county_fips'] = county_join['COUNTYFP'].values\n",
        "tweets['county_name'] = county_join['NAME'].values\n",
        "tweets['county_geoid'] = county_join['GEOID'].values\n",
        "\n",
        "city_join = gpd.sjoin_nearest(\n",
        "    tweets,\n",
        "    city_geometries[['geonameid', 'name', 'geometry']],\n",
        "    how='left',\n",
        "    distance_col='city_distance_m',\n",
        "    max_distance=CONFIG['cell_size_km'] * 1000 * 2\n",
        ")\n",
        "\n",
        "tweets['city_geonameid'] = city_join['geonameid'].values\n",
        "tweets['city_name'] = city_join['name'].values\n",
        "tweets['city_distance_m'] = city_join['city_distance_m'].values\n",
        "\n",
        "max_city_distance_m = MAX_CITY_DISTANCE_KM * 1000\n",
        "far_city_mask = tweets['city_distance_m'].notna() & (tweets['city_distance_m'] > max_city_distance_m)\n",
        "tweets.loc[far_city_mask, ['city_geonameid', 'city_name', 'city_distance_m']] = np.nan\n",
        "\n",
        "tweets = tweets.sort_values('timestamp').drop_duplicates(subset=['timestamp', 'geometry'])\n",
        "\n",
        "assert not tweets.empty, 'All tweets dropped during administrative enrichment.'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal binning and spatial grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_bin_hours = CONFIG['time_bin_hours']\n",
        "cell_size_m = CONFIG['cell_size_km'] * 1000\n",
        "\n",
        "start_time = tweets['timestamp'].min().floor(f'{time_bin_hours}H')\n",
        "end_time = tweets['timestamp'].max().ceil(f'{time_bin_hours}H')\n",
        "\n",
        "if start_time == end_time:\n",
        "    end_time = end_time + pd.Timedelta(hours=time_bin_hours)\n",
        "\n",
        "bin_edges = pd.date_range(start=start_time, end=end_time, freq=f'{time_bin_hours}H', tz='UTC')\n",
        "if len(bin_edges) < 2:\n",
        "    bin_edges = bin_edges.append(bin_edges[-1:] + pd.Timedelta(hours=time_bin_hours))\n",
        "\n",
        "relevant_counties = counties[counties.geometry.intersects(tweets.unary_union.buffer(cell_size_m))]\n",
        "if relevant_counties.empty:\n",
        "    relevant_counties = counties\n",
        "\n",
        "processing_extent = unary_union(relevant_counties.geometry).buffer(cell_size_m)\n",
        "\n",
        "xmin, ymin, xmax, ymax = processing_extent.bounds\n",
        "width = int(math.ceil((xmax - xmin) / cell_size_m))\n",
        "height = int(math.ceil((ymax - ymin) / cell_size_m))\n",
        "\n",
        "xmax = xmin + width * cell_size_m\n",
        "ymin = ymax - height * cell_size_m\n",
        "transform = from_origin(xmin, ymax, cell_size_m, cell_size_m)\n",
        "\n",
        "x_coords = xmin + cell_size_m * (0.5 + np.arange(width))\n",
        "y_coords = ymax - cell_size_m * (0.5 + np.arange(height))\n",
        "\n",
        "cell_area_m2 = cell_size_m ** 2\n",
        "\n",
        "assert width > 0 and height > 0, 'Invalid raster grid dimensions.'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fusion helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rasterize_polygons(gdf: gpd.GeoDataFrame, value_col: str) -> np.ndarray:\n",
        "    if gdf.empty:\n",
        "        return np.zeros((height, width), dtype='float32')\n",
        "    shapes = ((geom, val) for geom, val in zip(gdf.geometry, gdf[value_col]))\n",
        "    return features.rasterize(\n",
        "        shapes=shapes,\n",
        "        out_shape=(height, width),\n",
        "        transform=transform,\n",
        "        fill=0.0,\n",
        "        dtype='float32',\n",
        "        merge_alg=MergeAlg.add\n",
        "    )\n",
        "\n",
        "\n",
        "def rasterize_points(gdf: gpd.GeoDataFrame, value_col: str, influence_radius: float) -> np.ndarray:\n",
        "    if gdf.empty:\n",
        "        return np.zeros((height, width), dtype='float32')\n",
        "    buffered = gdf.copy()\n",
        "    buffered['geometry'] = buffered.geometry.buffer(influence_radius, cap_style=1)\n",
        "    return rasterize_polygons(buffered, value_col)\n",
        "\n",
        "\n",
        "def weight_normalizer(weights: dict) -> dict:\n",
        "    total = sum(weights.values())\n",
        "    if total == 0:\n",
        "        raise ValueError('Layer weights cannot all be zero.')\n",
        "    return {k: v / total for k, v in weights.items()}\n",
        "\n",
        "layer_weights = weight_normalizer(CONFIG['layer_weights'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time-slice assembly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iterative_arrays = []\n",
        "cumulative_arrays = []\n",
        "accumulator = np.zeros((height, width), dtype='float32')\n",
        "\n",
        "iterative_stats = []\n",
        "time_labels = []\n",
        "\n",
        "state_area = states.set_index('GEOID').geometry.area\n",
        "county_area = counties.set_index('GEOID').geometry.area\n",
        "city_buffer_radius = cell_size_m / 2\n",
        "\n",
        "for start, stop in zip(bin_edges[:-1], bin_edges[1:]):\n",
        "    mask = (tweets['timestamp'] >= start) & (tweets['timestamp'] < stop)\n",
        "    bin_tweets = tweets.loc[mask]\n",
        "    time_labels.append(start)\n",
        "    bin_summary = {\n",
        "        'start': start.isoformat(),\n",
        "        'end': stop.isoformat(),\n",
        "        'tweet_count': int(len(bin_tweets))\n",
        "    }\n",
        "\n",
        "    if bin_tweets.empty:\n",
        "        fused = np.zeros((height, width), dtype='float32')\n",
        "        iterative_arrays.append(fused)\n",
        "        accumulator = accumulator + fused\n",
        "        cumulative_arrays.append(accumulator.copy())\n",
        "        iterative_stats.append({**bin_summary, 'state_weighted': 0.0, 'county_weighted': 0.0, 'city_weighted': 0.0})\n",
        "        continue\n",
        "\n",
        "    state_counts = bin_tweets.groupby('state_geoid').size()\n",
        "    state_density = state_counts / state_area.reindex(state_counts.index)\n",
        "    state_layer = states.loc[states['GEOID'].isin(state_counts.index)].copy()\n",
        "    state_layer['density'] = state_layer['GEOID'].map(state_density.fillna(0))\n",
        "    state_raster = rasterize_polygons(state_layer, 'density') * cell_area_m2\n",
        "\n",
        "    county_counts = bin_tweets.groupby('county_geoid').size()\n",
        "    county_density = county_counts / county_area.reindex(county_counts.index)\n",
        "    county_layer = counties.loc[counties['GEOID'].isin(county_counts.index)].copy()\n",
        "    county_layer['density'] = county_layer['GEOID'].map(county_density.fillna(0))\n",
        "    county_raster = rasterize_polygons(county_layer, 'density') * cell_area_m2\n",
        "\n",
        "    city_counts = bin_tweets.groupby('city_geonameid').size().dropna()\n",
        "    if not city_counts.empty:\n",
        "        city_layer = city_geometries.loc[city_geometries['geonameid'].isin(city_counts.index)].copy()\n",
        "        city_layer['intensity'] = city_layer['geonameid'].map(city_counts).fillna(0)\n",
        "        city_raster = rasterize_points(city_layer, 'intensity', city_buffer_radius)\n",
        "    else:\n",
        "        city_raster = np.zeros((height, width), dtype='float32')\n",
        "\n",
        "    fused = (layer_weights['state'] * state_raster +\n",
        "             layer_weights['county'] * county_raster +\n",
        "             layer_weights['city'] * city_raster).astype('float32')\n",
        "\n",
        "    iterative_arrays.append(fused)\n",
        "    accumulator = accumulator + fused\n",
        "    cumulative_arrays.append(accumulator.copy())\n",
        "\n",
        "    iterative_stats.append({**bin_summary,\n",
        "                           'state_weighted': float(state_raster.sum() * layer_weights['state']),\n",
        "                           'county_weighted': float(county_raster.sum() * layer_weights['county']),\n",
        "                           'city_weighted': float(city_raster.sum() * layer_weights['city'])})\n",
        "\n",
        "assert len(iterative_arrays) == len(bin_edges) - 1, 'Mismatch between bins and raster slices.'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build multidimensional rasters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_index = pd.DatetimeIndex(time_labels).tz_convert('UTC').tz_localize(None)\n",
        "iterative_stack = np.stack(iterative_arrays)\n",
        "cumulative_stack = np.stack(cumulative_arrays)\n",
        "\n",
        "iterative_da = xr.DataArray(\n",
        "    iterative_stack,\n",
        "    coords={'time': time_index, 'y': y_coords, 'x': x_coords},\n",
        "    dims=('time', 'y', 'x'),\n",
        "    name='tweet_intensity'\n",
        ")\n",
        "\n",
        "cumulative_da = xr.DataArray(\n",
        "    cumulative_stack,\n",
        "    coords={'time': time_index, 'y': y_coords, 'x': x_coords},\n",
        "    dims=('time', 'y', 'x'),\n",
        "    name='tweet_intensity_cumulative'\n",
        ")\n",
        "\n",
        "attrs = {\n",
        "    'description': 'Weighted fusion of state, county, and city tweet signals per time slice.',\n",
        "    'event': event_key,\n",
        "    'cell_size_m': cell_size_m,\n",
        "    'crs_wkt': TARGET_CRS.to_wkt(),\n",
        "    'weights': json.dumps(layer_weights)\n",
        "}\n",
        "iterative_da.attrs.update(attrs)\n",
        "cumulative_da.attrs.update(attrs)\n",
        "\n",
        "iterative_da.time.attrs['long_name'] = 'Start of aggregation interval (UTC)'\n",
        "cumulative_da.time.attrs['long_name'] = 'Start of aggregation interval (UTC)'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick-look diagnostic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_idx = int(np.argmax([arr.sum() for arr in iterative_arrays])) if iterative_arrays else 0\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "if iterative_arrays:\n",
        "    im = ax[0].imshow(iterative_arrays[sample_idx], origin='upper', cmap='viridis')\n",
        "    ax[0].set_title(f'Iterative slice #{sample_idx}')\n",
        "    plt.colorbar(im, ax=ax[0], fraction=0.046, pad=0.04)\n",
        "    im2 = ax[1].imshow(cumulative_arrays[sample_idx], origin='upper', cmap='magma')\n",
        "    ax[1].set_title(f'Cumulative slice #{sample_idx}')\n",
        "    plt.colorbar(im2, ax=ax[1], fraction=0.046, pad=0.04)\n",
        "else:\n",
        "    ax[0].text(0.5, 0.5, 'No data', ha='center', va='center')\n",
        "    ax[1].text(0.5, 0.5, 'No data', ha='center', va='center')\n",
        "for axis in ax:\n",
        "    axis.set_xlabel('x pixels')\n",
        "    axis.set_ylabel('y pixels')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export artifacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_name = f\"{event_key}_{int(CONFIG['cell_size_km'])}km_{int(time_bin_hours)}h\"\n",
        "iterative_path = OUTPUT_ROOT / f\"{base_name}_iterative.nc\"\n",
        "cumulative_path = OUTPUT_ROOT / f\"{base_name}_cumulative.nc\"\n",
        "per_slice_dir = OUTPUT_ROOT / f\"{base_name}_per_slice\"\n",
        "per_slice_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "summary_payload = {\n",
        "    'event': event_key,\n",
        "    'time_slices': len(iterative_arrays),\n",
        "    'grid': {'height': height, 'width': width, 'cell_size_m': cell_size_m},\n",
        "    'weights': layer_weights,\n",
        "    'time_start': time_index.min().isoformat() if len(time_index) else None,\n",
        "    'time_end': time_index.max().isoformat() if len(time_index) else None,\n",
        "    'iterative_path': str(iterative_path),\n",
        "    'cumulative_path': str(cumulative_path),\n",
        "    'per_slice_directory': str(per_slice_dir),\n",
        "    'bin_summaries': iterative_stats\n",
        "}\n",
        "print(json.dumps(summary_payload, default=str, separators=(',', ':')))\n",
        "\n",
        "iterative_da.to_netcdf(iterative_path)\n",
        "cumulative_da.to_netcdf(cumulative_path)\n",
        "\n",
        "profile = {\n",
        "    'driver': 'GTiff',\n",
        "    'height': height,\n",
        "    'width': width,\n",
        "    'count': 1,\n",
        "    'dtype': 'float32',\n",
        "    'crs': TARGET_CRS.to_wkt(),\n",
        "    'transform': transform,\n",
        "    'compress': 'lzw'\n",
        "}\n",
        "for label, arr in zip(time_index, iterative_arrays):\n",
        "    slice_path = per_slice_dir / f\"{base_name}_{label.isoformat().replace(':', '')}.tif\"\n",
        "    with rasterio.open(slice_path, 'w', **profile) as dst:\n",
        "        dst.write(arr, 1)\n",
        "        dst.update_tags(start_time=label.isoformat() + 'Z',\n",
        "                        end_time=(label + pd.Timedelta(hours=time_bin_hours)).isoformat() + 'Z',\n",
        "                        layer_weights=json.dumps(layer_weights))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quality checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert iterative_da.notnull().any(), 'Iterative raster contains only nulls.'\n",
        "assert cumulative_da.notnull().any(), 'Cumulative raster contains only nulls.'\n",
        "assert iterative_da.time.size == len(iterative_arrays), 'Mismatch between time coordinate and arrays.'\n",
        "assert iterative_da.attrs['event'] == event_key, 'Event label mismatch in metadata.'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## README\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Inputs consumed**\n",
        "\n",
        "- `data/tables/cities1000.csv` (unchanged schema)\n",
        "- `data/shape_files/cb_2023_us_state_20m.shp`\n",
        "- `data/shape_files/cb_2023_us_county_20m.shp`\n",
        "- `data/geojson/{francine|helene}.geojson` based on the configuration\n",
        "\n",
        "**Fusion logic**\n",
        "\n",
        "Tweets inherit three spatial signals: (1) state polygons and (2) county polygons identified by containment, and (3) nearest cities within 50 km. For every time bin the notebook converts counts within those units to area-normalized densities, rasterizes each layer at the chosen resolution, buffers city points by half a cell to localize their influence, then combines the layers through the configurable weights. This balances broad coverage with localized emphasis without relying on a pre-existing named recipe.\n",
        "\n",
        "**Configuration applied in this run**\n",
        "\n",
        "```json\n",
        "{\"event\": \"francine\", \"cell_size_km\": 25, \"time_bin_hours\": 6, \"layer_weights\": {\"state\": 0.5, \"county\": 0.35, \"city\": 0.15}}\n",
        "```\n",
        "\n",
        "**CRS**\n",
        "\n",
        "All data are projected to EPSG:5070 (NAD83 / Conus Albers) to preserve area when distributing intensity; the CRS choice aligns with the predominantly continental US extent and produces meter-based cell sizes.\n",
        "\n",
        "**ArcGIS usage**\n",
        "\n",
        "1. In ArcGIS Pro, add the generated NetCDF (`*_iterative.nc`) as a multidimensional raster layer (Insert \u2192 Multidimensional Raster Layer).\n",
        "2. Ensure the `time` dimension is recognized; ArcGIS automatically reads the `time` coordinate as start times per slice.\n",
        "3. Optionally load the cumulative NetCDF or per-slice GeoTIFFs for validation.\n",
        "4. Enable time on the layer to scrub through the fused intensity across intervals.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}