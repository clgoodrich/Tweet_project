{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install fuzzywuzzy python-Levenshtein geopandas pandas numpy matplotlib",
   "id": "2f073628f0afb875"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Load GeoJSON files\n",
    "local_path = os.path.dirname(os.getcwd())\n",
    "# local_path = os.path.join(os.getcwd(),r'GitHub\\Tweet_project')\n",
    "# Correctly join the paths without a leading slash\n",
    "# The 'r' prefix is not necessary here, but it doesn't hurt\n",
    "francine_dir = r\"\\data\\geojson\\francine.geojson\"\n",
    "helene_dir = r\"\\data\\geojson\\helene.geojson\"\n",
    "francine_path = f\"{local_path}{francine_dir}\"\n",
    "helene_path = f\"{local_path}{helene_dir}\"\n",
    "# francine_path = os.path.join(local_path, '\\data\\geojson', 'francine.geojson')\n",
    "# helene_path = os.path.join(local_path, '\\data\\geojson', 'helene.geojson')\n",
    "print(francine_path)\n",
    "\n",
    "# francine_path = os.path.join(os.getcwd(), r'data\\geojson\\francine.geojson')\n",
    "# helene_path = os.path.join(os.getcwd(), r'data\\geojson\\helene.geojson')\n",
    "\n",
    "\n",
    "# francine_path = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\data\\geojson\\francine.geojson\"\n",
    "# helene_path = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\data\\geojson\\helene.geojson\"\n",
    "\n",
    "# Load into GeoDataFrames\n",
    "francine_gdf = gpd.read_file(francine_path)\n",
    "helene_gdf = gpd.read_file(helene_path)\n",
    "\n",
    "# Standardize timestamps to UTC\n",
    "francine_gdf['timestamp'] = pd.to_datetime(francine_gdf['time'], utc=True)\n",
    "# print(francine_gdf['time'])\n",
    "helene_gdf['timestamp'] = pd.to_datetime(helene_gdf['time'], utc=True)\n",
    "\n",
    "\n",
    "# Floor to 4-hour bins\n",
    "francine_gdf['time_bin'] = francine_gdf['timestamp'].dt.floor('4h')\n",
    "helene_gdf['time_bin'] = helene_gdf['timestamp'].dt.floor('4h')\n",
    "all_data = francine_gdf['time_bin'].unique()\n",
    "francine_gdf['unix_timestamp'] = francine_gdf['time_bin'].astype('int64') // 1000\n",
    "helene_gdf['unix_timestamp'] = helene_gdf['time_bin'].astype('int64') // 1000\n",
    "helene_timestamp_dict = dict(zip(helene_gdf['unix_timestamp'], helene_gdf['time_bin']))\n",
    "francine_timestamp_dict = dict(zip(francine_gdf['unix_timestamp'], francine_gdf['time_bin']))\n",
    "\n",
    "# Create readable bin labels for file naming\n",
    "francine_gdf['bin_label'] = francine_gdf['time_bin'].dt.strftime('%Y%m%d_%H%M')\n",
    "helene_gdf['bin_label'] = helene_gdf['time_bin'].dt.strftime('%Y%m%d_%H%M')\n",
    "# Display summary\n",
    "# print(\"FRANCINE Dataset:\")\n",
    "# print(f\"  Total tweets: {len(francine_gdf)}\")\n",
    "# print(f\"  Time range: {francine_gdf['time_bin'].min()} to {francine_gdf['time_bin'].max()}\")\n",
    "# print(f\"  Number of 4-hour bins: {francine_gdf['time_bin'].nunique()}\")\n",
    "# print(f\"\\nHELENE Dataset:\")\n",
    "# print(f\"  Total tweets: {len(helene_gdf)}\")\n",
    "# print(f\"  Time range: {helene_gdf['time_bin'].min()} to {helene_gdf['time_bin'].max()}\")\n",
    "# print(f\"  Number of 4-hour bins: {helene_gdf['time_bin'].nunique()}\")"
   ],
   "id": "791276bee3e4ebb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load reference shapefiles\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "\n",
    "# states_path = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\data\\shape_files\\cb_2023_us_state_20m.shp\"\n",
    "# counties_path = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\data\\shape_files\\cb_2023_us_county_20m.shp\"\n",
    "# cities_path = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\data\\shape_files\\US_Cities.shp\"\n",
    "states_dir = r\"\\data\\shape_files\\cb_2023_us_state_20m.shp\"\n",
    "counties_dir = r\"\\data\\shape_files\\cb_2023_us_county_20m.shp\"\n",
    "cities_dir = r\"\\data\\shape_files\\US_Cities.shp\"\n",
    "states_path = f\"{local_path}{states_dir}\"\n",
    "counties_path = f\"{local_path}{counties_dir}\"\n",
    "cities_path = f\"{local_path}{cities_dir}\"\n",
    "\n",
    "#\n",
    "# states_path = os.path.join(local_path, '\\data\\shape_files', 'cb_2023_us_state_20m.shp')\n",
    "# counties_path = os.path.join(local_path, '\\data\\shape_files', 'cb_2023_us_county_20m.shp')\n",
    "# cities_path = os.path.join(local_path, '\\data\\shape_files', 'US_Cities.shp')\n",
    "\n",
    "# Load spatial reference data\n",
    "states_gdf = gpd.read_file(states_path)\n",
    "counties_gdf = gpd.read_file(counties_path)\n",
    "cities_gdf = gpd.read_file(cities_path)\n",
    "\n",
    "# PLACE THIS CODE AFTER LOADING SHAPEFILES BUT BEFORE CREATING SIMPLE LOOKUPS\n",
    "# =============================================================================\n",
    "# MULTI-LEVEL GEOGRAPHIC MATCHING SYSTEM (ALL LEVELS)\n",
    "# =============================================================================\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "\n",
    "def preprocess_place_name(name):\n",
    "    \"\"\"Standardize place names for better matching\"\"\"\n",
    "    if pd.isna(name) or name == 'NAN':\n",
    "        return None\n",
    "\n",
    "    name = str(name).upper().strip()\n",
    "\n",
    "    # Common abbreviation standardizations\n",
    "    name = re.sub(r'\\bST\\.?\\b', 'SAINT', name)  # St. -> Saint\n",
    "    name = re.sub(r'\\bMT\\.?\\b', 'MOUNT', name)  # Mt. -> Mount\n",
    "    name = re.sub(r'\\bFT\\.?\\b', 'FORT', name)   # Ft. -> Fort\n",
    "    name = re.sub(r'\\bN\\.?\\b', 'NORTH', name)   # N. -> North\n",
    "    name = re.sub(r'\\bS\\.?\\b', 'SOUTH', name)   # S. -> South\n",
    "    name = re.sub(r'\\bE\\.?\\b', 'EAST', name)    # E. -> East\n",
    "    name = re.sub(r'\\bW\\.?\\b', 'WEST', name)    # W. -> West\n",
    "\n",
    "    # Remove extra spaces and punctuation\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation\n",
    "    name = re.sub(r'\\s+', ' ', name)     # Normalize spaces\n",
    "\n",
    "    return name.strip()\n",
    "\n",
    "def parse_gpe_entities(gpe_string):\n",
    "    \"\"\"Parse GPE string into multiple potential geographic entities\"\"\"\n",
    "    if not gpe_string or pd.isna(gpe_string) or str(gpe_string).strip() == '':\n",
    "        return []\n",
    "\n",
    "    gpe_string = str(gpe_string).strip()\n",
    "\n",
    "    # Split by common separators\n",
    "    entities = []\n",
    "\n",
    "    # Primary split by comma\n",
    "    parts = [part.strip() for part in gpe_string.split(',')]\n",
    "\n",
    "    for part in parts:\n",
    "        if part:\n",
    "            # Further split by other separators\n",
    "            sub_parts = re.split(r'[;&|]', part)\n",
    "            for sub_part in sub_parts:\n",
    "                sub_part = sub_part.strip()\n",
    "                if sub_part and len(sub_part) > 1:  # Ignore single characters\n",
    "                    entities.append(preprocess_place_name(sub_part))\n",
    "\n",
    "    # Remove None values and duplicates while preserving order\n",
    "    clean_entities = []\n",
    "    seen = set()\n",
    "    for entity in entities:\n",
    "        if entity and entity not in seen:\n",
    "            clean_entities.append(entity)\n",
    "            seen.add(entity)\n",
    "\n",
    "    return clean_entities\n",
    "\n",
    "def create_hierarchical_lookups(states_gdf, counties_gdf, cities_gdf):\n",
    "    \"\"\"Create hierarchical lookup dictionaries for fuzzy matching\"\"\"\n",
    "    print(\"\\nCreating hierarchical lookup dictionaries...\")\n",
    "\n",
    "    # 1. States - simple lookup with preprocessed names + abbreviations\n",
    "    state_lookup = {}\n",
    "    state_abbrev_to_name = {}  # Abbreviation to full name\n",
    "    state_name_to_abbrev = {}  # Full name to abbreviation\n",
    "\n",
    "    for idx, row in states_gdf.iterrows():\n",
    "        state_name = preprocess_place_name(row['NAME'])\n",
    "        if state_name:\n",
    "            state_lookup[state_name] = row.geometry\n",
    "            # Handle abbreviations if available\n",
    "            if 'STUSPS' in row:\n",
    "                abbrev = row['STUSPS'].upper()\n",
    "                state_abbrev_to_name[abbrev] = state_name\n",
    "                state_name_to_abbrev[state_name] = abbrev\n",
    "                # Also add abbreviation as a lookup option\n",
    "                state_lookup[abbrev] = row.geometry\n",
    "\n",
    "    # 2. Counties - organized by state\n",
    "    county_by_state = {}\n",
    "    county_lookup = {}\n",
    "\n",
    "    for idx, row in counties_gdf.iterrows():\n",
    "        county_name = preprocess_place_name(row['NAME'])\n",
    "        state_fips = row.get('STATEFP', '')\n",
    "\n",
    "        if county_name:\n",
    "            county_lookup[county_name] = row.geometry\n",
    "\n",
    "            # Try to get state name from STATEFP or other fields\n",
    "            state_name = None\n",
    "            if 'STATE_NAME' in row:\n",
    "                state_name = preprocess_place_name(row['STATE_NAME'])\n",
    "            else:\n",
    "                # Try to find state by FIPS code\n",
    "                for s_idx, s_row in states_gdf.iterrows():\n",
    "                    if s_row.get('STATEFP', '') == state_fips:\n",
    "                        state_name = preprocess_place_name(s_row['NAME'])\n",
    "                        break\n",
    "\n",
    "            if state_name:\n",
    "                if state_name not in county_by_state:\n",
    "                    county_by_state[state_name] = {}\n",
    "                county_by_state[state_name][county_name] = row.geometry\n",
    "\n",
    "    # 3. Cities - organized by state\n",
    "    city_by_state = {}\n",
    "    city_lookup = {}\n",
    "\n",
    "    for idx, row in cities_gdf.iterrows():\n",
    "        city_name = preprocess_place_name(row['NAME'])\n",
    "        state_abbrev = row.get('ST', '').upper()\n",
    "\n",
    "        if city_name:\n",
    "            city_lookup[city_name] = row.geometry\n",
    "\n",
    "            # Convert state abbreviation to full name\n",
    "            if state_abbrev in state_abbrev_to_name:\n",
    "                state_full = state_abbrev_to_name[state_abbrev]\n",
    "                if state_full not in city_by_state:\n",
    "                    city_by_state[state_full] = {}\n",
    "                city_by_state[state_full][city_name] = row.geometry\n",
    "    #\n",
    "    # print(f\"  States: {len(state_lookup)} (including abbreviations)\")\n",
    "    # print(f\"  Counties: {len(county_lookup)} (organized by {len(county_by_state)} states)\")\n",
    "    # print(f\"  Cities: {len(city_lookup)} (organized by {len(city_by_state)} states)\")\n",
    "\n",
    "    return {\n",
    "        'state_lookup': state_lookup,\n",
    "        'county_lookup': county_lookup,\n",
    "        'city_lookup': city_lookup,\n",
    "        'county_by_state': county_by_state,\n",
    "        'city_by_state': city_by_state,\n",
    "        'state_abbrev_to_name': state_abbrev_to_name,\n",
    "        'state_name_to_abbrev': state_name_to_abbrev\n",
    "    }\n",
    "\n",
    "def fuzzy_match_entity(entity, candidates, threshold=75):\n",
    "    \"\"\"Fuzzy match an entity against candidates\"\"\"\n",
    "    if not entity or not candidates:\n",
    "        return None, 0\n",
    "\n",
    "    # Try exact match first\n",
    "    if entity in candidates:\n",
    "        return entity, 100\n",
    "\n",
    "    # Use fuzzy matching\n",
    "    match = process.extractOne(entity, candidates.keys(), scorer=fuzz.ratio)\n",
    "\n",
    "    if match and match[1] >= threshold:\n",
    "        return match[0], match[1]\n",
    "\n",
    "    return None, 0\n",
    "\n",
    "def find_all_geographic_matches(entities, lookups):\n",
    "    \"\"\"Find ALL geographic matches (state, county, city) for the entities\"\"\"\n",
    "    if not entities:\n",
    "        return []\n",
    "\n",
    "    state_lookup = lookups['state_lookup']\n",
    "    county_lookup = lookups['county_lookup']\n",
    "    city_lookup = lookups['city_lookup']\n",
    "    county_by_state = lookups['county_by_state']\n",
    "    city_by_state = lookups['city_by_state']\n",
    "\n",
    "    # Store all successful matches\n",
    "    all_matches = []\n",
    "\n",
    "    # Context tracking for better matching\n",
    "    found_states = set()\n",
    "\n",
    "    # STEP 1: Find all state matches first\n",
    "    for entity in entities:\n",
    "        state_match, state_score = fuzzy_match_entity(entity, state_lookup, threshold=75)\n",
    "        if state_match:\n",
    "            all_matches.append(('STATE', state_match, state_lookup[state_match], state_score))\n",
    "            found_states.add(state_match)\n",
    "\n",
    "    # STEP 2: Find county matches (global first, then state-specific)\n",
    "    for entity in entities:\n",
    "        # Global county search\n",
    "        county_match, county_score = fuzzy_match_entity(entity, county_lookup, threshold=75)\n",
    "        if county_match:\n",
    "            all_matches.append(('COUNTY', county_match, county_lookup[county_match], county_score))\n",
    "\n",
    "        # State-specific county search (higher accuracy)\n",
    "        for state_name in found_states:\n",
    "            if state_name in county_by_state:\n",
    "                state_counties = county_by_state[state_name]\n",
    "                state_county_match, state_county_score = fuzzy_match_entity(entity, state_counties, threshold=70)\n",
    "                if state_county_match and state_county_score > county_score:\n",
    "                    # Replace with better state-specific match\n",
    "                    # Remove the global match if it exists\n",
    "                    all_matches = [m for m in all_matches if not (m[0] == 'COUNTY' and m[1] == county_match)]\n",
    "                    all_matches.append(('COUNTY', state_county_match, state_counties[state_county_match], state_county_score))\n",
    "\n",
    "    # STEP 3: Find city matches (global first, then state-specific)\n",
    "    for entity in entities:\n",
    "        # Global city search\n",
    "        city_match, city_score = fuzzy_match_entity(entity, city_lookup, threshold=75)\n",
    "        if city_match:\n",
    "            all_matches.append(('CITY', city_match, city_lookup[city_match], city_score))\n",
    "\n",
    "        # State-specific city search (higher accuracy)\n",
    "        for state_name in found_states:\n",
    "            if state_name in city_by_state:\n",
    "                state_cities = city_by_state[state_name]\n",
    "                state_city_match, state_city_score = fuzzy_match_entity(entity, state_cities, threshold=70)\n",
    "                if state_city_match and state_city_score > city_score:\n",
    "                    # Replace with better state-specific match\n",
    "                    # Remove the global match if it exists\n",
    "                    all_matches = [m for m in all_matches if not (m[0] == 'CITY' and m[1] == city_match)]\n",
    "                    all_matches.append(('CITY', state_city_match, state_cities[state_city_match], state_city_score))\n",
    "\n",
    "    # Remove duplicates (same scale + name)\n",
    "    unique_matches = []\n",
    "    seen_combinations = set()\n",
    "    for match in all_matches:\n",
    "        combo = (match[0], match[1])  # (scale, name)\n",
    "        if combo not in seen_combinations:\n",
    "            unique_matches.append(match)\n",
    "            seen_combinations.add(combo)\n",
    "\n",
    "    return unique_matches\n",
    "\n",
    "def multi_level_assign_scale_levels(row, lookups):\n",
    "    \"\"\"\n",
    "    Return ALL geographic scale levels that match this tweet\n",
    "    Returns a list of matches: [(scale, name, geom, score), ...]\n",
    "    \"\"\"\n",
    "    gpe = str(row.get('GPE', '')).strip()\n",
    "    fac = str(row.get('FAC', '')).strip()\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    # Parse GPE into multiple entities\n",
    "    entities = parse_gpe_entities(gpe)\n",
    "\n",
    "    if entities:\n",
    "        # Find all geographic matches\n",
    "        geo_matches = find_all_geographic_matches(entities, lookups)\n",
    "        matches.extend(geo_matches)\n",
    "\n",
    "    # Add facility as separate match if available\n",
    "    if fac and fac not in ['nan', 'NAN', '']:\n",
    "        matches.append(('FACILITY', fac, row.geometry, 100))\n",
    "\n",
    "    # If no matches found, return unmatched\n",
    "    if not matches:\n",
    "        matches.append(('UNMATCHED', None, row.geometry, 0))\n",
    "\n",
    "    return matches\n",
    "\n",
    "def expand_tweets_by_matches(gdf, lookups, dataset_name):\n",
    "    \"\"\"\n",
    "    Expand the GeoDataFrame so each tweet creates multiple rows (one per geographic match)\n",
    "    \"\"\"\n",
    "    print(f\"\\nExpanding {dataset_name} tweets by geographic matches...\")\n",
    "\n",
    "    expanded_rows = []\n",
    "\n",
    "    for idx, row in gdf.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(idx)\n",
    "        matches = multi_level_assign_scale_levels(row, lookups)\n",
    "\n",
    "        # Create one row per match\n",
    "        for scale, name, geom, score in matches:\n",
    "            new_row = row.copy()\n",
    "            new_row['scale_level'] = scale\n",
    "            new_row['matched_name'] = name\n",
    "            new_row['matched_geom'] = geom\n",
    "            new_row['match_score'] = score\n",
    "            new_row['original_index'] = idx  # Track original tweet\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    # Create new GeoDataFrame and preserve the original CRS\n",
    "    expanded_gdf = gpd.GeoDataFrame(expanded_rows, crs=gdf.crs)\n",
    "\n",
    "    # Print statistics\n",
    "    original_count = len(gdf)\n",
    "    expanded_count = len(expanded_gdf)\n",
    "    expansion_ratio = expanded_count / original_count\n",
    "\n",
    "    # print(f\"  Original tweets: {original_count}\")\n",
    "    # print(f\"  Expanded rows: {expanded_count}\")\n",
    "    # print(f\"  Expansion ratio: {expansion_ratio:.2f}x\")\n",
    "\n",
    "    # Print scale distribution\n",
    "    # scale_counts = expanded_gdf['scale_level'].value_counts()\n",
    "    # print(f\"  {dataset_name} scale distribution:\")\n",
    "    # for scale, count in scale_counts.items():\n",
    "    #     print(f\"    {scale}: {count}\")\n",
    "\n",
    "    # Print average match scores by scale level\n",
    "    # print(f\"  Average match scores:\")\n",
    "    # for scale in ['STATE', 'COUNTY', 'CITY', 'FACILITY']:\n",
    "    #     if scale in expanded_gdf['scale_level'].values:\n",
    "    #         avg_score = expanded_gdf[expanded_gdf['scale_level'] == scale]['match_score'].mean()\n",
    "            # print(f\"    {scale}: {avg_score:.1f}%\")\n",
    "\n",
    "    # Show some examples of multi-level matches\n",
    "    print(f\"  Sample multi-level matches:\")\n",
    "    # Group by original tweet and show ones with multiple matches\n",
    "    multi_matches = expanded_gdf.groupby('original_index').size()\n",
    "    multi_match_indices = multi_matches[multi_matches > 1].head(5).index\n",
    "\n",
    "    for orig_idx in multi_match_indices:\n",
    "        tweet_matches = expanded_gdf[expanded_gdf['original_index'] == orig_idx]\n",
    "        original_gpe = tweet_matches.iloc[0]['GPE']\n",
    "        match_summary = ', '.join([f\"{row['scale_level']}:{row['matched_name']}\" for _, row in tweet_matches.iterrows()])\n",
    "        # print(f\"    '{original_gpe}' → {match_summary}\")\n",
    "\n",
    "    return expanded_gdf\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTE MULTI-LEVEL FUZZY MATCHING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-LEVEL GEOGRAPHIC MATCHING (ALL LEVELS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create hierarchical lookups\n",
    "lookups = create_hierarchical_lookups(states_gdf, counties_gdf, cities_gdf)\n",
    "\n",
    "# Apply to both datasets (this will expand the datasets)\n",
    "francine_gdf = expand_tweets_by_matches(francine_gdf, lookups, \"FRANCINE\")\n",
    "helene_gdf = expand_tweets_by_matches(helene_gdf, lookups, \"HELENE\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-LEVEL FUZZY MATCHING COMPLETE ✓\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNote: Datasets are now expanded - each original tweet may have multiple rows\")\n",
    "print(\"representing different geographic scales (STATE, COUNTY, CITY, etc.)\")"
   ],
   "id": "69dfb5ea69886475"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def validate_city_matching(francine_gdf, helene_gdf, cities_lookup, state_lookup, county_lookup):\n",
    "    \"\"\"\n",
    "    Validate which cities from tweets are found in the cities shapefile\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CITY MATCHING VALIDATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Combine both datasets\n",
    "    all_tweets = pd.concat([francine_gdf, helene_gdf], ignore_index=True)\n",
    "\n",
    "    # Extract all unique GPE values (potential cities)\n",
    "    all_gpe_values = set()\n",
    "    for gpe in all_tweets['GPE'].dropna():\n",
    "        gpe_clean = str(gpe).upper().strip()\n",
    "        if gpe_clean and gpe_clean != 'NAN':\n",
    "            all_gpe_values.add(gpe_clean)\n",
    "\n",
    "    # print(f\"Total unique GPE values in tweets: {len(all_gpe_values)}\")\n",
    "\n",
    "    # Check matches against each lookup\n",
    "    state_matches = set(gpe for gpe in all_gpe_values if gpe in state_lookup)\n",
    "    county_matches = set(gpe for gpe in all_gpe_values if gpe in county_lookup)\n",
    "    city_matches = set(gpe for gpe in all_gpe_values if gpe in cities_lookup)\n",
    "\n",
    "    # Calculate what's left after state/county matching\n",
    "    remaining_after_states = all_gpe_values - state_matches\n",
    "    remaining_after_counties = remaining_after_states - county_matches\n",
    "    potential_cities = remaining_after_counties\n",
    "    # print(potential_cities)\n",
    "    # print(f\"\\nMatching Results:\")\n",
    "    # print(f\"  States matched: {len(state_matches)}\")\n",
    "    # print(f\"  Counties matched: {len(county_matches)}\")\n",
    "    # print(f\"  Cities matched: {len(city_matches)}\")\n",
    "    # print(f\"  Potential cities (not state/county): {len(potential_cities)}\")\n",
    "    # print(f\"  Cities found in shapefile: {len(city_matches)}\")\n",
    "\n",
    "    # Calculate city matching rate\n",
    "    if len(potential_cities) > 0:\n",
    "        city_match_rate = len(city_matches) / len(potential_cities) * 100\n",
    "        print(f\"  City matching rate: {city_match_rate:.1f}%\")\n",
    "\n",
    "    # Show some examples of successful city matches\n",
    "    print(f\"\\nSample successful city matches:\")\n",
    "    sample_cities = list(city_matches)[:10]\n",
    "    for city in sample_cities:\n",
    "        geom_type = cities_lookup[city].geom_type\n",
    "        print(f\"  - {city}: {geom_type}\")\n",
    "\n",
    "    # Show unmatched potential cities\n",
    "    unmatched_cities = potential_cities - city_matches\n",
    "    # print(f\"\\nUnmatched potential cities (first 20):\")\n",
    "    # unmatched_list = sorted(list(unmatched_cities))[:20]\n",
    "    # for city in unmatched_list:\n",
    "    #     print(f\"  - {city}\")\n",
    "    #\n",
    "    # if len(unmatched_cities) > 20:\n",
    "    #     print(f\"  ... and {len(unmatched_cities) - 20} more\")\n",
    "\n",
    "    # Show shapefile city name samples for comparison\n",
    "    # print(f\"\\nSample city names from shapefile (first 20):\")\n",
    "    shapefile_cities = sorted(list(cities_lookup.keys()))[:20]\n",
    "    # for city in shapefile_cities:\n",
    "    #     print(f\"  - {city}\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"VALIDATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return {\n",
    "        'total_gpe': len(all_gpe_values),\n",
    "        'state_matches': len(state_matches),\n",
    "        'county_matches': len(county_matches),\n",
    "        'city_matches': len(city_matches),\n",
    "        'potential_cities': len(potential_cities),\n",
    "        'unmatched_cities': unmatched_cities,\n",
    "        'city_match_rate': city_match_rate if len(potential_cities) > 0 else 0\n",
    "    }\n",
    "# Run the validation after creating lookups\n",
    "validation_results = validate_city_matching(francine_gdf, helene_gdf, lookups['city_lookup'], lookups['state_lookup'], lookups['county_lookup'])"
   ],
   "id": "dff57c3871a4e2e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " # Group tweets by 4-hour intervals and scale level\n",
    "# Using unix_timestamp for unambiguous temporal grouping\n",
    "\n",
    "# Alternative approach:\n",
    "francine_interval_counts = francine_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).agg({\n",
    "    'matched_geom': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Add count column separately\n",
    "count_series = francine_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).size()\n",
    "francine_interval_counts['count'] = count_series.values\n",
    "\n",
    "# Same for Helene\n",
    "helene_interval_counts = helene_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).agg({\n",
    "    'matched_geom': 'first'\n",
    "}).reset_index()\n",
    "count_series = helene_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).size()\n",
    "helene_interval_counts['count'] = count_series.values\n",
    "\n",
    "# Sort by timestamp to ensure chronological order\n",
    "francine_interval_counts = francine_interval_counts.sort_values('unix_timestamp')\n",
    "helene_interval_counts = helene_interval_counts.sort_values('unix_timestamp')\n",
    "\n",
    "# Calculate cumulative counts\n",
    "francine_interval_counts['cumulative_count'] = francine_interval_counts.groupby(['scale_level', 'matched_name'])['count'].cumsum()\n",
    "helene_interval_counts['cumulative_count'] = helene_interval_counts.groupby(['scale_level', 'matched_name'])['count'].cumsum()\n",
    "\n",
    "# Get unique time bins for iteration\n",
    "francine_time_bins = sorted(francine_gdf['unix_timestamp'].unique())\n",
    "helene_time_bins = sorted(helene_gdf['unix_timestamp'].unique())\n",
    "\n",
    "# Display summary statistics\n",
    "# print(\"FRANCINE Time Binning Summary:\")\n",
    "# print(f\"  Total time bins: {len(francine_time_bins)}\")\n",
    "# print(f\"  Total location-time combinations: {len(francine_interval_counts)}\")\n",
    "# print(f\"\\nSample interval counts:\")\n",
    "# print(francine_interval_counts.head(10))\n",
    "#\n",
    "# print(f\"\\nHELENE Time Binning Summary:\")\n",
    "# print(f\"  Total time bins: {len(helene_time_bins)}\")\n",
    "# print(f\"  Total location-time combinations: {len(helene_interval_counts)}\")\n",
    "# print(f\"\\nSample interval counts:\")\n",
    "# print(helene_interval_counts.head(10))"
   ],
   "id": "ea77071aed93ac17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: DEFINE MASTER GRID CANVAS\n",
    "# ==============================================================================\n",
    "\n",
    "# Configuration\n",
    "TARGET_CRS = 'EPSG:3857'  # Web Mercator\n",
    "CELL_SIZE_M = 1000  # 5 km in meters\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: CREATING MASTER GRID CANVAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Project both datasets to target CRS\n",
    "print(f\"\\nProjecting datasets to {TARGET_CRS}...\")\n",
    "francine_proj = francine_gdf.to_crs(TARGET_CRS)\n",
    "helene_proj = helene_gdf.to_crs(TARGET_CRS)\n",
    "\n",
    "# Also project reference geometries\n",
    "print(\"Projecting reference geometries...\")\n",
    "states_proj = states_gdf.to_crs(TARGET_CRS)\n",
    "counties_proj = counties_gdf.to_crs(TARGET_CRS)\n",
    "cities_proj = cities_gdf.to_crs(TARGET_CRS)\n",
    "# Calculate combined extent from both hurricanes\"\n",
    "print(\"\\nCalculating master extent...\")\n",
    "francine_bounds = francine_proj.total_bounds\n",
    "helene_bounds = helene_proj.total_bounds\n",
    "\n",
    "# Get union of both bounding boxes\n",
    "minx = min(francine_bounds[0], helene_bounds[0])\n",
    "miny = min(francine_bounds[1], helene_bounds[1])\n",
    "maxx = max(francine_bounds[2], helene_bounds[2])\n",
    "maxy = max(francine_bounds[3], helene_bounds[3])\n",
    "#\n",
    "# print(f\"  Master bounds (EPSG:3857):\")\n",
    "# print(f\"    minx: {minx:,.2f}\")\n",
    "# print(f\"    miny: {miny:,.2f}\")\n",
    "# print(f\"    maxx: {maxx:,.2f}\")\n",
    "# print(f\"    maxy: {maxy:,.2f}\")\n",
    "\n",
    "# Calculate grid dimensions\n",
    "width = int(np.ceil((maxx - minx) / CELL_SIZE_M))\n",
    "height = int(np.ceil((maxy - miny) / CELL_SIZE_M))\n",
    "\n",
    "print(f\"\\nGrid Configuration:\")\n",
    "print(f\"  Cell size: {CELL_SIZE_M:,} meters ({CELL_SIZE_M/1000} km)\")\n",
    "print(f\"  Grid dimensions: {width} x {height} cells\")\n",
    "print(f\"  Total cells: {width * height:,}\")\n",
    "\n",
    "# Create master transform\n",
    "master_transform = from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "print(f\"\\nMaster Transform:\")\n",
    "print(f\"  {master_transform}\")\n",
    "\n",
    "# Calculate actual coverage area\n",
    "area_km2 = (width * height * CELL_SIZE_M * CELL_SIZE_M) / 1_000_000\n",
    "print(f\"\\nCoverage area: {area_km2:,.2f} km²\")\n",
    "\n",
    "# Store grid parameters for later use\n",
    "grid_params = {\n",
    "    'crs': TARGET_CRS,\n",
    "    'cell_size': CELL_SIZE_M,\n",
    "    'width': width,\n",
    "    'height': height,\n",
    "    'bounds': (minx, miny, maxx, maxy),\n",
    "    'transform': master_transform\n",
    "}\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"MASTER GRID CANVAS READY ✓\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "# Update lookup dictionaries with projected geometries\n",
    "print(\"\\nUpdating geometry lookups with projected coordinates...\")\n",
    "state_lookup_proj = dict(zip(states_proj['NAME'].str.upper(), states_proj.geometry))\n",
    "county_lookup_proj = dict(zip(counties_proj['NAME'].str.upper(), counties_proj.geometry))\n",
    "cities_lookup_proj = dict(zip(cities_proj['NAME'].str.upper(), cities_proj.geometry))\n",
    "validation_results = validate_city_matching(francine_gdf, helene_gdf, lookups['city_lookup'], lookups['state_lookup'], lookups['county_lookup'])\n",
    "print(\"Lookup dictionaries updated with projected geometries ✓\")"
   ],
   "id": "48d88a363ca9f14c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.features import geometry_mask\n",
    "# ==============================================================================\n",
    "# STEP 2: MAIN RASTERIZATION LOOP - TIME ITERATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Create output directories\n",
    "rasters_dir = r\"\\rasters_output\"\n",
    "output_dir = f\"{local_path}{rasters_dir}\"\n",
    "# output_dir = os.path.join(local_path, 'rasters_output')\n",
    "# output_dir = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# def create_hierarchical_rasters(data, grid_params, time_bin):\n",
    "#     \"\"\"\n",
    "#     Create hierarchically weighted rasters instead of additive ones\n",
    "#     \"\"\"\n",
    "#     print(f\"    Creating hierarchical raster for time {time_bin}...\")\n",
    "#\n",
    "#     # Initialize output grid\n",
    "#     output_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "#\n",
    "#     # 1. Create BASE LAYER from states (distributed evenly)\n",
    "#     state_data = data[data['scale_level'] == 'STATE']\n",
    "#     if len(state_data) > 0:\n",
    "#         state_counts = state_data.groupby('matched_name')['count'].sum()\n",
    "#\n",
    "#         for state_name, tweet_count in state_counts.items():\n",
    "#             if state_name in state_lookup_proj:\n",
    "#                 state_geom = state_lookup_proj[state_name]\n",
    "#                 shapes = [(state_geom, 1)]\n",
    "#\n",
    "#                 mask = rasterize(\n",
    "#                     shapes=shapes,\n",
    "#                     out_shape=(grid_params['height'], grid_params['width']),\n",
    "#                     transform=grid_params['transform'],\n",
    "#                     fill=0,\n",
    "#                     dtype=np.float32,\n",
    "#                     all_touched=True\n",
    "#                 )\n",
    "#\n",
    "#                 # Distribute tweets across state pixels\n",
    "#                 pixels_in_state = np.sum(mask)\n",
    "#                 if pixels_in_state > 0:\n",
    "#                     # DISTRIBUTE instead of multiply\n",
    "#                     output_grid += mask * (tweet_count / pixels_in_state) * 0.1  # Reduced weight\n",
    "#\n",
    "#     # 2. Add COUNTY hotspots (concentrated boost)\n",
    "#     county_data = data[data['scale_level'] == 'COUNTY']\n",
    "#     if len(county_data) > 0:\n",
    "#         county_counts = county_data.groupby('matched_name')['count'].sum()\n",
    "#\n",
    "#         for county_name, tweet_count in county_counts.items():\n",
    "#             if county_name in county_lookup_proj:\n",
    "#                 county_geom = county_lookup_proj[county_name]\n",
    "#                 shapes = [(county_geom, 1)]\n",
    "#\n",
    "#                 mask = rasterize(\n",
    "#                     shapes=shapes,\n",
    "#                     out_shape=(grid_params['height'], grid_params['width']),\n",
    "#                     transform=grid_params['transform'],\n",
    "#                     fill=0,\n",
    "#                     dtype=np.float32,\n",
    "#                     all_touched=True\n",
    "#                 )\n",
    "#\n",
    "#                 pixels_in_county = np.sum(mask)\n",
    "#                 if pixels_in_county > 0:\n",
    "#                     # Add as concentrated boost, not total count\n",
    "#                     concentration = tweet_count / np.sqrt(pixels_in_county)  # Square root scaling\n",
    "#                     output_grid += mask * concentration * 0.5\n",
    "#\n",
    "#     # 3. City and Facility layers remain strong local signals\n",
    "#     # Keep your existing city and facility code but with adjusted weights\n",
    "#     city_data = data[data['scale_level'] == 'CITY']\n",
    "#     if len(city_data) > 0:\n",
    "#         city_counts = city_data.groupby('matched_name')['count'].sum()\n",
    "#\n",
    "#         for city_name, tweet_count in city_counts.items():\n",
    "#             if city_name in cities_lookup_proj:\n",
    "#                 city_geom = cities_lookup_proj[city_name]\n",
    "#                 shapes = [(city_geom, 1)]\n",
    "#\n",
    "#                 mask = rasterize(\n",
    "#                     shapes=shapes,\n",
    "#                     out_shape=(grid_params['height'], grid_params['width']),\n",
    "#                     transform=grid_params['transform'],\n",
    "#                     fill=0,\n",
    "#                     dtype=np.float32,\n",
    "#                     all_touched=True\n",
    "#                 )\n",
    "#\n",
    "#                 # Cities get direct count multiplication\n",
    "#                 output_grid += mask * tweet_count\n",
    "#\n",
    "#     # Facilities with KDE remain the same\n",
    "#     facility_data = data[data['scale_level'] == 'FACILITY']\n",
    "#     if len(facility_data) > 0:\n",
    "#         # Your existing facility KDE code\n",
    "#         facility_grid = create_facility_raster(data, grid_params)\n",
    "#         output_grid += facility_grid\n",
    "#\n",
    "#     return output_grid\n",
    "# def create_hierarchical_rasters(data, grid_params, time_bin):\n",
    "#     \"\"\"\n",
    "#     Create hierarchically weighted rasters with VISIBLE state boundaries\n",
    "#     \"\"\"\n",
    "#     print(f\"    Creating hierarchical raster for time {time_bin}...\")\n",
    "#\n",
    "#     output_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "#\n",
    "#     # 1. STATE BASE LAYER - needs to be visible!\n",
    "#     state_data = data[data['scale_level'] == 'STATE']\n",
    "#     if len(state_data) > 0:\n",
    "#         state_counts = state_data.groupby('matched_name')['count'].sum()\n",
    "#\n",
    "#         for state_name, tweet_count in state_counts.items():\n",
    "#             if state_name in state_lookup_proj:\n",
    "#                 state_geom = state_lookup_proj[state_name]\n",
    "#                 shapes = [(state_geom, 1)]\n",
    "#\n",
    "#                 mask = rasterize(\n",
    "#                     shapes=shapes,\n",
    "#                     out_shape=(grid_params['height'], grid_params['width']),\n",
    "#                     transform=grid_params['transform'],\n",
    "#                     fill=0,\n",
    "#                     dtype=np.float32,\n",
    "#                     all_touched=True\n",
    "#                 )\n",
    "#\n",
    "#                 # BASE VALUE: Give each state pixel a minimum visible value\n",
    "#                 # Use log scaling for tweet count to compress range\n",
    "#                 base_value = np.log1p(tweet_count) * 2  # Log scale with multiplier\n",
    "#                 output_grid += mask * base_value\n",
    "#\n",
    "#     # 2. COUNTY LAYER - stronger local boost\n",
    "#     county_data = data[data['scale_level'] == 'COUNTY']\n",
    "#     if len(county_data) > 0:\n",
    "#         county_counts = county_data.groupby('matched_name')['count'].sum()\n",
    "#\n",
    "#         for county_name, tweet_count in county_counts.items():\n",
    "#             if county_name in county_lookup_proj:\n",
    "#                 county_geom = county_lookup_proj[county_name]\n",
    "#                 shapes = [(county_geom, 1)]\n",
    "#\n",
    "#                 mask = rasterize(\n",
    "#                     shapes=shapes,\n",
    "#                     out_shape=(grid_params['height'], grid_params['width']),\n",
    "#                     transform=grid_params['transform'],\n",
    "#                     fill=0,\n",
    "#                     dtype=np.float32,\n",
    "#                     all_touched=True\n",
    "#                 )\n",
    "#\n",
    "#                 # ADDITIVE BOOST: Counties add on top of state base\n",
    "#                 county_value = np.log1p(tweet_count) * 5  # Stronger than state\n",
    "#                 output_grid += mask * county_value\n",
    "#\n",
    "#     # 3. CITY LAYER - even stronger\n",
    "#     city_data = data[data['scale_level'] == 'CITY']\n",
    "#     if len(city_data) > 0:\n",
    "#         city_counts = city_data.groupby('matched_name')['count'].sum()\n",
    "#\n",
    "#         for city_name, tweet_count in city_counts.items():\n",
    "#             if city_name in cities_lookup_proj:\n",
    "#                 city_geom = cities_lookup_proj[city_name]\n",
    "#                 shapes = [(city_geom, 1)]\n",
    "#\n",
    "#                 mask = rasterize(\n",
    "#                     shapes=shapes,\n",
    "#                     out_shape=(grid_params['height'], grid_params['width']),\n",
    "#                     transform=grid_params['transform'],\n",
    "#                     fill=0,\n",
    "#                     dtype=np.float32,\n",
    "#                     all_touched=True\n",
    "#                 )\n",
    "#\n",
    "#                 # Cities get strong boost\n",
    "#                 city_value = np.log1p(tweet_count) * 10\n",
    "#                 output_grid += mask * city_value\n",
    "#\n",
    "#     # 4. FACILITIES - highest intensity (keep your existing KDE code)\n",
    "#     facility_data = data[data['scale_level'] == 'FACILITY']\n",
    "#     if len(facility_data) > 0:\n",
    "#         facility_grid = create_facility_raster(data, grid_params)\n",
    "#         output_grid += facility_grid\n",
    "#\n",
    "#     return output_grid\n",
    "\n",
    "def create_hierarchical_rasters(data, grid_params, time_bin):\n",
    "    \"\"\"Create hierarchically weighted rasters with automatic parent state inclusion\"\"\"\n",
    "    print(f\"    Creating hierarchical raster for time {time_bin}...\")\n",
    "\n",
    "    output_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "    states_to_include = set()  # Track which states need base layers\n",
    "\n",
    "    # 1. First pass: identify all states that need base layers\n",
    "    state_data = data[data['scale_level'] == 'STATE']\n",
    "    if len(state_data) > 0:\n",
    "        states_to_include.update(state_data['matched_name'].unique())\n",
    "\n",
    "    # Check counties - add their parent states\n",
    "    county_data = data[data['scale_level'] == 'COUNTY']\n",
    "    for county_name in county_data['matched_name'].unique():\n",
    "        if county_name in county_lookup_proj:\n",
    "            # Find parent state by spatial containment\n",
    "            county_geom = county_lookup_proj[county_name]\n",
    "            for state_name, state_geom in state_lookup_proj.items():\n",
    "                if state_geom.contains(county_geom.centroid):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "\n",
    "    # Check cities - add their parent states\n",
    "    city_data = data[data['scale_level'] == 'CITY']\n",
    "    for city_name in city_data['matched_name'].unique():\n",
    "        if city_name in cities_lookup_proj:\n",
    "            city_geom = cities_lookup_proj[city_name]\n",
    "            for state_name, state_geom in state_lookup_proj.items():\n",
    "                if state_geom.contains(city_geom.centroid):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "\n",
    "    # 2. Rasterize all states that need inclusion\n",
    "    for state_name in states_to_include:\n",
    "        if state_name in state_lookup_proj:\n",
    "            state_geom = state_lookup_proj[state_name]\n",
    "            mask = rasterize(\n",
    "                [(state_geom, 1)],\n",
    "                out_shape=(grid_params['height'], grid_params['width']),\n",
    "                transform=grid_params['transform'],\n",
    "                fill=0, dtype=np.float32, all_touched=True\n",
    "            )\n",
    "\n",
    "            # Get tweet count if state was mentioned, else use minimal base\n",
    "            if state_name in state_data['matched_name'].values:\n",
    "                tweet_count = state_data[state_data['matched_name'] == state_name]['count'].sum()\n",
    "            else:\n",
    "                tweet_count = 1  # Minimal base for implied states\n",
    "\n",
    "            base_value = np.log1p(tweet_count) * 2\n",
    "            output_grid += mask * base_value\n",
    "\n",
    "    # 3. Add counties (same as before)\n",
    "    if len(county_data) > 0:\n",
    "        county_counts = county_data.groupby('matched_name')['count'].sum()\n",
    "        for county_name, tweet_count in county_counts.items():\n",
    "            if county_name in county_lookup_proj:\n",
    "                mask = rasterize(\n",
    "                    [(county_lookup_proj[county_name], 1)],\n",
    "                    out_shape=(grid_params['height'], grid_params['width']),\n",
    "                    transform=grid_params['transform'],\n",
    "                    fill=0, dtype=np.float32, all_touched=True\n",
    "                )\n",
    "                output_grid += mask * np.log1p(tweet_count) * 5\n",
    "\n",
    "    # 4. Add cities (same as before)\n",
    "    if len(city_data) > 0:\n",
    "        city_counts = city_data.groupby('matched_name')['count'].sum()\n",
    "        for city_name, tweet_count in city_counts.items():\n",
    "            if city_name in cities_lookup_proj:\n",
    "                mask = rasterize(\n",
    "                    [(cities_lookup_proj[city_name], 1)],\n",
    "                    out_shape=(grid_params['height'], grid_params['width']),\n",
    "                    transform=grid_params['transform'],\n",
    "                    fill=0, dtype=np.float32, all_touched=True\n",
    "                )\n",
    "                output_grid += mask * np.log1p(tweet_count) * 10\n",
    "\n",
    "    # 5. Add facilities\n",
    "    facility_data = data[data['scale_level'] == 'FACILITY']\n",
    "    if len(facility_data) > 0:\n",
    "        output_grid += create_facility_raster(data, grid_params)\n",
    "\n",
    "    return output_grid\n",
    "\n",
    "def process_hurricane(hurricane_name, gdf_proj, interval_counts, time_bins, timestamp_dict):\n",
    "    \"\"\"\n",
    "    Process a single hurricane through all time bins\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"PROCESSING: {hurricane_name.upper()}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    # Create hurricane-specific output directory\n",
    "    hurricane_dir = os.path.join(output_dir, hurricane_name.lower())\n",
    "    os.makedirs(hurricane_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize cumulative grid (persists across time bins)\n",
    "    cumulative_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "\n",
    "    # Loop through each time bin chronologically\n",
    "    for idx, time_bin in enumerate(time_bins):\n",
    "        print(f\"\\n--- Time Bin {idx+1}/{len(time_bins)}: {time_bin} ---\")\n",
    "\n",
    "        # Filter data for current time bin\n",
    "        current_data = interval_counts[interval_counts['unix_timestamp'] == time_bin]\n",
    "        tweet_count = len(current_data)\n",
    "        print(f\"  Tweets in this bin: {tweet_count}\")\n",
    "\n",
    "        # Initialize incremental grid for this time bin\n",
    "        # REPLACE THIS:\n",
    "        # incremental_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "        # state_raster = create_state_raster(current_data, grid_params)\n",
    "        # incremental_grid += state_raster\n",
    "        # county_raster = create_county_raster(current_data, grid_params)\n",
    "        # incremental_grid += county_raster\n",
    "        # city_raster = create_city_raster(current_data, grid_params)\n",
    "        # incremental_grid += city_raster\n",
    "        # facility_raster = create_facility_raster(current_data, grid_params)\n",
    "        # incremental_grid += facility_raster\n",
    "\n",
    "        # WITH THIS:\n",
    "        incremental_grid = create_hierarchical_rasters(current_data, grid_params, time_bin)\n",
    "\n",
    "        # === END PLACEHOLDERS ===\n",
    "\n",
    "        # Update cumulative grid\n",
    "        cumulative_grid += incremental_grid\n",
    "\n",
    "        # Save rasters\n",
    "        save_raster(incremental_grid, hurricane_dir, hurricane_name, time_bin, 'increment', timestamp_dict)\n",
    "        save_raster(cumulative_grid, hurricane_dir, hurricane_name, time_bin, 'cumulative', timestamp_dict)\n",
    "\n",
    "        print(f\"  Incremental max value: {np.max(incremental_grid):.2f}\")\n",
    "        print(f\"  Cumulative max value: {np.max(cumulative_grid):.2f}\")\n",
    "\n",
    "    print(f\"\\n{hurricane_name.upper()} processing complete!\")\n",
    "    print(f\"Output saved to: {hurricane_dir}\")\n",
    "    return\n",
    "\n",
    "# ==============================================================================\n",
    "# PLACEHOLDER FUNCTIONS (TO BE IMPLEMENTED)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_state_raster(data, grid_params):\n",
    "    \"\"\"Rasterize state-level tweets\"\"\"\n",
    "    print(\"    [STATE] Creating state raster...\")\n",
    "\n",
    "    # Initialize empty raster\n",
    "    state_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "\n",
    "    # Filter for STATE-level tweets only\n",
    "    state_data = data[data['scale_level'] == 'STATE']\n",
    "\n",
    "    if len(state_data) == 0:\n",
    "        print(\"      No state-level tweets in this time bin\")\n",
    "        return state_grid\n",
    "\n",
    "    # Group by state name and sum counts\n",
    "    state_counts = state_data.groupby('matched_name')['count'].sum()\n",
    "\n",
    "    print(f\"      Processing {len(state_counts)} unique states\")\n",
    "\n",
    "    # Process each state\n",
    "    for state_name, tweet_count in state_counts.items():\n",
    "        if state_name in state_lookup_proj:\n",
    "            # Get the state geometry\n",
    "            state_geom = state_lookup_proj[state_name]\n",
    "\n",
    "            # Rasterize the polygon\n",
    "            # Create a list of (geometry, value) tuples\n",
    "            shapes = [(state_geom, 1)]\n",
    "\n",
    "            # Rasterize to temporary grid\n",
    "            temp_grid = rasterio.features.rasterize(\n",
    "                shapes=shapes,\n",
    "                out_shape=(grid_params['height'], grid_params['width']),\n",
    "                transform=grid_params['transform'],\n",
    "                fill=0,\n",
    "                dtype=np.float32,\n",
    "                all_touched=True  # Include all pixels touched by polygon\n",
    "            )\n",
    "\n",
    "            # Multiply by tweet count and add to state grid\n",
    "            state_grid += temp_grid * tweet_count\n",
    "\n",
    "            print(f\"      - {state_name}: {tweet_count} tweets, {np.sum(temp_grid)} pixels\")\n",
    "        else:\n",
    "            print(f\"      WARNING: State '{state_name}' not found in lookup\")\n",
    "\n",
    "    total_value = np.sum(state_grid)\n",
    "    max_value = np.max(state_grid)\n",
    "    # print(f\"      Total state grid value: {total_value:.0f}, Max pixel: {max_value:.0f}\")\n",
    "\n",
    "    return state_grid\n",
    "\n",
    "def create_county_raster(data, grid_params):\n",
    "    \"\"\"Rasterize county-level tweets with hotspot multiplier\"\"\"\n",
    "    print(\"    [COUNTY] Creating county raster...\")\n",
    "\n",
    "    # Initialize empty raster\n",
    "    county_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "\n",
    "    # Filter for COUNTY-level tweets only\n",
    "    county_data = data[data['scale_level'] == 'COUNTY']\n",
    "\n",
    "    if len(county_data) == 0:\n",
    "        print(\"      No county-level tweets in this time bin\")\n",
    "        return county_grid\n",
    "\n",
    "    # Group by county name and sum counts\n",
    "    county_counts = county_data.groupby('matched_name')['count'].sum()\n",
    "\n",
    "    print(f\"      Processing {len(county_counts)} unique counties\")\n",
    "\n",
    "    # HOTSPOT MULTIPLIER for counties\n",
    "    county_multiplier = 3  # Make counties 3x more prominent\n",
    "\n",
    "    # Process each county\n",
    "    for county_name, tweet_count in county_counts.items():\n",
    "        if county_name in county_lookup_proj:\n",
    "            # Get the county geometry\n",
    "            county_geom = county_lookup_proj[county_name]\n",
    "\n",
    "            # Rasterize the polygon\n",
    "            shapes = [(county_geom, 1)]\n",
    "\n",
    "            # Rasterize to temporary grid\n",
    "            temp_grid = rasterize(\n",
    "                shapes=shapes,\n",
    "                out_shape=(grid_params['height'], grid_params['width']),\n",
    "                transform=grid_params['transform'],\n",
    "                fill=0,\n",
    "                dtype=np.float32,\n",
    "                all_touched=True  # Include all pixels touched by polygon\n",
    "            )\n",
    "\n",
    "            # Multiply by tweet count AND multiplier for hotspot effect\n",
    "            county_grid += temp_grid * tweet_count * county_multiplier\n",
    "\n",
    "            # print(f\"      - {county_name}: {tweet_count} tweets × {county_multiplier} = {tweet_count * county_multiplier}, {np.sum(temp_grid)} pixels\")\n",
    "        else:\n",
    "            print(f\"      WARNING: County '{county_name}' not found in lookup\")\n",
    "\n",
    "    total_value = np.sum(county_grid)\n",
    "    max_value = np.max(county_grid)\n",
    "    # print(f\"      Total county grid value: {total_value:.0f}, Max pixel: {max_value:.0f}\")\n",
    "\n",
    "    return county_grid\n",
    "\n",
    "def create_city_raster(data, grid_params):\n",
    "    \"\"\"Rasterize city-level tweets using city polygon geometries with hotspot multiplier\"\"\"\n",
    "    print(\"    [CITY] Creating city raster...\")\n",
    "\n",
    "    # Initialize empty raster\n",
    "    city_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "\n",
    "    # Filter for CITY-level tweets only\n",
    "    city_data = data[data['scale_level'] == 'CITY']\n",
    "\n",
    "    if len(city_data) == 0:\n",
    "        print(\"      No city-level tweets in this time bin\")\n",
    "        return city_grid\n",
    "\n",
    "    # Group by city name and sum counts\n",
    "    city_counts = city_data.groupby('matched_name')['count'].sum()\n",
    "\n",
    "    print(f\"      Processing {len(city_counts)} unique cities\")\n",
    "\n",
    "    # HOTSPOT MULTIPLIER for cities\n",
    "    city_multiplier = 5     # Make cities 5x more prominent\n",
    "\n",
    "    # Process each city\n",
    "    for city_name, tweet_count in city_counts.items():\n",
    "        if city_name in cities_lookup_proj:\n",
    "            # Get the city polygon geometry (already projected to grid CRS)\n",
    "            city_geom_proj = cities_lookup_proj[city_name]\n",
    "\n",
    "            # Rasterize the city polygon directly\n",
    "            shapes = [(city_geom_proj, 1)]\n",
    "\n",
    "            # Rasterize to temporary grid\n",
    "            temp_grid = rasterize(\n",
    "                shapes=shapes,\n",
    "                out_shape=(grid_params['height'], grid_params['width']),\n",
    "                transform=grid_params['transform'],\n",
    "                fill=0,\n",
    "                dtype=np.float32,\n",
    "                all_touched=True  # Include all pixels touched by city boundary\n",
    "            )\n",
    "\n",
    "            # Multiply by tweet count AND multiplier for hotspot effect\n",
    "            city_grid += temp_grid * tweet_count * city_multiplier\n",
    "\n",
    "            print(f\"      - {city_name}: {tweet_count} tweets × {city_multiplier} = {tweet_count * city_multiplier}, {np.sum(temp_grid)} pixels\")\n",
    "        else:\n",
    "            print(f\"      WARNING: City '{city_name}' not found in projected lookup\")\n",
    "\n",
    "    total_value = np.sum(city_grid)\n",
    "    max_value = np.max(city_grid)\n",
    "    # print(f\"      Total city grid value: {total_value:.0f}, Max pixel: {max_value:.0f}\")\n",
    "\n",
    "    return city_grid\n",
    "\n",
    "def create_facility_raster(data, grid_params):\n",
    "    \"\"\"Create KDE raster for facility points with strong hotspot multiplier\"\"\"\n",
    "    print(\"    [FACILITY] Creating facility raster...\")\n",
    "\n",
    "    # Initialize empty raster\n",
    "    facility_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "\n",
    "    # Filter for FACILITY-level tweets only\n",
    "    facility_data = data[data['scale_level'] == 'FACILITY']\n",
    "\n",
    "    if len(facility_data) == 0:\n",
    "        print(\"      No facility-level tweets in this time bin\")\n",
    "        return facility_grid\n",
    "\n",
    "    # Group by facility coordinates (using matched_name as proxy) and sum counts\n",
    "    facility_counts = facility_data.groupby('matched_name')['count'].sum()\n",
    "\n",
    "    print(f\"      Processing {len(facility_counts)} unique facilities\")\n",
    "\n",
    "    # HOTSPOT PARAMETERS for facilities\n",
    "    sigma_meters = 2 * grid_params['cell_size']  # 10 km for 5km cells\n",
    "    sigma_pixels = sigma_meters / grid_params['cell_size']  # Convert to pixel units\n",
    "    facility_multiplier = 10  # Make facilities 10x more prominent (strongest hotspots)\n",
    "\n",
    "    # Process each facility\n",
    "    facilities_processed = 0\n",
    "    for facility_name, tweet_count in facility_counts.items():\n",
    "        # Get facility data to extract geometry\n",
    "        facility_rows = facility_data[facility_data['matched_name'] == facility_name]\n",
    "\n",
    "        if len(facility_rows) > 0:\n",
    "            # Get the point geometry (should be from the tweet's geocoded location)\n",
    "            facility_point = facility_rows.iloc[0]['matched_geom']\n",
    "\n",
    "            # Project point to grid CRS if needed\n",
    "            if hasattr(facility_point, 'x') and hasattr(facility_point, 'y'):\n",
    "                # Create GeoSeries to handle projection\n",
    "                point_geoseries = gpd.GeoSeries([facility_point], crs='EPSG:4326')\n",
    "                point_proj = point_geoseries.to_crs(grid_params['crs']).iloc[0]\n",
    "\n",
    "                # Convert point coordinates to pixel indices\n",
    "                px = (point_proj.x - grid_params['bounds'][0]) / grid_params['cell_size']\n",
    "                py = (grid_params['bounds'][3] - point_proj.y) / grid_params['cell_size']\n",
    "\n",
    "                # Check if point is within grid bounds\n",
    "                if 0 <= px < grid_params['width'] and 0 <= py < grid_params['height']:\n",
    "                    # Create point raster with tweet count at location\n",
    "                    point_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "                    point_grid[int(py), int(px)] = tweet_count\n",
    "\n",
    "                    # Apply Gaussian filter to create kernel density\n",
    "                    kernel_grid = gaussian_filter(point_grid, sigma=sigma_pixels, mode='constant', cval=0)\n",
    "\n",
    "                    # FIXED: Only add once with proper multiplier\n",
    "                    facility_grid += kernel_grid * facility_multiplier\n",
    "\n",
    "                    facilities_processed += 1\n",
    "                    effective_value = tweet_count * facility_multiplier\n",
    "                    # print(f\"      - {facility_name}: {tweet_count} tweets × {facility_multiplier} = {effective_value}, KDE at ({point_proj.x:.0f}, {point_proj.y:.0f})\")\n",
    "                else:\n",
    "                    print(f\"      WARNING: Facility '{facility_name}' outside grid bounds\")\n",
    "            else:\n",
    "                print(f\"      WARNING: Invalid geometry for facility '{facility_name}'\")\n",
    "\n",
    "    print(f\"      Processed {facilities_processed} facilities with sigma={sigma_pixels:.2f} pixels\")\n",
    "\n",
    "    total_value = np.sum(facility_grid)\n",
    "    max_value = np.max(facility_grid)\n",
    "    # print(f\"      Total facility grid value: {total_value:.2f}, Max pixel: {max_value:.2f}\")\n",
    "\n",
    "    return facility_grid\n",
    "\n",
    "def save_raster(grid, output_dir, hurricane_name, time_bin, raster_type, timestamp_dict):\n",
    "    \"\"\"Save raster as GeoTIFF in type-specific subdirectory\"\"\"\n",
    "    # Create subdirectory for raster type\n",
    "    type_dir = os.path.join(output_dir, raster_type)\n",
    "    os.makedirs(type_dir, exist_ok=True)\n",
    "\n",
    "    # Convert unix timestamp (microseconds) back to datetime\n",
    "    time_str = timestamp_dict[time_bin].strftime('%Y%m%d_%H%M%S')\n",
    "    # time_str = pd.Timestamp(time_bin, unit='us').strftime('%Y%m%d_%H%M%S')\n",
    "    print([time_str])\n",
    "    filename = f\"{hurricane_name}_tweets_{time_str}.tif\"\n",
    "    filepath = os.path.join(type_dir, filename)\n",
    "\n",
    "    with rasterio.open(\n",
    "        filepath, 'w',\n",
    "        driver='GTiff',\n",
    "        height=grid_params['height'],\n",
    "        width=grid_params['width'],\n",
    "        count=1,\n",
    "        dtype=grid.dtype,\n",
    "        crs=grid_params['crs'],\n",
    "        transform=grid_params['transform'],\n",
    "        compress='lzw'\n",
    "    ) as dst:\n",
    "        dst.write(grid, 1)\n",
    "\n",
    "    print(f\"    Saved: {raster_type}/{filename}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECUTE PROCESSING FOR BOTH HURRICANES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING RASTERIZATION PROCESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process Francine\n",
    "process_hurricane('francine', francine_proj, francine_interval_counts, francine_time_bins, francine_timestamp_dict)\n",
    "\n",
    "# Process Helene\n",
    "process_hurricane('helene', helene_proj, helene_interval_counts, helene_time_bins, helene_timestamp_dict)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL PROCESSING COMPLETE! ✓\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "8649229f42c4e94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T16:53:50.683014Z",
     "start_time": "2025-10-18T16:53:50.670469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# STEP 5: POST-PROCESSING & ASSEMBLY\n",
    "# ==============================================================================\n",
    "import glob\n",
    "def create_metadata_index(hurricane_name, hurricane_dir):\n",
    "    \"\"\"Create CSV index of all rasters with metadata\"\"\"\n",
    "    print(f\"\\nCreating metadata index for {hurricane_name}...\")\n",
    "\n",
    "    # Get all increment and cumulative TIFFs\n",
    "    increment_files = sorted(glob.glob(os.path.join(hurricane_dir, f\"{hurricane_name}_increment_*.tif\")))\n",
    "    cumulative_files = sorted(glob.glob(os.path.join(hurricane_dir, f\"{hurricane_name}_cumulative_*.tif\")))\n",
    "\n",
    "    metadata_rows = []\n",
    "\n",
    "    for tif_path in increment_files + cumulative_files:\n",
    "        filename = os.path.basename(tif_path)\n",
    "\n",
    "        # Extract time and type from filename\n",
    "        parts = filename.replace('.tif', '').split('_')\n",
    "        raster_type = parts[-2]  # 'increment' or 'cumulative'\n",
    "        time_str = parts[-1]     # e.g., '20240910_0000'\n",
    "\n",
    "        # Open raster to get stats\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            data = src.read(1)\n",
    "\n",
    "            metadata_rows.append({\n",
    "                'filename': filename,\n",
    "                'type': raster_type,\n",
    "                'time_str': time_str,\n",
    "                'min_value': np.min(data),\n",
    "                'max_value': np.max(data),\n",
    "                'mean_value': np.mean(data),\n",
    "                'total_value': np.sum(data),\n",
    "                'non_zero_pixels': np.count_nonzero(data)\n",
    "            })\n",
    "\n",
    "    # Create DataFrame and save\n",
    "    metadata_df = pd.DataFrame(metadata_rows)\n",
    "    index_path = os.path.join(hurricane_dir, f\"{hurricane_name}_index.csv\")\n",
    "    metadata_df.to_csv(index_path, index=False)\n",
    "\n",
    "    print(f\"  Index saved: {index_path}\")\n",
    "    print(f\"  Total rasters cataloged: {len(metadata_rows)}\")\n",
    "\n",
    "    return metadata_df\n",
    "\n",
    "\n",
    "def create_vrt_stacks(hurricane_name, hurricane_dir):\n",
    "    \"\"\"Create VRT files using rasterio (no GDAL needed)\"\"\"\n",
    "    print(f\"\\nCreating VRT stacks for {hurricane_name}...\")\n",
    "\n",
    "    # Simply skip VRT creation or create a text-based reference file\n",
    "    increment_files = sorted(glob.glob(os.path.join(hurricane_dir, f\"{hurricane_name}_increment_*.tif\")))\n",
    "\n",
    "    # Create a simple text list file instead\n",
    "    list_file = os.path.join(hurricane_dir, f\"{hurricane_name}_increment_files.txt\")\n",
    "    with open(list_file, 'w') as f:\n",
    "        for file in increment_files:\n",
    "            f.write(file + '\\n')\n",
    "\n",
    "    print(f\"  Created file list: {hurricane_name}_increment_files.txt\")\n",
    "    print(f\"  Import these files directly in ArcGIS Pro\")\n",
    "\n",
    "def print_summary_stats(hurricane_name, hurricane_dir):\n",
    "    \"\"\"Print summary statistics for the hurricane dataset\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY: {hurricane_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    increment_files = glob.glob(os.path.join(hurricane_dir, f\"{hurricane_name}_increment_*.tif\"))\n",
    "    cumulative_files = glob.glob(os.path.join(hurricane_dir, f\"{hurricane_name}_cumulative_*.tif\"))\n",
    "\n",
    "    print(f\"  Total time slices: {len(increment_files)}\")\n",
    "    print(f\"  Increment rasters: {len(increment_files)}\")\n",
    "    print(f\"  Cumulative rasters: {len(cumulative_files)}\")\n",
    "    print(f\"  Output directory: {hurricane_dir}\")\n",
    "\n",
    "    # Get final cumulative stats\n",
    "    if cumulative_files:\n",
    "        final_cumulative = sorted(cumulative_files)[-1]\n",
    "        with rasterio.open(final_cumulative) as src:\n",
    "            final_data = src.read(1)\n",
    "            print(f\"\\n  Final Cumulative Statistics:\")\n",
    "            print(f\"    Total value: {np.sum(final_data):,.0f}\")\n",
    "            print(f\"    Max pixel value: {np.max(final_data):,.2f}\")\n",
    "            print(f\"    Active pixels: {np.count_nonzero(final_data):,}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# RUN POST-PROCESSING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: POST-PROCESSING & ASSEMBLY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Process Francine\n",
    "francine_dir = os.path.join(output_dir, 'francine')\n",
    "if os.path.exists(francine_dir):\n",
    "    francine_metadata = create_metadata_index('francine', francine_dir)\n",
    "    create_vrt_stacks('francine', francine_dir)\n",
    "    print_summary_stats('francine', francine_dir)\n",
    "\n",
    "# Process Helene\n",
    "helene_dir = os.path.join(output_dir, 'helene')\n",
    "if os.path.exists(helene_dir):\n",
    "    helene_metadata = create_metadata_index('helene', helene_dir)\n",
    "    create_vrt_stacks('helene', helene_dir)\n",
    "    print_summary_stats('helene', helene_dir)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POST-PROCESSING COMPLETE! ✓\")\n"
   ],
   "id": "7e5c84ddfb92e658",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: POST-PROCESSING & ASSEMBLY\n",
      "============================================================\n",
      "\n",
      "Creating metadata index for francine...\n",
      "  Index saved: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\francine\\francine_index.csv\n",
      "  Total rasters cataloged: 0\n",
      "\n",
      "Creating VRT stacks for francine...\n",
      "  Created file list: francine_increment_files.txt\n",
      "  Import these files directly in ArcGIS Pro\n",
      "\n",
      "============================================================\n",
      "SUMMARY: FRANCINE\n",
      "============================================================\n",
      "  Total time slices: 0\n",
      "  Increment rasters: 0\n",
      "  Cumulative rasters: 0\n",
      "  Output directory: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\francine\n",
      "\n",
      "Creating metadata index for helene...\n",
      "  Index saved: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\helene_index.csv\n",
      "  Total rasters cataloged: 0\n",
      "\n",
      "Creating VRT stacks for helene...\n",
      "  Created file list: helene_increment_files.txt\n",
      "  Import these files directly in ArcGIS Pro\n",
      "\n",
      "============================================================\n",
      "SUMMARY: HELENE\n",
      "============================================================\n",
      "  Total time slices: 0\n",
      "  Increment rasters: 0\n",
      "  Cumulative rasters: 0\n",
      "  Output directory: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\n",
      "\n",
      "============================================================\n",
      "POST-PROCESSING COMPLETE! ✓\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T16:53:51.474244Z",
     "start_time": "2025-10-18T16:53:50.689968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PLACE THIS CODE AFTER YOUR RASTER GENERATION IS COMPLETE\n",
    "# =============================================================================\n",
    "# HEATMAP POST-PROCESSING AND STYLING\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from rasterio.plot import show\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def apply_heatmap_smoothing(input_raster_path, output_raster_path, sigma_multiplier=2.0):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing to create heatmap effect\n",
    "\n",
    "    Parameters:\n",
    "    - sigma_multiplier: Controls smoothing intensity (higher = more blur)\n",
    "    \"\"\"\n",
    "    with rasterio.open(input_raster_path) as src:\n",
    "        data = src.read(1).astype(np.float32)\n",
    "        profile = src.profile.copy()\n",
    "\n",
    "        # Apply gaussian filter for heatmap smoothing\n",
    "        # Sigma is relative to cell size for consistent smoothing\n",
    "        sigma = sigma_multiplier  # pixels\n",
    "        smoothed_data = gaussian_filter(data, sigma=sigma, mode='constant', cval=0)\n",
    "\n",
    "        # Ensure no negative values\n",
    "        smoothed_data = np.maximum(smoothed_data, 0)\n",
    "\n",
    "        # Update profile for output\n",
    "        profile.update(dtype=rasterio.float32, compress='lzw')\n",
    "\n",
    "        # Write smoothed raster\n",
    "        with rasterio.open(output_raster_path, 'w', **profile) as dst:\n",
    "            dst.write(smoothed_data, 1)\n",
    "\n",
    "    print(f\"  Smoothed: {os.path.basename(output_raster_path)}\")\n",
    "\n",
    "    # Return stats\n",
    "    original_max = np.max(data)\n",
    "    smoothed_max = np.max(smoothed_data)\n",
    "    return original_max, smoothed_max\n",
    "\n",
    "def create_heatmap_versions(hurricane_dir, hurricane_name, sigma=2.0):\n",
    "    \"\"\"Create smoothed heatmap versions of all rasters\"\"\"\n",
    "    print(f\"\\nCreating heatmap versions for {hurricane_name}...\")\n",
    "\n",
    "    # Create heatmap subdirectory\n",
    "    heatmap_dir = os.path.join(hurricane_dir, 'heatmap')\n",
    "    os.makedirs(heatmap_dir, exist_ok=True)\n",
    "\n",
    "    # Process all cumulative rasters (these are usually more interesting for heatmaps)\n",
    "    cumulative_files = glob.glob(os.path.join(hurricane_dir, f\"{hurricane_name}_cumulative_*.tif\"))\n",
    "\n",
    "    stats = []\n",
    "    for tif_path in sorted(cumulative_files):\n",
    "        filename = os.path.basename(tif_path)\n",
    "        heatmap_filename = filename.replace('.tif', '_heatmap.tif')\n",
    "        heatmap_path = os.path.join(heatmap_dir, heatmap_filename)\n",
    "\n",
    "        orig_max, smooth_max = apply_heatmap_smoothing(tif_path, heatmap_path, sigma)\n",
    "        stats.append((filename, orig_max, smooth_max))\n",
    "\n",
    "    print(f\"  Created {len(cumulative_files)} heatmap rasters in: {heatmap_dir}\")\n",
    "\n",
    "    # Print smoothing statistics\n",
    "    print(f\"  Smoothing statistics:\")\n",
    "    for filename, orig_max, smooth_max in stats[:3]:  # Show first 3\n",
    "        print(f\"    {filename}: {orig_max:.1f} → {smooth_max:.1f} (max value)\")\n",
    "\n",
    "    return heatmap_dir\n",
    "\n",
    "def create_heatmap_colormap():\n",
    "    \"\"\"Create yellow-to-red heatmap colormap with transparent zero\"\"\"\n",
    "    colors = [\n",
    "        (0, 0, 0, 0),        # Transparent for 0\n",
    "        (1, 1, 0, 1),        # Yellow (low values)\n",
    "        (1, 0.75, 0, 1),     # Yellow-Orange\n",
    "        (1, 0.5, 0, 1),      # Orange\n",
    "        (1, 0.25, 0, 1),     # Orange-Red\n",
    "        (1, 0, 0, 1),        # Red (high values)\n",
    "        (0.55, 0, 0, 1),     # Dark Red (highest)\n",
    "    ]\n",
    "\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('heatmap', colors, N=256)\n",
    "    cmap.set_under(color=(0, 0, 0, 0))  # Ensure values <= 0 are transparent\n",
    "\n",
    "    return cmap\n",
    "\n",
    "def preview_heatmap(raster_path, title=\"Heatmap Preview\"):\n",
    "    \"\"\"Create a preview of the heatmap with proper styling\"\"\"\n",
    "\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        data = src.read(1)\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "        # Get custom colormap\n",
    "        cmap = create_heatmap_colormap()\n",
    "\n",
    "        # Set up visualization\n",
    "        # Use 95th percentile for max to avoid outliers dominating the scale\n",
    "        vmax = np.percentile(data[data > 0], 95) if np.any(data > 0) else np.max(data)\n",
    "        vmin = 0\n",
    "\n",
    "        # Display raster\n",
    "        show(src, ax=ax, cmap=cmap, vmin=vmin, vmax=vmax, alpha=0.8)\n",
    "\n",
    "        # Styling\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "        # Add colorbar\n",
    "        im = ax.images[0]\n",
    "        cbar = plt.colorbar(im, ax=ax, shrink=0.8, aspect=20)\n",
    "        cbar.set_label('Tweet Density', rotation=270, labelpad=20)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"  Data range: {np.min(data):.1f} - {np.max(data):.1f}\")\n",
    "        print(f\"  95th percentile: {vmax:.1f}\")\n",
    "        print(f\"  Non-zero pixels: {np.count_nonzero(data):,}\")\n",
    "\n",
    "def export_styled_geotiff(input_path, output_path, apply_colormap=True):\n",
    "    \"\"\"Export a styled GeoTIFF with embedded colormap for direct use in web maps\"\"\"\n",
    "\n",
    "    with rasterio.open(input_path) as src:\n",
    "        data = src.read(1)\n",
    "        profile = src.profile.copy()\n",
    "\n",
    "        if apply_colormap:\n",
    "            # Convert to 8-bit for colormap application\n",
    "            vmax = np.percentile(data[data > 0], 95) if np.any(data > 0) else np.max(data)\n",
    "            data_normalized = np.clip((data / vmax) * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "            # Update profile for 8-bit output\n",
    "            profile.update(\n",
    "                dtype=rasterio.uint8,\n",
    "                count=1,\n",
    "                compress='lzw'\n",
    "            )\n",
    "\n",
    "            # Write the normalized data\n",
    "            with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                dst.write(data_normalized, 1)\n",
    "\n",
    "                # Create and write colormap\n",
    "                cmap = create_heatmap_colormap()\n",
    "                colormap = {}\n",
    "                for i in range(256):\n",
    "                    rgba = cmap(i / 255.0)\n",
    "                    # Convert to 0-255 range\n",
    "                    colormap[i] = tuple(int(c * 255) for c in rgba)\n",
    "\n",
    "                dst.write_colormap(1, colormap)\n",
    "        else:\n",
    "            # Keep as float32\n",
    "            with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                dst.write(data, 1)\n",
    "\n",
    "    print(f\"  Exported styled raster: {os.path.basename(output_path)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTE HEATMAP PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def process_hurricane_heatmaps(hurricane_name, hurricane_dir, preview_latest=True):\n",
    "    \"\"\"Complete heatmap processing pipeline for a hurricane\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"HEATMAP PROCESSING: {hurricane_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Step 1: Create smoothed heatmap versions\n",
    "    heatmap_dir = create_heatmap_versions(hurricane_dir, hurricane_name, sigma=2.5)\n",
    "\n",
    "    # Step 2: Create web-ready styled versions\n",
    "    styled_dir = os.path.join(hurricane_dir, 'styled')\n",
    "    os.makedirs(styled_dir, exist_ok=True)\n",
    "\n",
    "    heatmap_files = glob.glob(os.path.join(heatmap_dir, \"*.tif\"))\n",
    "\n",
    "    for heatmap_file in sorted(heatmap_files):\n",
    "        filename = os.path.basename(heatmap_file)\n",
    "        styled_filename = filename.replace('_heatmap.tif', '_styled.tif')\n",
    "        styled_path = os.path.join(styled_dir, styled_filename)\n",
    "\n",
    "        export_styled_geotiff(heatmap_file, styled_path, apply_colormap=True)\n",
    "\n",
    "    # Step 3: Preview the latest heatmap\n",
    "    if preview_latest and heatmap_files:\n",
    "        latest_heatmap = sorted(heatmap_files)[-1]\n",
    "        preview_heatmap(latest_heatmap, f\"{hurricane_name.title()} - Latest Cumulative Heatmap\")\n",
    "\n",
    "    print(f\"\\n✓ {hurricane_name.upper()} heatmap processing complete!\")\n",
    "    print(f\"  Smoothed rasters: {heatmap_dir}\")\n",
    "    print(f\"  Styled rasters: {styled_dir}\")\n",
    "\n",
    "    return heatmap_dir, styled_dir\n",
    "\n",
    "# Run processing for your hurricanes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HEATMAP POST-PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Process both hurricanes\n",
    "francine_dir = os.path.join(output_dir, 'francine')\n",
    "helene_dir = os.path.join(output_dir, 'helene')\n",
    "\n",
    "if os.path.exists(francine_dir):\n",
    "    francine_heatmap_dir, francine_styled_dir = process_hurricane_heatmaps('francine', francine_dir)\n",
    "\n",
    "if os.path.exists(helene_dir):\n",
    "    helene_heatmap_dir, helene_styled_dir = process_hurricane_heatmaps('helene', helene_dir)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL HEATMAP PROCESSING COMPLETE! 🔥\")\n"
   ],
   "id": "7ac549035d07e970",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HEATMAP POST-PROCESSING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "HEATMAP PROCESSING: FRANCINE\n",
      "============================================================\n",
      "\n",
      "Creating heatmap versions for francine...\n",
      "  Created 0 heatmap rasters in: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\francine\\heatmap\n",
      "  Smoothing statistics:\n",
      "\n",
      "✓ FRANCINE heatmap processing complete!\n",
      "  Smoothed rasters: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\francine\\heatmap\n",
      "  Styled rasters: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\francine\\styled\n",
      "\n",
      "============================================================\n",
      "HEATMAP PROCESSING: HELENE\n",
      "============================================================\n",
      "\n",
      "Creating heatmap versions for helene...\n",
      "  Created 0 heatmap rasters in: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\heatmap\n",
      "  Smoothing statistics:\n",
      "\n",
      "✓ HELENE heatmap processing complete!\n",
      "  Smoothed rasters: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\heatmap\n",
      "  Styled rasters: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\styled\n",
      "\n",
      "============================================================\n",
      "ALL HEATMAP PROCESSING COMPLETE! 🔥\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T16:53:51.561716Z",
     "start_time": "2025-10-18T16:53:51.483632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import arcpy\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Paths\n",
    "\n",
    "raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\cumulative\"\n",
    "gdb_path = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\Tweet_project.gdb\"\n",
    "mosaic_name = \"helene_cumulative_mosaic_v2\"\n",
    "\n",
    "# Create geodatabase if it doesn't exist\n",
    "if not arcpy.Exists(gdb_path):\n",
    "    arcpy.CreateFileGDB_management(os.path.dirname(gdb_path), os.path.basename(gdb_path))\n",
    "\n",
    "# Create mosaic dataset\n",
    "mosaic_path = os.path.join(gdb_path, mosaic_name)\n",
    "if arcpy.Exists(mosaic_path):\n",
    "    arcpy.Delete_management(mosaic_path)\n",
    "\n",
    "arcpy.CreateMosaicDataset_management(gdb_path, mosaic_name, \"PROJCS['WGS_1984_Web_Mercator_Auxiliary_Sphere',GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Mercator_Auxiliary_Sphere'],PARAMETER['False_Easting',0.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',0.0],PARAMETER['Standard_Parallel_1',0.0],PARAMETER['Auxiliary_Sphere_Type',0.0],UNIT['Meter',1.0]]\")\n",
    "\n",
    "print(f\"Created mosaic dataset: {mosaic_path}\")\n",
    "\n",
    "# Add rasters to mosaic\n",
    "arcpy.AddRastersToMosaicDataset_management(\n",
    "    mosaic_path,\n",
    "    \"Raster Dataset\",\n",
    "    raster_folder,\n",
    "    filter=\"*.tif\"\n",
    ")\n",
    "\n",
    "print(\"Added rasters to mosaic dataset\")\n",
    "\n",
    "# Add time field\n",
    "arcpy.AddField_management(mosaic_path, \"date\", \"DATE\")\n",
    "\n",
    "# Calculate time from filename\n",
    "with arcpy.da.UpdateCursor(mosaic_path, [\"Name\", \"date\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        filename = row[0]\n",
    "        # Remove .tif extension and split\n",
    "        parts = filename.replace(\".tif\", \"\").split(\"_\")\n",
    "\n",
    "        # Join last two parts to get full timestamp: 20240926 + 080000\n",
    "        time_str = parts[-2] + parts[-1]  # Combines date and time\n",
    "\n",
    "        # Parse: 20240926080000 -> datetime\n",
    "        dt = datetime.strptime(time_str, \"%Y%m%d%H%M%S\")\n",
    "        print(f\"{filename} -> {time_str} -> {dt}\")\n",
    "        row[1] = dt\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Time field populated\")\n",
    "\n",
    "# Configure mosaic properties\n",
    "arcpy.SetMosaicDatasetProperties_management(\n",
    "    mosaic_path,\n",
    "    start_time_field=\"date\"\n",
    ")\n",
    "\n",
    "print(\"Mosaic dataset configured with time dimension\")\n",
    "\n",
    "print(f\"\\nMosaic dataset complete: {mosaic_path}\")\n",
    "print(\"To apply symbology in ArcGIS Pro:\")\n",
    "print(f\"1. Add mosaic to map: {mosaic_path}\")\n",
    "print(f\"2. Right-click layer > Symbology > Import\")\n",
    "print(f\"3. Select: {symbology_file}\")\n",
    "print(\"4. Enable time slider to animate cumulative growth\")"
   ],
   "id": "25a47a2355a4f113",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arcpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01marcpy\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdatetime\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m datetime\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'arcpy'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ae94f045c90be546"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import arcpy\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Paths\n",
    "\n",
    "raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\increment\"\n",
    "gdb_path = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\Tweet_project.gdb\"\n",
    "mosaic_name = \"helene_cumulative_mosaic_v3\"\n",
    "coordinate_system = \"PROJCS['WGS_1984_Web_Mercator_Auxiliary_Sphere',GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Mercator_Auxiliary_Sphere'],PARAMETER['False_Easting',0.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',0.0],PARAMETER['Standard_Parallel_1',0.0],PARAMETER['Auxiliary_Sphere_Type',0.0],UNIT['Meter',1.0]]\"\n",
    "# Create geodatabase if it doesn't exist\n",
    "if not arcpy.Exists(gdb_path):\n",
    "    arcpy.CreateFileGDB_management(os.path.dirname(gdb_path), os.path.basename(gdb_path))\n",
    "\n",
    "# Create mosaic dataset\n",
    "# Create mosaic dataset with the CORRECT pixel type\n",
    "mosaic_path = os.path.join(gdb_path, mosaic_name)\n",
    "if arcpy.Exists(mosaic_path):\n",
    "    arcpy.Delete_management(mosaic_path)\n",
    "\n",
    "# --- THIS IS THE LINE TO CHANGE ---\n",
    "arcpy.CreateMosaicDataset_management(\n",
    "    in_workspace=gdb_path,\n",
    "    in_mosaicdataset_name=mosaic_name,\n",
    "    coordinate_system=coordinate_system,\n",
    "    num_bands=1, # Add this line\n",
    "    pixel_type='32_BIT_FLOAT' # Add this line\n",
    ")\n",
    "print(f\"Created mosaic dataset: {mosaic_path}\")\n",
    "\n",
    "# Add rasters to mosaic\n",
    "arcpy.AddRastersToMosaicDataset_management(\n",
    "    mosaic_path,\n",
    "    \"Raster Dataset\",\n",
    "    raster_folder,\n",
    "    filter=\"*.tif\"\n",
    ")\n",
    "\n",
    "print(\"Added rasters to mosaic dataset\")\n",
    "\n",
    "# Add time field\n",
    "arcpy.AddField_management(mosaic_path, \"date\", \"DATE\")\n",
    "\n",
    "# Calculate time from filename\n",
    "# Calculate time from filename\n",
    "# Calculate time from filename\n",
    "with arcpy.da.UpdateCursor(mosaic_path, [\"Name\", \"date\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        filename = row[0]\n",
    "        # Remove .tif extension and split\n",
    "        parts = filename.replace(\".tif\", \"\").split(\"_\")\n",
    "\n",
    "        # Join last two parts to get full timestamp: 20240926 + 080000\n",
    "        time_str = parts[-2] + parts[-1]  # Combines date and time\n",
    "\n",
    "        # Parse: 20240926080000 -> datetime\n",
    "        dt = datetime.strptime(time_str, \"%Y%m%d%H%M%S\")\n",
    "        print(f\"{filename} -> {time_str} -> {dt}\")\n",
    "        row[1] = dt\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Time field populated\")\n",
    "\n",
    "# Configure mosaic properties\n",
    "arcpy.SetMosaicDatasetProperties_management(\n",
    "    mosaic_path,\n",
    "    start_time_field=\"date\"\n",
    ")\n",
    "\n",
    "print(\"Mosaic dataset configured with time dimension\")\n",
    "arcpy.CalculateStatistics_management(mosaic_path)\n",
    "print(f\"\\nMosaic dataset complete: {mosaic_path}\")\n",
    "print(\"To apply symbology in ArcGIS Pro:\")\n",
    "print(f\"1. Add mosaic to map: {mosaic_path}\")\n",
    "print(f\"2. Right-click layer > Symbology > Import\")\n",
    "print(f\"3. Select: {symbology_file}\")\n",
    "print(\"4. Enable time slider to animate cumulative growth\")"
   ],
   "id": "722378782c07ba6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89b9760dec1b9b93"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
