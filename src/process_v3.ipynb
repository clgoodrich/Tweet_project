{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# !pip install fuzzywuzzy python-Levenshtein geopandas pandas numpy matplotlib",
   "id": "203a6780cd79d505",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# This cell handles the initial loading and preparation of the hurricane tweet data.\n",
    "# Key steps include:\n",
    "# 1. Importing necessary libraries for data manipulation, file paths, and time handling.\n",
    "# 2. Constructing file paths to the GeoJSON data for Hurricanes Francine and Helene.\n",
    "# 3. Loading the spatial data into GeoDataFrames.\n",
    "# 4. Standardizing all timestamps to Coordinated Universal Time (UTC).\n",
    "# 5. Aggregating the data into discrete 4-hour time bins for temporal analysis.\n",
    "# 6. Creating various time-related columns (Unix timestamps, readable labels) for later use.\n",
    "# =============================================================================\n",
    "\n",
    "# Import core libraries\n",
    "import geopandas as gpd  # Used for working with geospatial data.\n",
    "import pandas as pd      # Used for data manipulation and analysis in DataFrames.\n",
    "import os                # Provides a way of using operating system dependent functionality, like file paths.\n",
    "from datetime import datetime, timezone # Used for handling date and time objects.\n",
    "\n",
    "# --- 1. Load GeoJSON files ---\n",
    "# Get the parent directory of the current working directory to build relative paths.\n",
    "# This makes the script more portable as it doesn't rely on a hardcoded absolute path.\n",
    "local_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Define the relative paths to the GeoJSON files for each hurricane.\n",
    "francine_dir = r\"\\data\\geojson\\francine.geojson\"\n",
    "helene_dir = r\"\\data\\geojson\\helene.geojson\"\n",
    "\n",
    "# Combine the base path and relative directory to create full, absolute paths to the files.\n",
    "francine_path = f\"{local_path}{francine_dir}\"\n",
    "helene_path = f\"{local_path}{helene_dir}\"\n",
    "\n",
    "# --- 2. Load data into GeoDataFrames ---\n",
    "# A GeoDataFrame is a pandas DataFrame with a special 'geometry' column that allows for spatial operations.\n",
    "francine_gdf = gpd.read_file(francine_path)\n",
    "helene_gdf = gpd.read_file(helene_path)\n",
    "\n",
    "# --- 3. Standardize timestamps to UTC ---\n",
    "# Convert the original 'time' column into a pandas datetime object.\n",
    "# Setting `utc=True` ensures all timestamps are in a single, unambiguous timezone (UTC).\n",
    "# This is crucial for accurate temporal comparisons and binning.\n",
    "francine_gdf['timestamp'] = pd.to_datetime(francine_gdf['time'], utc=True)\n",
    "helene_gdf['timestamp'] = pd.to_datetime(helene_gdf['time'], utc=True)\n",
    "\n",
    "# --- 4. Group data into 4-hour time bins ---\n",
    "# The `dt.floor('4h')` function rounds each timestamp *down* to the nearest 4-hour interval.\n",
    "# For example, 09:35 becomes 08:00, 15:59 becomes 12:00. This aggregates tweets into discrete time windows.\n",
    "francine_gdf['time_bin'] = francine_gdf['timestamp'].dt.floor('4h')\n",
    "helene_gdf['time_bin'] = helene_gdf['timestamp'].dt.floor('4h')\n",
    "\n",
    "# --- 5. Create Unix timestamps and lookup dictionaries ---\n",
    "# Convert the binned datetime objects into Unix timestamps (as an integer).\n",
    "# The `// 1000` division is likely to convert from nanoseconds or microseconds to seconds, a more standard Unix format.\n",
    "francine_gdf['unix_timestamp'] = francine_gdf['time_bin'].astype('int64') // 1000\n",
    "helene_gdf['unix_timestamp'] = helene_gdf['time_bin'].astype('int64') // 1000\n",
    "\n",
    "# Create dictionaries to map the numeric Unix timestamp back to its original datetime object.\n",
    "# This provides a quick way to retrieve the readable time bin later in the script without recalculating it.\n",
    "helene_timestamp_dict = dict(zip(helene_gdf['unix_timestamp'], helene_gdf['time_bin']))\n",
    "francine_timestamp_dict = dict(zip(francine_gdf['unix_timestamp'], francine_gdf['time_bin']))\n",
    "\n",
    "# --- 6. Create readable labels for file naming ---\n",
    "# The `dt.strftime` function formats the datetime object into a specific string format.\n",
    "# Here, '%Y%m%d_%H%M' creates a clean, sortable label like '20240926_0800', which is ideal for filenames.\n",
    "francine_gdf['bin_label'] = francine_gdf['time_bin'].dt.strftime('%Y%m%d_%H%M')\n",
    "helene_gdf['bin_label'] = helene_gdf['time_bin'].dt.strftime('%Y%m%d_%H%M')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load reference shapefiles\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "\n",
    "states_dir = r\"\\data\\shape_files\\cb_2023_us_state_20m.shp\"\n",
    "counties_dir = r\"\\data\\shape_files\\cb_2023_us_county_20m.shp\"\n",
    "cities_dir = r\"\\data\\shape_files\\US_Cities.shp\"\n",
    "states_path = f\"{local_path}{states_dir}\"\n",
    "counties_path = f\"{local_path}{counties_dir}\"\n",
    "cities_path = f\"{local_path}{cities_dir}\"\n",
    "\n",
    "\n",
    "# Load spatial reference data\n",
    "states_gdf = gpd.read_file(states_path)\n",
    "counties_gdf = gpd.read_file(counties_path)\n",
    "cities_gdf = gpd.read_file(cities_path)\n",
    "\n",
    "# =============================================================================\n",
    "# MULTI-LEVEL GEOGRAPHIC MATCHING (SINGLE BEST PER ENTITY: STATE>COUNTY>CITY)\n",
    "# with conservative guards against false positives (e.g., FRANCINE→FRANKLIN)\n",
    "# =============================================================================\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "\n",
    "# --- Safeguards ---------------------------------------------------------------\n",
    "BLOCKLIST_EXACT = {\n",
    "    # hurricane/event terms and generic weather tokens to ignore unless exact place\n",
    "    'FRANCINE', 'HELENE', 'HURRICANE', 'STORM', 'TROPICAL', 'TS', 'CYCLONE',\n",
    "    'FLOOD', 'RAIN', 'WIND', 'SURGE', 'LANDFALL', 'EYE', 'TRACK', 'BAND'\n",
    "}\n",
    "\n",
    "def is_blocklisted(entity: str) -> bool:\n",
    "    e = (entity or '').strip().upper()\n",
    "    return e in BLOCKLIST_EXACT\n",
    "\n",
    "def preprocess_place_name(name):\n",
    "    \"\"\"Standardize place names for better matching.\"\"\"\n",
    "    if pd.isna(name) or name == 'NAN':\n",
    "        return None\n",
    "    name = str(name).upper().strip()\n",
    "    name = re.sub(r'\\bST\\.?\\b', 'SAINT', name)\n",
    "    name = re.sub(r'\\bMT\\.?\\b', 'MOUNT', name)\n",
    "    name = re.sub(r'\\bFT\\.?\\b', 'FORT', name)\n",
    "    name = re.sub(r'\\bN\\.?\\b', 'NORTH', name)\n",
    "    name = re.sub(r'\\bS\\.?\\b', 'SOUTH', name)\n",
    "    name = re.sub(r'\\bE\\.?\\b', 'EAST', name)\n",
    "    name = re.sub(r'\\bW\\.?\\b', 'WEST', name)\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    return name.strip()\n",
    "\n",
    "def parse_gpe_entities(gpe_string):\n",
    "    \"\"\"Split the GPE field into potential place names.\"\"\"\n",
    "    if not gpe_string or pd.isna(gpe_string) or str(gpe_string).strip() == '':\n",
    "        return []\n",
    "    gpe_string = str(gpe_string).strip()\n",
    "    entities = []\n",
    "    for part in [p.strip() for p in gpe_string.split(',')]:\n",
    "        if not part:\n",
    "            continue\n",
    "        for sub in re.split(r'[;&|]', part):\n",
    "            sub = preprocess_place_name(sub)\n",
    "            if sub and len(sub) > 1:\n",
    "                entities.append(sub)\n",
    "    # remove dups\n",
    "    seen, clean = set(), []\n",
    "    for e in entities:\n",
    "        if e not in seen:\n",
    "            clean.append(e)\n",
    "            seen.add(e)\n",
    "    return clean\n",
    "\n",
    "def create_hierarchical_lookups(states_gdf, counties_gdf, cities_gdf):\n",
    "    \"\"\"Build dictionaries for all levels.\"\"\"\n",
    "    print(\"\\nCreating hierarchical lookup dictionaries...\")\n",
    "\n",
    "    state_lookup, state_abbrev_to_name, state_name_to_abbrev = {}, {}, {}\n",
    "    for _, row in states_gdf.iterrows():\n",
    "        sname = preprocess_place_name(row['NAME'])\n",
    "        if not sname:\n",
    "            continue\n",
    "        state_lookup[sname] = row.geometry\n",
    "        if 'STUSPS' in row:\n",
    "            abbr = str(row['STUSPS']).upper()\n",
    "            state_abbrev_to_name[abbr] = sname\n",
    "            state_name_to_abbrev[sname] = abbr\n",
    "            state_lookup[abbr] = row.geometry\n",
    "\n",
    "    county_lookup, county_by_state = {}, {}\n",
    "    for _, row in counties_gdf.iterrows():\n",
    "        cname = preprocess_place_name(row['NAME'])\n",
    "        sfips = row.get('STATEFP', '')\n",
    "        if not cname:\n",
    "            continue\n",
    "        county_lookup[cname] = row.geometry\n",
    "        sname = None\n",
    "        if 'STATE_NAME' in row:\n",
    "            sname = preprocess_place_name(row['STATE_NAME'])\n",
    "        else:\n",
    "            for _, srow in states_gdf.iterrows():\n",
    "                if srow.get('STATEFP', '') == sfips:\n",
    "                    sname = preprocess_place_name(srow['NAME'])\n",
    "                    break\n",
    "        if sname:\n",
    "            county_by_state.setdefault(sname, {})[cname] = row.geometry\n",
    "\n",
    "    city_lookup, city_by_state = {}, {}\n",
    "    for _, row in cities_gdf.iterrows():\n",
    "        cname = preprocess_place_name(row['NAME'])\n",
    "        stabbr = str(row.get('ST', '')).upper()\n",
    "        if not cname:\n",
    "            continue\n",
    "        city_lookup[cname] = row.geometry\n",
    "        if stabbr in state_abbrev_to_name:\n",
    "            sfull = state_abbrev_to_name[stabbr]\n",
    "            city_by_state.setdefault(sfull, {})[cname] = row.geometry\n",
    "\n",
    "    return {\n",
    "        'state_lookup': state_lookup,\n",
    "        'county_lookup': county_lookup,\n",
    "        'city_lookup': city_lookup,\n",
    "        'county_by_state': county_by_state,\n",
    "        'city_by_state': city_by_state,\n",
    "        'state_abbrev_to_name': state_abbrev_to_name,\n",
    "        'state_name_to_abbrev': state_name_to_abbrev\n",
    "    }\n",
    "\n",
    "# --- Smart fuzzy matching (with conservative guards) --------------------------\n",
    "def _best_name_by_ratio(entity, names):\n",
    "    \"\"\"Get best candidate name by fuzz.ratio; returns (name, ratio) or (None,0).\"\"\"\n",
    "    if not names:\n",
    "        return (None, 0)\n",
    "    match = process.extractOne(entity, names, scorer=fuzz.ratio)\n",
    "    return (match[0], match[1]) if match else (None, 0)\n",
    "\n",
    "def fuzzy_match_best(entity, candidates, base_threshold):\n",
    "    \"\"\"\n",
    "    Return (name, geom, score) for the best candidate meeting conservative checks:\n",
    "      - Blocklisted tokens: only allow EXACT match (no fuzzy).\n",
    "      - Single-token with len>=6: ONLY exact match (prevents FRANCINE→FRANKLIN).\n",
    "      - Otherwise require BOTH ratio and token_set_ratio >= threshold.\n",
    "    \"\"\"\n",
    "    if not entity or not candidates:\n",
    "        return None, None, 0\n",
    "\n",
    "    # Exact fast-path\n",
    "    if entity in candidates:\n",
    "        return entity, candidates[entity], 100\n",
    "\n",
    "    # Blocklist: refuse fuzzy for these tokens\n",
    "    if is_blocklisted(entity):\n",
    "        return None, None, 0\n",
    "\n",
    "    # Single-token long strings: exact only\n",
    "    if ' ' not in entity and len(entity) >= 6:\n",
    "        return None, None, 0\n",
    "\n",
    "    # Compute best by ratio, then validate with token_set_ratio\n",
    "    names = list(candidates.keys())\n",
    "    best_name, ratio_score = _best_name_by_ratio(entity, names)\n",
    "    if not best_name:\n",
    "        return None, None, 0\n",
    "\n",
    "    set_score = fuzz.token_set_ratio(entity, best_name)\n",
    "\n",
    "    # Dynamic tightening for very short tokens (reduce false hits)\n",
    "    threshold = base_threshold\n",
    "    if len(entity) <= 3:\n",
    "        threshold = max(threshold, 95)\n",
    "    elif len(entity) <= 5:\n",
    "        threshold = max(threshold, 90)\n",
    "\n",
    "    if ratio_score >= threshold and set_score >= threshold:\n",
    "        return best_name, candidates[best_name], int((ratio_score + set_score) / 2)\n",
    "\n",
    "    return None, None, 0\n",
    "\n",
    "def resolve_best_for_entity(entity, lookups, state_context):\n",
    "    \"\"\"\n",
    "    Pick ONE best match with priority: STATE > COUNTY > CITY.\n",
    "    COUNTY/CITY biased by any known states in state_context.\n",
    "    \"\"\"\n",
    "    st_lu = lookups['state_lookup']\n",
    "    co_lu = lookups['county_lookup']\n",
    "    ci_lu = lookups['city_lookup']\n",
    "    co_by_state = lookups['county_by_state']\n",
    "    ci_by_state = lookups['city_by_state']\n",
    "\n",
    "    # 1) STATE (exact or USPS abbrev or conservative fuzzy)\n",
    "    n, g, sc = fuzzy_match_best(entity, st_lu, 85)  # states: higher threshold\n",
    "    if sc > 0:\n",
    "        return ('STATE', n, g, sc)\n",
    "\n",
    "    # 2) COUNTY (prefer in-state)\n",
    "    best = (None, None, 0)\n",
    "    for s in state_context:\n",
    "        if s in co_by_state:\n",
    "            n2, g2, sc2 = fuzzy_match_best(entity, co_by_state[s], 85)\n",
    "            if sc2 > best[2]:\n",
    "                best = (n2, g2, sc2)\n",
    "    if best[2] == 0:\n",
    "        n2, g2, sc2 = fuzzy_match_best(entity, co_lu, 90)  # global county stricter\n",
    "        if sc2 > best[2]:\n",
    "            best = (n2, g2, sc2)\n",
    "    if best[2] > 0:\n",
    "        return ('COUNTY', best[0], best[1], best[2])\n",
    "\n",
    "    # 3) CITY (prefer in-state)\n",
    "    best = (None, None, 0)\n",
    "    for s in state_context:\n",
    "        if s in ci_by_state:\n",
    "            n3, g3, sc3 = fuzzy_match_best(entity, ci_by_state[s], 85)\n",
    "            if sc3 > best[2]:\n",
    "                best = (n3, g3, sc3)\n",
    "    if best[2] == 0:\n",
    "        n3, g3, sc3 = fuzzy_match_best(entity, ci_lu, 90)  # global city stricter\n",
    "        if sc3 > best[2]:\n",
    "            best = (n3, g3, sc3)\n",
    "    if best[2] > 0:\n",
    "        return ('CITY', best[0], best[1], best[2])\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_all_geographic_matches_single_per_entity(entities, lookups):\n",
    "    \"\"\"Return one best match per entity, using STATE>COUNTY>CITY and state context.\"\"\"\n",
    "    if not entities:\n",
    "        return []\n",
    "\n",
    "    # Build state context from the entities themselves (helps county/city later).\n",
    "    state_context = set()\n",
    "    for e in entities:\n",
    "        n, g, sc = fuzzy_match_best(e, lookups['state_lookup'], 85)\n",
    "        if sc > 0 and n:\n",
    "            state_context.add(n)\n",
    "\n",
    "    results, seen = [], set()\n",
    "    for e in entities:\n",
    "        m = resolve_best_for_entity(e, lookups, state_context)\n",
    "        if not m:\n",
    "            continue\n",
    "        key = (m[0], m[1])  # (scale, name)\n",
    "        if key not in seen:\n",
    "            results.append(m)\n",
    "            seen.add(key)\n",
    "    return results\n",
    "\n",
    "def multi_level_assign_scale_levels(row, lookups):\n",
    "    \"\"\"Return list of matches [(scale,name,geom,score), ...] for this tweet.\"\"\"\n",
    "    gpe = str(row.get('GPE', '')).strip()\n",
    "    fac = str(row.get('FAC', '')).strip()\n",
    "    matches = []\n",
    "\n",
    "    entities = parse_gpe_entities(gpe)\n",
    "    if entities:\n",
    "        geo_matches = find_all_geographic_matches_single_per_entity(entities, lookups)\n",
    "        matches.extend(geo_matches)\n",
    "\n",
    "    if fac and fac not in ['nan', 'NAN', '']:\n",
    "        matches.append(('FACILITY', preprocess_place_name(fac), row.geometry, 100))\n",
    "\n",
    "    if not matches:\n",
    "        matches.append(('UNMATCHED', None, row.geometry, 0))\n",
    "    return matches\n",
    "\n",
    "def expand_tweets_by_matches(gdf, lookups, dataset_name):\n",
    "    print(f\"\\nExpanding {dataset_name} tweets by geographic matches...\")\n",
    "    expanded_rows = []\n",
    "    for i, row in gdf.iterrows():\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        for scale, name, geom, score in multi_level_assign_scale_levels(row, lookups):\n",
    "            new_row = row.copy()\n",
    "            new_row['scale_level'] = scale\n",
    "            new_row['matched_name'] = name\n",
    "            new_row['matched_geom'] = geom\n",
    "            new_row['match_score'] = score\n",
    "            new_row['original_index'] = i\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    expanded_gdf = gpd.GeoDataFrame(expanded_rows, crs=gdf.crs)\n",
    "    print(\"  Sample multi-level matches:\")\n",
    "    multi = expanded_gdf.groupby('original_index').size()\n",
    "    idxs = multi[multi > 1].head(5).index\n",
    "    for j in idxs:\n",
    "        t = expanded_gdf[expanded_gdf['original_index'] == j]\n",
    "        gpe = t.iloc[0]['GPE']\n",
    "        summ = ', '.join([f\"{r['scale_level']}:{r['matched_name']}\" for _, r in t.iterrows()])\n",
    "        print(f\"    '{gpe}' → {summ}\")\n",
    "    return expanded_gdf\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTE MULTI-LEVEL FUZZY MATCHING\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-LEVEL GEOGRAPHIC MATCHING (STATE>COUNTY>CITY, conservative)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lookups = create_hierarchical_lookups(states_gdf, counties_gdf, cities_gdf)\n",
    "francine_gdf = expand_tweets_by_matches(francine_gdf, lookups, \"FRANCINE\")\n",
    "helene_gdf   = expand_tweets_by_matches(helene_gdf, lookups, \"HELENE\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-LEVEL FUZZY MATCHING COMPLETE ✓\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGuards active: blocklist, exact-only for long single tokens, dual-score thresholds.\")\n"
   ],
   "id": "102cf233829866a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # Group tweets by 4-hour intervals and scale level\n",
    "# Using unix_timestamp for unambiguous temporal grouping\n",
    "\n",
    "# Alternative approach:\n",
    "francine_interval_counts = francine_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).agg({\n",
    "    'matched_geom': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Add count column separately\n",
    "count_series = francine_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).size()\n",
    "francine_interval_counts['count'] = count_series.values\n",
    "\n",
    "# Same for Helene\n",
    "helene_interval_counts = helene_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).agg({\n",
    "    'matched_geom': 'first'\n",
    "}).reset_index()\n",
    "count_series = helene_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).size()\n",
    "helene_interval_counts['count'] = count_series.values\n",
    "\n",
    "# Sort by timestamp to ensure chronological order\n",
    "francine_interval_counts = francine_interval_counts.sort_values('unix_timestamp')\n",
    "helene_interval_counts = helene_interval_counts.sort_values('unix_timestamp')\n",
    "\n",
    "# Calculate cumulative counts\n",
    "francine_interval_counts['cumulative_count'] = francine_interval_counts.groupby(['scale_level', 'matched_name'])['count'].cumsum()\n",
    "helene_interval_counts['cumulative_count'] = helene_interval_counts.groupby(['scale_level', 'matched_name'])['count'].cumsum()\n",
    "\n",
    "# Get unique time bins for iteration\n",
    "francine_time_bins = sorted(francine_gdf['unix_timestamp'].unique())\n",
    "helene_time_bins = sorted(helene_gdf['unix_timestamp'].unique())\n",
    "\n"
   ],
   "id": "95d5ae7c58fc2af4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: DEFINE MASTER GRID CANVAS\n",
    "# ==============================================================================\n",
    "\n",
    "# Configuration\n",
    "TARGET_CRS = 'EPSG:3857'  # Web Mercator\n",
    "CELL_SIZE_M = 1000  # 5 km in meters\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: CREATING MASTER GRID CANVAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Project both datasets to target CRS\n",
    "print(f\"\\nProjecting datasets to {TARGET_CRS}...\")\n",
    "francine_proj = francine_gdf.to_crs(TARGET_CRS)\n",
    "helene_proj = helene_gdf.to_crs(TARGET_CRS)\n",
    "\n",
    "# Also project reference geometries\n",
    "print(\"Projecting reference geometries...\")\n",
    "states_proj = states_gdf.to_crs(TARGET_CRS)\n",
    "counties_proj = counties_gdf.to_crs(TARGET_CRS)\n",
    "cities_proj = cities_gdf.to_crs(TARGET_CRS)\n",
    "# Calculate combined extent from both hurricanes\"\n",
    "print(\"\\nCalculating master extent...\")\n",
    "francine_bounds = francine_proj.total_bounds\n",
    "helene_bounds = helene_proj.total_bounds\n",
    "\n",
    "# Get union of both bounding boxes\n",
    "minx = min(francine_bounds[0], helene_bounds[0])\n",
    "miny = min(francine_bounds[1], helene_bounds[1])\n",
    "maxx = max(francine_bounds[2], helene_bounds[2])\n",
    "maxy = max(francine_bounds[3], helene_bounds[3])\n",
    "#\n",
    "# print(f\"  Master bounds (EPSG:3857):\")\n",
    "# print(f\"    minx: {minx:,.2f}\")\n",
    "# print(f\"    miny: {miny:,.2f}\")\n",
    "# print(f\"    maxx: {maxx:,.2f}\")\n",
    "# print(f\"    maxy: {maxy:,.2f}\")\n",
    "\n",
    "# Calculate grid dimensions\n",
    "width = int(np.ceil((maxx - minx) / CELL_SIZE_M))\n",
    "height = int(np.ceil((maxy - miny) / CELL_SIZE_M))\n",
    "\n",
    "print(f\"\\nGrid Configuration:\")\n",
    "print(f\"  Cell size: {CELL_SIZE_M:,} meters ({CELL_SIZE_M/1000} km)\")\n",
    "print(f\"  Grid dimensions: {width} x {height} cells\")\n",
    "print(f\"  Total cells: {width * height:,}\")\n",
    "\n",
    "# Create master transform\n",
    "master_transform = from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "print(f\"\\nMaster Transform:\")\n",
    "print(f\"  {master_transform}\")\n",
    "\n",
    "# Calculate actual coverage area\n",
    "area_km2 = (width * height * CELL_SIZE_M * CELL_SIZE_M) / 1_000_000\n",
    "print(f\"\\nCoverage area: {area_km2:,.2f} km²\")\n",
    "\n",
    "# Store grid parameters for later use\n",
    "grid_params = {\n",
    "    'crs': TARGET_CRS,\n",
    "    'cell_size': CELL_SIZE_M,\n",
    "    'width': width,\n",
    "    'height': height,\n",
    "    'bounds': (minx, miny, maxx, maxy),\n",
    "    'transform': master_transform\n",
    "}\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"MASTER GRID CANVAS READY ✓\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "# Update lookup dictionaries with projected geometries\n",
    "print(\"\\nUpdating geometry lookups with projected coordinates...\")\n",
    "state_lookup_proj = dict(zip(states_proj['NAME'].str.upper(), states_proj.geometry))\n",
    "county_lookup_proj = dict(zip(counties_proj['NAME'].str.upper(), counties_proj.geometry))\n",
    "cities_lookup_proj = dict(zip(cities_proj['NAME'].str.upper(), cities_proj.geometry))\n",
    "# validation_results = validate_city_matching(francine_gdf, helene_gdf, lookups['city_lookup'], lookups['state_lookup'], lookups['county_lookup'])\n",
    "print(\"Lookup dictionaries updated with projected geometries ✓\")"
   ],
   "id": "5bd77653518eadb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T05:07:56.113725Z",
     "start_time": "2025-11-02T05:07:48.599931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "from KDEpy import FFTKDE\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: MAIN RASTERIZATION LOOP - TIME ITERATION (UNCHANGED CORE, BONUS ADDED)\n",
    "# ==============================================================================\n",
    "\n",
    "# Create output directories\n",
    "rasters_dir = r\"\\rasters_output\"\n",
    "output_dir = f\"{local_path}{rasters_dir}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def create_hierarchical_rasters(data, grid_params, time_bin):\n",
    "    \"\"\"Create hierarchically weighted rasters with automatic parent state inclusion\"\"\"\n",
    "    print(f\"    Creating hierarchical raster for time {time_bin}...\")\n",
    "\n",
    "    output_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "    states_to_include = set()  # Track which states need base layers\n",
    "\n",
    "    # 1. States (identify base layers to include)\n",
    "    state_data = data[data['scale_level'] == 'STATE']\n",
    "    if len(state_data) > 0:\n",
    "        states_to_include.update(state_data['matched_name'].unique())\n",
    "\n",
    "    # 1b. Counties -> parent states\n",
    "    county_data = data[data['scale_level'] == 'COUNTY']\n",
    "    for county_name in county_data['matched_name'].unique():\n",
    "        if county_name in county_lookup_proj:\n",
    "            county_geom = county_lookup_proj[county_name]\n",
    "            for state_name, state_geom in state_lookup_proj.items():\n",
    "                if state_geom.contains(county_geom.centroid):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "\n",
    "    # 1c. Cities -> parent states\n",
    "    city_data = data[data['scale_level'] == 'CITY']\n",
    "    for city_name in city_data['matched_name'].unique():\n",
    "        if city_name in cities_lookup_proj:\n",
    "            city_geom = cities_lookup_proj[city_name]\n",
    "            for state_name, state_geom in state_lookup_proj.items():\n",
    "                if state_geom.contains(city_geom.centroid):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "\n",
    "    # 2. Rasterize states\n",
    "    for state_name in states_to_include:\n",
    "        if state_name in state_lookup_proj:\n",
    "            state_geom = state_lookup_proj[state_name]\n",
    "            mask = rasterize(\n",
    "                [(state_geom, 1)],\n",
    "                out_shape=(grid_params['height'], grid_params['width']),\n",
    "                transform=grid_params['transform'],\n",
    "                fill=0, dtype=np.float32, all_touched=True\n",
    "            )\n",
    "            if state_name in state_data['matched_name'].values:\n",
    "                tweet_count = state_data[state_data['matched_name'] == state_name]['count'].sum()\n",
    "            else:\n",
    "                tweet_count = 1  # minimal base\n",
    "            base_value = np.log1p(tweet_count) * 2\n",
    "            output_grid += mask * base_value\n",
    "\n",
    "    # 3. Add counties (fill)\n",
    "    if len(county_data) > 0:\n",
    "        county_counts = county_data.groupby('matched_name')['count'].sum()\n",
    "        for county_name, tweet_count in county_counts.items():\n",
    "            if county_name in county_lookup_proj:\n",
    "                mask = rasterize(\n",
    "                    [(county_lookup_proj[county_name], 1)],\n",
    "                    out_shape=(grid_params['height'], grid_params['width']),\n",
    "                    transform=grid_params['transform'],\n",
    "                    fill=0, dtype=np.float32, all_touched=True\n",
    "                )\n",
    "                output_grid += mask * np.log1p(tweet_count) * 5\n",
    "\n",
    "    # 4. Add cities (polygon fill as you already had)\n",
    "    if len(city_data) > 0:\n",
    "        city_counts = city_data.groupby('matched_name')['count'].sum()\n",
    "        for city_name, tweet_count in city_counts.items():\n",
    "            if city_name in cities_lookup_proj:\n",
    "                mask = rasterize(\n",
    "                    [(cities_lookup_proj[city_name], 1)],\n",
    "                    out_shape=(grid_params['height'], grid_params['width']),\n",
    "                    transform=grid_params['transform'],\n",
    "                    fill=0, dtype=np.float32, all_touched=True\n",
    "                )\n",
    "                output_grid += mask * np.log1p(tweet_count) * 10\n",
    "\n",
    "    # 5. Facilities (unchanged)\n",
    "    facility_data = data[data['scale_level'] == 'FACILITY']\n",
    "    if len(facility_data) > 0:\n",
    "        output_grid += create_facility_raster(data, grid_params)\n",
    "\n",
    "    return output_grid\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# BONUS: KDEpy (FFTKDE) city-centroid KDE per-bin and cumulative\n",
    "# ==============================================================================\n",
    "\n",
    "def _city_centroids_and_weights(city_data):\n",
    "    \"\"\"\n",
    "    Return (N,2) array of centroid coordinates (already projected) and weights.\n",
    "    Falls back to .centroid for polygons; passes through points. Skips invalids.\n",
    "    \"\"\"\n",
    "    xs, ys, ws = [], [], []\n",
    "    if len(city_data) == 0:\n",
    "        return None, None\n",
    "\n",
    "    city_counts = city_data.groupby('matched_name')['count'].sum()\n",
    "\n",
    "    for city_name, w in city_counts.items():\n",
    "        geom = cities_lookup_proj.get(city_name)\n",
    "        if geom is None:\n",
    "            continue\n",
    "        if isinstance(geom, (Polygon, MultiPolygon)):\n",
    "            c = geom.centroid\n",
    "        elif isinstance(geom, Point):\n",
    "            c = geom\n",
    "        else:\n",
    "            c = getattr(geom, 'centroid', None)\n",
    "            if c is None:\n",
    "                continue\n",
    "        xs.append(c.x)\n",
    "        ys.append(c.y)\n",
    "        ws.append(float(w))\n",
    "    if len(xs) == 0:\n",
    "        return None, None\n",
    "    xy = np.column_stack([np.array(xs, dtype='float64'),\n",
    "                          np.array(ys, dtype='float64')])\n",
    "    w = np.array(ws, dtype='float64')\n",
    "    return xy, w\n",
    "\n",
    "\n",
    "def _evaluate_fftkde_on_grid(xy, weights, grid_params, bw_meters=10000.0):\n",
    "    \"\"\"\n",
    "    Evaluate FFTKDE on a rectilinear, lexicographically-sorted grid of cell centers.\n",
    "    Returns float32 array aligned to raster orientation (row 0 = top).\n",
    "    \"\"\"\n",
    "    H, W = grid_params['height'], grid_params['width']\n",
    "    xmin, ymin, xmax, ymax = grid_params['bounds']\n",
    "    cs = grid_params['cell_size']\n",
    "\n",
    "    # 1) Build cell-center coordinates, strictly ASCENDING (equidistant)\n",
    "    xs = xmin + (np.arange(W) + 0.5) * cs              # left -> right\n",
    "    ys = ymin + (np.arange(H) + 0.5) * cs              # bottom -> top\n",
    "\n",
    "    # 2) Cartesian product of (x, y) with lexicographic sort by (x, y)\n",
    "    #    NOTE: product order from meshgrid (x fast) then sort ensures monotonicity\n",
    "    X, Y = np.meshgrid(xs, ys, indexing='xy')          # X shape (H,W), Y shape (H,W)\n",
    "    eval_pts = np.column_stack([X.ravel(order='F'), Y.ravel(order='F')])  # x changes slow, y fast\n",
    "    # Now sort by x then y to satisfy KDEpy's grid_is_sorted()\n",
    "    order = np.lexsort((eval_pts[:, 1], eval_pts[:, 0]))\n",
    "    eval_pts_sorted = eval_pts[order]\n",
    "\n",
    "    # 3) Fit KDE (bandwidth in same linear units as xy: meters)\n",
    "    kde = FFTKDE(kernel='gaussian', bw=float(bw_meters)).fit(\n",
    "        xy.astype('float64'), weights=weights.astype('float64')\n",
    "    )\n",
    "\n",
    "    # 4) Evaluate on the sorted grid of query points (returns flat array)\n",
    "    z_flat = kde.evaluate(eval_pts_sorted)\n",
    "\n",
    "    # 5) Undo the sort to original Fortran-ordered layout, then reshape to (H, W)\n",
    "    inv_order = np.empty_like(order)\n",
    "    inv_order[order] = np.arange(order.size)\n",
    "    z_unsorted = z_flat[inv_order]\n",
    "\n",
    "    # Our eval grid used order='F' (column-major): reshape accordingly, then flipud\n",
    "    z = z_unsorted.reshape(H, W, order='F')\n",
    "    z = np.flipud(z).astype('float32')                 # top row = maxY\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "def process_hurricane(hurricane_name, gdf_proj, interval_counts, time_bins, timestamp_dict):\n",
    "    \"\"\"\n",
    "    Process a single hurricane through all time bins\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"PROCESSING: {hurricane_name.upper()}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "    print(gdf_proj)\n",
    "\n",
    "    # hurricane output root\n",
    "    hurricane_dir = os.path.join(output_dir, hurricane_name.lower())\n",
    "    os.makedirs(hurricane_dir, exist_ok=True)\n",
    "\n",
    "    # Main cumulative (your existing)\n",
    "    cumulative_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "    # BONUS cumulative KDE\n",
    "    cumulative_city_kde = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "\n",
    "    # Tunables for KDEpy\n",
    "    CITY_KDE_BW_METERS = 10000.0  # ~10 km smoothing; adjust as needed\n",
    "\n",
    "    for idx, time_bin in enumerate(time_bins):\n",
    "        # Filter this bin\n",
    "        current_data = interval_counts[interval_counts['unix_timestamp'] == time_bin]\n",
    "\n",
    "        # Your existing per-bin surface\n",
    "        incremental_grid = create_hierarchical_rasters(current_data, grid_params, time_bin)\n",
    "\n",
    "        # === BONUS: KDEpy city-centroid KDE (iterative) ===\n",
    "        city_bin = current_data[current_data['scale_level'] == 'CITY']\n",
    "        xy, w = _city_centroids_and_weights(city_bin)\n",
    "\n",
    "        if xy is not None:\n",
    "            city_iter_kde = _evaluate_fftkde_on_grid(\n",
    "                xy, w, grid_params, bw_meters=CITY_KDE_BW_METERS\n",
    "            )\n",
    "            cumulative_city_kde += city_iter_kde\n",
    "        else:\n",
    "            city_iter_kde = np.zeros_like(cumulative_city_kde, dtype=np.float32)\n",
    "\n",
    "        # Update main cumulative\n",
    "        cumulative_grid += incremental_grid\n",
    "\n",
    "        # Save (existing)\n",
    "        save_raster(incremental_grid, hurricane_dir, hurricane_name, time_bin, 'increment', timestamp_dict)\n",
    "        save_raster(cumulative_grid, hurricane_dir, hurricane_name, time_bin, 'cumulative', timestamp_dict)\n",
    "\n",
    "        # Save BONUS KDEpy rasters\n",
    "        save_raster(city_iter_kde, hurricane_dir, hurricane_name, time_bin, 'city_kde_fftkde_increment', timestamp_dict)\n",
    "        save_raster(cumulative_city_kde, hurricane_dir, hurricane_name, time_bin, 'city_kde_fftkde_cumulative', timestamp_dict)\n",
    "\n",
    "        print(f\"  Incremental max:       {np.max(incremental_grid):.3f}\")\n",
    "        print(f\"  Cumulative max:        {np.max(cumulative_grid):.3f}\")\n",
    "        print(f\"  City KDE (iter) max:   {np.max(city_iter_kde):.6f}\")\n",
    "        print(f\"  City KDE (cum) max:    {np.max(cumulative_city_kde):.6f}\")\n",
    "\n",
    "    print(f\"\\n{hurricane_name.upper()} processing complete!\")\n",
    "    print(f\"Output saved to: {hurricane_dir}\")\n",
    "    return\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# PLACEHOLDER FUNCTIONS (UNCHANGED)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_facility_raster(data, grid_params):\n",
    "    \"\"\"Create KDE raster for facility points with strong hotspot multiplier\"\"\"\n",
    "    print(\"    [FACILITY] Creating facility raster...\")\n",
    "\n",
    "    facility_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "    facility_data = data[data['scale_level'] == 'FACILITY']\n",
    "    if len(facility_data) == 0:\n",
    "        print(\"      No facility-level tweets in this time bin\")\n",
    "        return facility_grid\n",
    "\n",
    "    facility_counts = facility_data.groupby('matched_name')['count'].sum()\n",
    "    sigma_meters = 2 * grid_params['cell_size']\n",
    "    sigma_pixels = sigma_meters / grid_params['cell_size']\n",
    "    facility_multiplier = 10\n",
    "\n",
    "    facilities_processed = 0\n",
    "    for facility_name, tweet_count in facility_counts.items():\n",
    "        rows = facility_data[facility_data['matched_name'] == facility_name]\n",
    "        if len(rows) == 0:\n",
    "            continue\n",
    "        facility_point = rows.iloc[0]['matched_geom']\n",
    "        if hasattr(facility_point, 'x') and hasattr(facility_point, 'y'):\n",
    "            point_geoseries = gpd.GeoSeries([facility_point], crs='EPSG:4326')\n",
    "            point_proj = point_geoseries.to_crs(grid_params['crs']).iloc[0]\n",
    "            px = (point_proj.x - grid_params['bounds'][0]) / grid_params['cell_size']\n",
    "            py = (grid_params['bounds'][3] - point_proj.y) / grid_params['cell_size']\n",
    "            if 0 <= px < grid_params['width'] and 0 <= py < grid_params['height']:\n",
    "                point_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "                point_grid[int(py), int(px)] = tweet_count\n",
    "                # simple gaussian via pixels (kept as-is)\n",
    "                from scipy.ndimage import gaussian_filter\n",
    "                kernel_grid = gaussian_filter(point_grid, sigma=sigma_pixels, mode='constant', cval=0)\n",
    "                facility_grid += kernel_grid * facility_multiplier\n",
    "                facilities_processed += 1\n",
    "            else:\n",
    "                print(f\"      WARNING: Facility '{facility_name}' outside grid bounds\")\n",
    "        else:\n",
    "            print(f\"      WARNING: Invalid geometry for facility '{facility_name}'\")\n",
    "\n",
    "    print(f\"      Processed {facilities_processed} facilities with sigma={sigma_pixels:.2f} pixels\")\n",
    "    return facility_grid\n",
    "\n",
    "\n",
    "def save_raster(grid, output_dir, hurricane_name, time_bin, raster_type, timestamp_dict):\n",
    "    \"\"\"Save raster as GeoTIFF in type-specific subdirectory\"\"\"\n",
    "    type_dir = os.path.join(output_dir, raster_type)\n",
    "    os.makedirs(type_dir, exist_ok=True)\n",
    "    print('max grid', np.max(grid))\n",
    "    time_str = timestamp_dict[time_bin].strftime('%Y%m%d_%H%M%S')\n",
    "    print([time_str])\n",
    "    filename = f\"{hurricane_name}_tweets_{time_str}.tif\"\n",
    "    filepath = os.path.join(type_dir, filename)\n",
    "    print(grid_params)\n",
    "    with rasterio.open(\n",
    "        filepath, 'w',\n",
    "        driver='GTiff',\n",
    "        height=grid_params['height'],\n",
    "        width=grid_params['width'],\n",
    "        count=1,\n",
    "        dtype=grid.dtype,\n",
    "        crs=grid_params['crs'],\n",
    "        transform=grid_params['transform'],\n",
    "        compress='lzw'\n",
    "    ) as dst:\n",
    "        dst.write(grid, 1)\n",
    "    print(f\"    Saved: {raster_type}/{filename}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECUTE PROCESSING FOR BOTH HURRICANES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING RASTERIZATION PROCESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "process_hurricane('francine', francine_proj, francine_interval_counts, francine_time_bins, francine_timestamp_dict)\n",
    "process_hurricane('helene', helene_proj, helene_interval_counts, helene_time_bins, helene_timestamp_dict)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL PROCESSING COMPLETE! ✓\")\n",
    "print(\"=\" * 60)\n"
   ],
   "id": "2e1bc16e47f51e00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING RASTERIZATION PROCESS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING: FRANCINE\n",
      "============================================================\n",
      "\n",
      "     FAC LOC                   GPE                      time   Latitude  \\\n",
      "0                        Louisiana 2024-09-10 23:58:43+00:00  30.870388   \n",
      "1                      New Orleans 2024-09-10 23:56:22+00:00  29.975998   \n",
      "2                        Louisiana 2024-09-10 23:55:12+00:00  30.870388   \n",
      "3                        Louisiana 2024-09-10 23:54:54+00:00  30.870388   \n",
      "4              Francine, Louisiana 2024-09-10 23:52:34+00:00  29.961202   \n",
      "...   ..  ..                   ...                       ...        ...   \n",
      "2299          Lafayette, Louisiana 2024-09-10 10:30:14+00:00  30.226219   \n",
      "2299          Lafayette, Louisiana 2024-09-10 10:30:14+00:00  30.226219   \n",
      "2300                     Louisiana 2024-09-10 10:23:56+00:00  30.870388   \n",
      "2301                     Louisiana 2024-09-10 09:56:03+00:00  30.870388   \n",
      "2302                     Louisiana 2024-09-10 09:39:36+00:00  30.870388   \n",
      "\n",
      "      Longitude  make_polygon                           geometry  \\\n",
      "0    -92.007126             1  POINT (-10242186.416 3615927.986)   \n",
      "1    -90.078213             0  POINT (-10027460.769 3500465.022)   \n",
      "2    -92.007126             1  POINT (-10242186.416 3615927.986)   \n",
      "3    -92.007126             1  POINT (-10242186.416 3615927.986)   \n",
      "4    -90.206790             0  POINT (-10041773.934 3498563.736)   \n",
      "...         ...           ...                                ...   \n",
      "2299 -92.017820             0  POINT (-10243376.889 3532661.417)   \n",
      "2299 -92.017820             0  POINT (-10243376.889 3532661.417)   \n",
      "2300 -92.007126             1  POINT (-10242186.416 3615927.986)   \n",
      "2301 -92.007126             1  POINT (-10242186.416 3615927.986)   \n",
      "2302 -92.007126             1  POINT (-10242186.416 3615927.986)   \n",
      "\n",
      "                     timestamp                  time_bin  unix_timestamp  \\\n",
      "0    2024-09-10 23:58:43+00:00 2024-09-10 20:00:00+00:00      1725998400   \n",
      "1    2024-09-10 23:56:22+00:00 2024-09-10 20:00:00+00:00      1725998400   \n",
      "2    2024-09-10 23:55:12+00:00 2024-09-10 20:00:00+00:00      1725998400   \n",
      "3    2024-09-10 23:54:54+00:00 2024-09-10 20:00:00+00:00      1725998400   \n",
      "4    2024-09-10 23:52:34+00:00 2024-09-10 20:00:00+00:00      1725998400   \n",
      "...                        ...                       ...             ...   \n",
      "2299 2024-09-10 10:30:14+00:00 2024-09-10 08:00:00+00:00      1725955200   \n",
      "2299 2024-09-10 10:30:14+00:00 2024-09-10 08:00:00+00:00      1725955200   \n",
      "2300 2024-09-10 10:23:56+00:00 2024-09-10 08:00:00+00:00      1725955200   \n",
      "2301 2024-09-10 09:56:03+00:00 2024-09-10 08:00:00+00:00      1725955200   \n",
      "2302 2024-09-10 09:39:36+00:00 2024-09-10 08:00:00+00:00      1725955200   \n",
      "\n",
      "          bin_label scale_level matched_name  \\\n",
      "0     20240910_2000       STATE    LOUISIANA   \n",
      "1     20240910_2000        CITY  NEW ORLEANS   \n",
      "2     20240910_2000       STATE    LOUISIANA   \n",
      "3     20240910_2000       STATE    LOUISIANA   \n",
      "4     20240910_2000       STATE    LOUISIANA   \n",
      "...             ...         ...          ...   \n",
      "2299  20240910_0800      COUNTY    LAFAYETTE   \n",
      "2299  20240910_0800       STATE    LOUISIANA   \n",
      "2300  20240910_0800       STATE    LOUISIANA   \n",
      "2301  20240910_0800       STATE    LOUISIANA   \n",
      "2302  20240910_0800       STATE    LOUISIANA   \n",
      "\n",
      "                                           matched_geom  match_score  \\\n",
      "0     POLYGON ((-94.0430515276176 32.6930299766656, ...          100   \n",
      "1     MULTIPOLYGON (((-10017501.047895897 3489812.12...          100   \n",
      "2     POLYGON ((-94.0430515276176 32.6930299766656, ...          100   \n",
      "3     POLYGON ((-94.0430515276176 32.6930299766656, ...          100   \n",
      "4     POLYGON ((-94.0430515276176 32.6930299766656, ...          100   \n",
      "...                                                 ...          ...   \n",
      "2299  POLYGON ((-92.247873 30.211337, -92.142184 30....          100   \n",
      "2299  POLYGON ((-94.0430515276176 32.6930299766656, ...          100   \n",
      "2300  POLYGON ((-94.0430515276176 32.6930299766656, ...          100   \n",
      "2301  POLYGON ((-94.0430515276176 32.6930299766656, ...          100   \n",
      "2302  POLYGON ((-94.0430515276176 32.6930299766656, ...          100   \n",
      "\n",
      "      original_index  \n",
      "0                  0  \n",
      "1                  1  \n",
      "2                  2  \n",
      "3                  3  \n",
      "4                  4  \n",
      "...              ...  \n",
      "2299            2299  \n",
      "2299            2299  \n",
      "2300            2300  \n",
      "2301            2301  \n",
      "2302            2302  \n",
      "\n",
      "[2752 rows x 17 columns]\n",
      "    Creating hierarchical raster for time 1725868800...\n",
      "max grid 4.394449\n",
      "['20240909_080000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: increment/francine_tweets_20240909_080000.tif\n",
      "max grid 4.394449\n",
      "['20240909_080000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: cumulative/francine_tweets_20240909_080000.tif\n",
      "max grid 0.0\n",
      "['20240909_080000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: city_kde_fftkde_increment/francine_tweets_20240909_080000.tif\n",
      "max grid 0.0\n",
      "['20240909_080000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: city_kde_fftkde_cumulative/francine_tweets_20240909_080000.tif\n",
      "  Incremental max:       4.394\n",
      "  Cumulative max:        4.394\n",
      "  City KDE (iter) max:   0.000000\n",
      "  City KDE (cum) max:    0.000000\n",
      "    Creating hierarchical raster for time 1725883200...\n",
      "max grid 8.050703\n",
      "['20240909_120000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: increment/francine_tweets_20240909_120000.tif\n",
      "max grid 12.445152\n",
      "['20240909_120000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: cumulative/francine_tweets_20240909_120000.tif\n",
      "max grid 0.0\n",
      "['20240909_120000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: city_kde_fftkde_increment/francine_tweets_20240909_120000.tif\n",
      "max grid 0.0\n",
      "['20240909_120000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: city_kde_fftkde_cumulative/francine_tweets_20240909_120000.tif\n",
      "  Incremental max:       8.051\n",
      "  Cumulative max:        12.445\n",
      "  City KDE (iter) max:   0.000000\n",
      "  City KDE (cum) max:    0.000000\n",
      "    Creating hierarchical raster for time 1725897600...\n",
      "    [FACILITY] Creating facility raster...\n",
      "      Processed 1 facilities with sigma=2.00 pixels\n",
      "max grid 12.407104\n",
      "['20240909_160000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: increment/francine_tweets_20240909_160000.tif\n",
      "max grid 23.139366\n",
      "['20240909_160000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: cumulative/francine_tweets_20240909_160000.tif\n",
      "max grid 0.0\n",
      "['20240909_160000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: city_kde_fftkde_increment/francine_tweets_20240909_160000.tif\n",
      "max grid 0.0\n",
      "['20240909_160000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: city_kde_fftkde_cumulative/francine_tweets_20240909_160000.tif\n",
      "  Incremental max:       12.407\n",
      "  Cumulative max:        23.139\n",
      "  City KDE (iter) max:   0.000000\n",
      "  City KDE (cum) max:    0.000000\n",
      "    Creating hierarchical raster for time 1725912000...\n",
      "max grid 8.286269\n",
      "['20240909_200000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: increment/francine_tweets_20240909_200000.tif\n",
      "max grid 31.425636\n",
      "['20240909_200000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: cumulative/francine_tweets_20240909_200000.tif\n",
      "max grid 0.0\n",
      "['20240909_200000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: city_kde_fftkde_increment/francine_tweets_20240909_200000.tif\n",
      "max grid 0.0\n",
      "['20240909_200000']\n",
      "{'crs': 'EPSG:3857', 'cell_size': 1000, 'width': 3364, 'height': 2195, 'bounds': (np.float64(-11854083.11341075), np.float64(2947395.714660338), np.float64(-8490833.94284347), np.float64(5142357.362880709)), 'transform': Affine(np.float64(999.7768045681569), np.float64(0.0), np.float64(-11854083.11341075),\n",
      "       np.float64(0.0), np.float64(-999.9825276630389), np.float64(5142357.362880709))}\n",
      "    Saved: city_kde_fftkde_cumulative/francine_tweets_20240909_200000.tif\n",
      "  Incremental max:       8.286\n",
      "  Cumulative max:        31.426\n",
      "  City KDE (iter) max:   0.000000\n",
      "  City KDE (cum) max:    0.000000\n",
      "    Creating hierarchical raster for time 1725926400...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to solve for support numerically. Use a kernel with finite support or scale data to smaller bw.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\KDEpy\\FFTKDE.py:181\u001B[39m, in \u001B[36mFFTKDE.evaluate\u001B[39m\u001B[34m(self, grid_points)\u001B[39m\n\u001B[32m    180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m181\u001B[39m     real_bw = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_kernel_practical_support\u001B[49m\n\u001B[32m    182\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n",
      "\u001B[31mAttributeError\u001B[39m: 'FFTKDE' object has no attribute '_kernel_practical_support'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\KDEpy\\kernel_funcs.py:298\u001B[39m, in \u001B[36mKernel.practical_support\u001B[39m\u001B[34m(self, bw, atol)\u001B[39m\n\u001B[32m    297\u001B[39m xtol = \u001B[32m1e-3\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m298\u001B[39m ans = \u001B[43mbrentq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m8\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[43mbw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxtol\u001B[49m\u001B[43m=\u001B[49m\u001B[43mxtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_output\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    299\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m ans + xtol\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\scipy\\optimize\\_zeros_py.py:846\u001B[39m, in \u001B[36mbrentq\u001B[39m\u001B[34m(f, a, b, args, xtol, rtol, maxiter, full_output, disp)\u001B[39m\n\u001B[32m    845\u001B[39m f = _wrap_nan_raise(f)\n\u001B[32m--> \u001B[39m\u001B[32m846\u001B[39m r = \u001B[43m_zeros\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_brentq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrtol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaxiter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    847\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m results_c(full_output, r, \u001B[33m\"\u001B[39m\u001B[33mbrentq\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mValueError\u001B[39m: f(a) and f(b) must have different signs",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 323\u001B[39m\n\u001B[32m    320\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mSTARTING RASTERIZATION PROCESS\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    321\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m60\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m323\u001B[39m \u001B[43mprocess_hurricane\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mfrancine\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrancine_proj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrancine_interval_counts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrancine_time_bins\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrancine_timestamp_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    324\u001B[39m process_hurricane(\u001B[33m'\u001B[39m\u001B[33mhelene\u001B[39m\u001B[33m'\u001B[39m, helene_proj, helene_interval_counts, helene_time_bins, helene_timestamp_dict)\n\u001B[32m    326\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m60\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 215\u001B[39m, in \u001B[36mprocess_hurricane\u001B[39m\u001B[34m(hurricane_name, gdf_proj, interval_counts, time_bins, timestamp_dict)\u001B[39m\n\u001B[32m    212\u001B[39m xy, w = _city_centroids_and_weights(city_bin)\n\u001B[32m    214\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m xy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m215\u001B[39m     city_iter_kde = \u001B[43m_evaluate_fftkde_on_grid\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    216\u001B[39m \u001B[43m        \u001B[49m\u001B[43mxy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbw_meters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCITY_KDE_BW_METERS\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    218\u001B[39m     cumulative_city_kde += city_iter_kde\n\u001B[32m    219\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 167\u001B[39m, in \u001B[36m_evaluate_fftkde_on_grid\u001B[39m\u001B[34m(xy, weights, grid_params, bw_meters)\u001B[39m\n\u001B[32m    162\u001B[39m kde = FFTKDE(kernel=\u001B[33m'\u001B[39m\u001B[33mgaussian\u001B[39m\u001B[33m'\u001B[39m, bw=\u001B[38;5;28mfloat\u001B[39m(bw_meters)).fit(\n\u001B[32m    163\u001B[39m     xy.astype(\u001B[33m'\u001B[39m\u001B[33mfloat64\u001B[39m\u001B[33m'\u001B[39m), weights=weights.astype(\u001B[33m'\u001B[39m\u001B[33mfloat64\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    164\u001B[39m )\n\u001B[32m    166\u001B[39m \u001B[38;5;66;03m# 4) Evaluate on the sorted grid of query points (returns flat array)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m167\u001B[39m z_flat = \u001B[43mkde\u001B[49m\u001B[43m.\u001B[49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43meval_pts_sorted\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    169\u001B[39m \u001B[38;5;66;03m# 5) Undo the sort to original Fortran-ordered layout, then reshape to (H, W)\u001B[39;00m\n\u001B[32m    170\u001B[39m inv_order = np.empty_like(order)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\KDEpy\\FFTKDE.py:183\u001B[39m, in \u001B[36mFFTKDE.evaluate\u001B[39m\u001B[34m(self, grid_points)\u001B[39m\n\u001B[32m    181\u001B[39m         real_bw = \u001B[38;5;28mself\u001B[39m._kernel_practical_support\n\u001B[32m    182\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m183\u001B[39m         real_bw = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkernel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpractical_support\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    185\u001B[39m \u001B[38;5;66;03m# Compute L, the number of dx'es to move out from 0 in kernel\u001B[39;00m\n\u001B[32m    186\u001B[39m L = np.minimum(np.floor(real_bw / dx), num_intervals + \u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\KDEpy\\kernel_funcs.py:305\u001B[39m, in \u001B[36mKernel.practical_support\u001B[39m\u001B[34m(self, bw, atol)\u001B[39m\n\u001B[32m    300\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[32m    301\u001B[39m     msg = (\n\u001B[32m    302\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUnable to solve for support numerically. Use a \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    303\u001B[39m         + \u001B[33m\"\u001B[39m\u001B[33mkernel with finite support or scale data to smaller bw.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    304\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m305\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n",
      "\u001B[31mValueError\u001B[39m: Unable to solve for support numerically. Use a kernel with finite support or scale data to smaller bw."
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import arcpy\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Note this is to be inserted into the python command window\n",
    "# Paths\n",
    "gdb_path = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\Tweet_project.gdb\"\n",
    "\n",
    "\n",
    "raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\cumulative\"\n",
    "mosaic_name = \"helene_cumulative_mosaic_v2\"\n",
    "\n",
    "raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\increment\"\n",
    "mosaic_name = \"helene_increment_mosaic_v2\"\n",
    "\n",
    "raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\francine\\cumulative\"\n",
    "mosaic_name = \"francine_cumulative_mosaic_v2\"\n",
    "\n",
    "raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\francine\\increment\"\n",
    "mosaic_name = \"francine_increment_mosaic_v2\"\n",
    "\n",
    "\n",
    "\n",
    "# Create geodatabase if it doesn't exist\n",
    "if not arcpy.Exists(gdb_path):\n",
    "    arcpy.CreateFileGDB_management(os.path.dirname(gdb_path), os.path.basename(gdb_path))\n",
    "\n",
    "# Create mosaic dataset\n",
    "mosaic_path = os.path.join(gdb_path, mosaic_name)\n",
    "if arcpy.Exists(mosaic_path):\n",
    "    arcpy.Delete_management(mosaic_path)\n",
    "\n",
    "arcpy.CreateMosaicDataset_management(gdb_path, mosaic_name, \"PROJCS['WGS_1984_Web_Mercator_Auxiliary_Sphere',GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Mercator_Auxiliary_Sphere'],PARAMETER['False_Easting',0.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',0.0],PARAMETER['Standard_Parallel_1',0.0],PARAMETER['Auxiliary_Sphere_Type',0.0],UNIT['Meter',1.0]]\")\n",
    "\n",
    "print(f\"Created mosaic dataset: {mosaic_path}\")\n",
    "\n",
    "# Add rasters to mosaic\n",
    "arcpy.AddRastersToMosaicDataset_management(\n",
    "    mosaic_path,\n",
    "    \"Raster Dataset\",\n",
    "    raster_folder,\n",
    "    filter=\"*.tif\"\n",
    ")\n",
    "\n",
    "print(\"Added rasters to mosaic dataset\")\n",
    "\n",
    "# Add time field\n",
    "arcpy.AddField_management(mosaic_path, \"date\", \"DATE\")\n",
    "\n",
    "# Calculate time from filename\n",
    "with arcpy.da.UpdateCursor(mosaic_path, [\"Name\", \"date\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        filename = row[0]\n",
    "        # Remove .tif extension and split\n",
    "        parts = filename.replace(\".tif\", \"\").split(\"_\")\n",
    "\n",
    "        # Join last two parts to get full timestamp: 20240926 + 080000\n",
    "        time_str = parts[-2] + parts[-1]  # Combines date and time\n",
    "\n",
    "        # Parse: 20240926080000 -> datetime\n",
    "        dt = datetime.strptime(time_str, \"%Y%m%d%H%M%S\")\n",
    "        print(f\"{filename} -> {time_str} -> {dt}\")\n",
    "        row[1] = dt\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Time field populated\")\n",
    "\n",
    "# Configure mosaic properties\n",
    "arcpy.SetMosaicDatasetProperties_management(\n",
    "    mosaic_path,\n",
    "    start_time_field=\"date\"\n",
    ")\n",
    "\n",
    "print(\"Mosaic dataset configured with time dimension\")\n",
    "\n",
    "print(f\"\\nMosaic dataset complete: {mosaic_path}\")\n",
    "print(\"To apply symbology in ArcGIS Pro:\")\n",
    "print(f\"1. Add mosaic to map: {mosaic_path}\")\n",
    "print(f\"2. Right-click layer > Symbology > Import\")\n",
    "print(f\"3. Select: {symbology_file}\")\n",
    "print(\"4. Enable time slider to animate cumulative growth\")"
   ],
   "id": "80b96b516fb40db0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
