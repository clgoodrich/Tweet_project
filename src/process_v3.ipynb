{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# !pip install fuzzywuzzy python-Levenshtein geopandas pandas numpy matplotlib",
   "id": "203a6780cd79d505",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# This cell handles the initial loading and preparation of the hurricane tweet data.\n",
    "# Key steps include:\n",
    "# 1. Importing necessary libraries for data manipulation, file paths, and time handling.\n",
    "# 2. Constructing file paths to the GeoJSON data for Hurricanes Francine and Helene.\n",
    "# 3. Loading the spatial data into GeoDataFrames.\n",
    "# 4. Standardizing all timestamps to Coordinated Universal Time (UTC).\n",
    "# 5. Aggregating the data into discrete 4-hour time bins for temporal analysis.\n",
    "# 6. Creating various time-related columns (Unix timestamps, readable labels) for later use.\n",
    "# =============================================================================\n",
    "\n",
    "# Import core libraries\n",
    "import geopandas as gpd  # Used for working with geospatial data.\n",
    "import pandas as pd      # Used for data manipulation and analysis in DataFrames.\n",
    "import os                # Provides a way of using operating system dependent functionality, like file paths.\n",
    "from datetime import datetime, timezone # Used for handling date and time objects. \n",
    "\n",
    "# --- 1. Load GeoJSON files ---\n",
    "# Get the parent directory of the current working directory to build relative paths.\n",
    "# This makes the script more portable as it doesn't rely on a hardcoded absolute path.\n",
    "local_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Define the relative paths to the GeoJSON files for each hurricane.\n",
    "francine_dir = r\"\\data\\geojson\\francine.geojson\"\n",
    "helene_dir = r\"\\data\\geojson\\helene.geojson\"\n",
    "\n",
    "# Combine the base path and relative directory to create full, absolute paths to the files.\n",
    "francine_path = f\"{local_path}{francine_dir}\"\n",
    "helene_path = f\"{local_path}{helene_dir}\"\n",
    "\n",
    "# --- 2. Load data into GeoDataFrames ---\n",
    "# A GeoDataFrame is a pandas DataFrame with a special 'geometry' column that allows for spatial operations.\n",
    "francine_gdf = gpd.read_file(francine_path)\n",
    "helene_gdf = gpd.read_file(helene_path)\n",
    "\n",
    "# --- 3. Standardize timestamps to UTC ---\n",
    "# Convert the original 'time' column into a pandas datetime object.\n",
    "# Setting `utc=True` ensures all timestamps are in a single, unambiguous timezone (UTC).\n",
    "# This is crucial for accurate temporal comparisons and binning.\n",
    "francine_gdf['timestamp'] = pd.to_datetime(francine_gdf['time'], utc=True)\n",
    "helene_gdf['timestamp'] = pd.to_datetime(helene_gdf['time'], utc=True)\n",
    "\n",
    "# --- 4. Group data into 4-hour time bins ---\n",
    "# The `dt.floor('4h')` function rounds each timestamp *down* to the nearest 4-hour interval.\n",
    "# For example, 09:35 becomes 08:00, 15:59 becomes 12:00. This aggregates tweets into discrete time windows.\n",
    "francine_gdf['time_bin'] = francine_gdf['timestamp'].dt.floor('4h')\n",
    "helene_gdf['time_bin'] = helene_gdf['timestamp'].dt.floor('4h')\n",
    "\n",
    "# --- 5. Create Unix timestamps and lookup dictionaries ---\n",
    "# Convert the binned datetime objects into Unix timestamps (as an integer).\n",
    "# The `// 1000` division is likely to convert from nanoseconds or microseconds to seconds, a more standard Unix format.\n",
    "francine_gdf['unix_timestamp'] = francine_gdf['time_bin'].astype('int64') // 1000\n",
    "helene_gdf['unix_timestamp'] = helene_gdf['time_bin'].astype('int64') // 1000\n",
    "\n",
    "# Create dictionaries to map the numeric Unix timestamp back to its original datetime object.\n",
    "# This provides a quick way to retrieve the readable time bin later in the script without recalculating it.\n",
    "helene_timestamp_dict = dict(zip(helene_gdf['unix_timestamp'], helene_gdf['time_bin']))\n",
    "francine_timestamp_dict = dict(zip(francine_gdf['unix_timestamp'], francine_gdf['time_bin']))\n",
    "\n",
    "# --- 6. Create readable labels for file naming ---\n",
    "# The `dt.strftime` function formats the datetime object into a specific string format.\n",
    "# Here, '%Y%m%d_%H%M' creates a clean, sortable label like '20240926_0800', which is ideal for filenames.\n",
    "francine_gdf['bin_label'] = francine_gdf['time_bin'].dt.strftime('%Y%m%d_%H%M')\n",
    "helene_gdf['bin_label'] = helene_gdf['time_bin'].dt.strftime('%Y%m%d_%H%M')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load reference shapefiles\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "\n",
    "states_dir = r\"\\data\\shape_files\\cb_2023_us_state_20m.shp\"\n",
    "counties_dir = r\"\\data\\shape_files\\cb_2023_us_county_20m.shp\"\n",
    "cities_dir = r\"\\data\\shape_files\\US_Cities.shp\"\n",
    "states_path = f\"{local_path}{states_dir}\"\n",
    "counties_path = f\"{local_path}{counties_dir}\"\n",
    "cities_path = f\"{local_path}{cities_dir}\"\n",
    "\n",
    "\n",
    "# Load spatial reference data\n",
    "states_gdf = gpd.read_file(states_path)\n",
    "counties_gdf = gpd.read_file(counties_path)\n",
    "cities_gdf = gpd.read_file(cities_path)\n",
    "\n",
    "# PLACE THIS CODE AFTER LOADING SHAPEFILES BUT BEFORE CREATING SIMPLE LOOKUPS\n",
    "# =============================================================================\n",
    "# MULTI-LEVEL GEOGRAPHIC MATCHING SYSTEM (ALL LEVELS)\n",
    "# =============================================================================\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "\n",
    "def preprocess_place_name(name):\n",
    "    \"\"\"Standardize place names for better matching\"\"\"\n",
    "    if pd.isna(name) or name == 'NAN':\n",
    "        return None\n",
    "\n",
    "    name = str(name).upper().strip()\n",
    "\n",
    "    # Common abbreviation standardizations\n",
    "    name = re.sub(r'\\bST\\.?\\b', 'SAINT', name)  # St. -> Saint\n",
    "    name = re.sub(r'\\bMT\\.?\\b', 'MOUNT', name)  # Mt. -> Mount\n",
    "    name = re.sub(r'\\bFT\\.?\\b', 'FORT', name)   # Ft. -> Fort\n",
    "    name = re.sub(r'\\bN\\.?\\b', 'NORTH', name)   # N. -> North\n",
    "    name = re.sub(r'\\bS\\.?\\b', 'SOUTH', name)   # S. -> South\n",
    "    name = re.sub(r'\\bE\\.?\\b', 'EAST', name)    # E. -> East\n",
    "    name = re.sub(r'\\bW\\.?\\b', 'WEST', name)    # W. -> West\n",
    "\n",
    "    # Remove extra spaces and punctuation\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation\n",
    "    name = re.sub(r'\\s+', ' ', name)     # Normalize spaces\n",
    "\n",
    "    return name.strip()\n",
    "\n",
    "def parse_gpe_entities(gpe_string):\n",
    "    \"\"\"Parse GPE string into multiple potential geographic entities\"\"\"\n",
    "    if not gpe_string or pd.isna(gpe_string) or str(gpe_string).strip() == '':\n",
    "        return []\n",
    "\n",
    "    gpe_string = str(gpe_string).strip()\n",
    "\n",
    "    # Split by common separators\n",
    "    entities = []\n",
    "\n",
    "    # Primary split by comma\n",
    "    parts = [part.strip() for part in gpe_string.split(',')]\n",
    "\n",
    "    for part in parts:\n",
    "        if part:\n",
    "            # Further split by other separators\n",
    "            sub_parts = re.split(r'[;&|]', part)\n",
    "            for sub_part in sub_parts:\n",
    "                sub_part = sub_part.strip()\n",
    "                if sub_part and len(sub_part) > 1:  # Ignore single characters\n",
    "                    entities.append(preprocess_place_name(sub_part))\n",
    "\n",
    "    # Remove None values and duplicates while preserving order\n",
    "    clean_entities = []\n",
    "    seen = set()\n",
    "    for entity in entities:\n",
    "        if entity and entity not in seen:\n",
    "            clean_entities.append(entity)\n",
    "            seen.add(entity)\n",
    "\n",
    "    return clean_entities\n",
    "\n",
    "def create_hierarchical_lookups(states_gdf, counties_gdf, cities_gdf):\n",
    "    \"\"\"Create hierarchical lookup dictionaries for fuzzy matching\"\"\"\n",
    "    print(\"\\nCreating hierarchical lookup dictionaries...\")\n",
    "\n",
    "    # 1. States - simple lookup with preprocessed names + abbreviations\n",
    "    state_lookup = {}\n",
    "    state_abbrev_to_name = {}  # Abbreviation to full name\n",
    "    state_name_to_abbrev = {}  # Full name to abbreviation\n",
    "\n",
    "    for idx, row in states_gdf.iterrows():\n",
    "        state_name = preprocess_place_name(row['NAME'])\n",
    "        if state_name:\n",
    "            state_lookup[state_name] = row.geometry\n",
    "            # Handle abbreviations if available\n",
    "            if 'STUSPS' in row:\n",
    "                abbrev = row['STUSPS'].upper()\n",
    "                state_abbrev_to_name[abbrev] = state_name\n",
    "                state_name_to_abbrev[state_name] = abbrev\n",
    "                # Also add abbreviation as a lookup option\n",
    "                state_lookup[abbrev] = row.geometry\n",
    "\n",
    "    # 2. Counties - organized by state\n",
    "    county_by_state = {}\n",
    "    county_lookup = {}\n",
    "\n",
    "    for idx, row in counties_gdf.iterrows():\n",
    "        county_name = preprocess_place_name(row['NAME'])\n",
    "        state_fips = row.get('STATEFP', '')\n",
    "\n",
    "        if county_name:\n",
    "            county_lookup[county_name] = row.geometry\n",
    "\n",
    "            # Try to get state name from STATEFP or other fields\n",
    "            state_name = None\n",
    "            if 'STATE_NAME' in row:\n",
    "                state_name = preprocess_place_name(row['STATE_NAME'])\n",
    "            else:\n",
    "                # Try to find state by FIPS code\n",
    "                for s_idx, s_row in states_gdf.iterrows():\n",
    "                    if s_row.get('STATEFP', '') == state_fips:\n",
    "                        state_name = preprocess_place_name(s_row['NAME'])\n",
    "                        break\n",
    "\n",
    "            if state_name:\n",
    "                if state_name not in county_by_state:\n",
    "                    county_by_state[state_name] = {}\n",
    "                county_by_state[state_name][county_name] = row.geometry\n",
    "\n",
    "    # 3. Cities - organized by state\n",
    "    city_by_state = {}\n",
    "    city_lookup = {}\n",
    "\n",
    "    for idx, row in cities_gdf.iterrows():\n",
    "        city_name = preprocess_place_name(row['NAME'])\n",
    "        state_abbrev = row.get('ST', '').upper()\n",
    "\n",
    "        if city_name:\n",
    "            city_lookup[city_name] = row.geometry\n",
    "\n",
    "            # Convert state abbreviation to full name\n",
    "            if state_abbrev in state_abbrev_to_name:\n",
    "                state_full = state_abbrev_to_name[state_abbrev]\n",
    "                if state_full not in city_by_state:\n",
    "                    city_by_state[state_full] = {}\n",
    "                city_by_state[state_full][city_name] = row.geometry\n",
    "    #\n",
    "\n",
    "\n",
    "    return {\n",
    "        'state_lookup': state_lookup,\n",
    "        'county_lookup': county_lookup,\n",
    "        'city_lookup': city_lookup,\n",
    "        'county_by_state': county_by_state,\n",
    "        'city_by_state': city_by_state,\n",
    "        'state_abbrev_to_name': state_abbrev_to_name,\n",
    "        'state_name_to_abbrev': state_name_to_abbrev\n",
    "    }\n",
    "\n",
    "def fuzzy_match_entity(entity, candidates, threshold=75):\n",
    "    \"\"\"Fuzzy match an entity against candidates\"\"\"\n",
    "    if not entity or not candidates:\n",
    "        return None, 0\n",
    "\n",
    "    # Try exact match first\n",
    "    if entity in candidates:\n",
    "        return entity, 100\n",
    "\n",
    "    # Use fuzzy matching\n",
    "    match = process.extractOne(entity, candidates.keys(), scorer=fuzz.ratio)\n",
    "\n",
    "    if match and match[1] >= threshold:\n",
    "        return match[0], match[1]\n",
    "\n",
    "    return None, 0\n",
    "\n",
    "def find_all_geographic_matches(entities, lookups):\n",
    "    \"\"\"Find ALL geographic matches (state, county, city) for the entities\"\"\"\n",
    "    if not entities:\n",
    "        return []\n",
    "\n",
    "    state_lookup = lookups['state_lookup']\n",
    "    county_lookup = lookups['county_lookup']\n",
    "    city_lookup = lookups['city_lookup']\n",
    "    county_by_state = lookups['county_by_state']\n",
    "    city_by_state = lookups['city_by_state']\n",
    "\n",
    "    # Store all successful matches\n",
    "    all_matches = []\n",
    "\n",
    "    # Context tracking for better matching\n",
    "    found_states = set()\n",
    "\n",
    "    # STEP 1: Find all state matches first\n",
    "    for entity in entities:\n",
    "        state_match, state_score = fuzzy_match_entity(entity, state_lookup, threshold=75)\n",
    "        if state_match:\n",
    "            all_matches.append(('STATE', state_match, state_lookup[state_match], state_score))\n",
    "            found_states.add(state_match)\n",
    "\n",
    "    # STEP 2: Find county matches (global first, then state-specific)\n",
    "    for entity in entities:\n",
    "        # Global county search\n",
    "        county_match, county_score = fuzzy_match_entity(entity, county_lookup, threshold=75)\n",
    "        if county_match:\n",
    "            all_matches.append(('COUNTY', county_match, county_lookup[county_match], county_score))\n",
    "\n",
    "        # State-specific county search (higher accuracy)\n",
    "        for state_name in found_states:\n",
    "            if state_name in county_by_state:\n",
    "                state_counties = county_by_state[state_name]\n",
    "                state_county_match, state_county_score = fuzzy_match_entity(entity, state_counties, threshold=70)\n",
    "                if state_county_match and state_county_score > county_score:\n",
    "                    # Replace with better state-specific match\n",
    "                    # Remove the global match if it exists\n",
    "                    all_matches = [m for m in all_matches if not (m[0] == 'COUNTY' and m[1] == county_match)]\n",
    "                    all_matches.append(('COUNTY', state_county_match, state_counties[state_county_match], state_county_score))\n",
    "\n",
    "    # STEP 3: Find city matches (global first, then state-specific)\n",
    "    for entity in entities:\n",
    "        # Global city search\n",
    "        city_match, city_score = fuzzy_match_entity(entity, city_lookup, threshold=75)\n",
    "        if city_match:\n",
    "            all_matches.append(('CITY', city_match, city_lookup[city_match], city_score))\n",
    "\n",
    "        # State-specific city search (higher accuracy)\n",
    "        for state_name in found_states:\n",
    "            if state_name in city_by_state:\n",
    "                state_cities = city_by_state[state_name]\n",
    "                state_city_match, state_city_score = fuzzy_match_entity(entity, state_cities, threshold=70)\n",
    "                if state_city_match and state_city_score > city_score:\n",
    "                    # Replace with better state-specific match\n",
    "                    # Remove the global match if it exists\n",
    "                    all_matches = [m for m in all_matches if not (m[0] == 'CITY' and m[1] == city_match)]\n",
    "                    all_matches.append(('CITY', state_city_match, state_cities[state_city_match], state_city_score))\n",
    "\n",
    "    # Remove duplicates (same scale + name)\n",
    "    unique_matches = []\n",
    "    seen_combinations = set()\n",
    "    for match in all_matches:\n",
    "        combo = (match[0], match[1])  # (scale, name)\n",
    "        if combo not in seen_combinations:\n",
    "            unique_matches.append(match)\n",
    "            seen_combinations.add(combo)\n",
    "\n",
    "    return unique_matches\n",
    "\n",
    "def multi_level_assign_scale_levels(row, lookups):\n",
    "    \"\"\"\n",
    "    Return ALL geographic scale levels that match this tweet\n",
    "    Returns a list of matches: [(scale, name, geom, score), ...]\n",
    "    \"\"\"\n",
    "    gpe = str(row.get('GPE', '')).strip()\n",
    "    fac = str(row.get('FAC', '')).strip()\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    # Parse GPE into multiple entities\n",
    "    entities = parse_gpe_entities(gpe)\n",
    "\n",
    "    if entities:\n",
    "        # Find all geographic matches\n",
    "        geo_matches = find_all_geographic_matches(entities, lookups)\n",
    "        matches.extend(geo_matches)\n",
    "\n",
    "    # Add facility as separate match if available\n",
    "    if fac and fac not in ['nan', 'NAN', '']:\n",
    "        matches.append(('FACILITY', fac, row.geometry, 100))\n",
    "\n",
    "    # If no matches found, return unmatched\n",
    "    if not matches:\n",
    "        matches.append(('UNMATCHED', None, row.geometry, 0))\n",
    "\n",
    "    return matches\n",
    "\n",
    "def expand_tweets_by_matches(gdf, lookups, dataset_name):\n",
    "    \"\"\"\n",
    "    Expand the GeoDataFrame so each tweet creates multiple rows (one per geographic match)\n",
    "    \"\"\"\n",
    "    print(f\"\\nExpanding {dataset_name} tweets by geographic matches...\")\n",
    "\n",
    "    expanded_rows = []\n",
    "\n",
    "    for idx, row in gdf.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(idx)\n",
    "        matches = multi_level_assign_scale_levels(row, lookups)\n",
    "\n",
    "        # Create one row per match\n",
    "        for scale, name, geom, score in matches:\n",
    "            new_row = row.copy()\n",
    "            new_row['scale_level'] = scale\n",
    "            new_row['matched_name'] = name\n",
    "            new_row['matched_geom'] = geom\n",
    "            new_row['match_score'] = score\n",
    "            new_row['original_index'] = idx  # Track original tweet\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    # Create new GeoDataFrame and preserve the original CRS\n",
    "    expanded_gdf = gpd.GeoDataFrame(expanded_rows, crs=gdf.crs)\n",
    "\n",
    "    # Show some examples of multi-level matches\n",
    "    print(f\"  Sample multi-level matches:\")\n",
    "    # Group by original tweet and show ones with multiple matches\n",
    "    multi_matches = expanded_gdf.groupby('original_index').size()\n",
    "    multi_match_indices = multi_matches[multi_matches > 1].head(5).index\n",
    "\n",
    "    for orig_idx in multi_match_indices:\n",
    "        tweet_matches = expanded_gdf[expanded_gdf['original_index'] == orig_idx]\n",
    "        original_gpe = tweet_matches.iloc[0]['GPE']\n",
    "        match_summary = ', '.join([f\"{row['scale_level']}:{row['matched_name']}\" for _, row in tweet_matches.iterrows()])\n",
    "        # print(f\"    '{original_gpe}' → {match_summary}\")\n",
    "\n",
    "    return expanded_gdf\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTE MULTI-LEVEL FUZZY MATCHING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-LEVEL GEOGRAPHIC MATCHING (ALL LEVELS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create hierarchical lookups\n",
    "lookups = create_hierarchical_lookups(states_gdf, counties_gdf, cities_gdf)\n",
    "\n",
    "# Apply to both datasets (this will expand the datasets)\n",
    "francine_gdf = expand_tweets_by_matches(francine_gdf, lookups, \"FRANCINE\")\n",
    "helene_gdf = expand_tweets_by_matches(helene_gdf, lookups, \"HELENE\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-LEVEL FUZZY MATCHING COMPLETE ✓\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNote: Datasets are now expanded - each original tweet may have multiple rows\")\n",
    "print(\"representing different geographic scales (STATE, COUNTY, CITY, etc.)\")"
   ],
   "id": "102cf233829866a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # Group tweets by 4-hour intervals and scale level\n",
    "# Using unix_timestamp for unambiguous temporal grouping\n",
    "\n",
    "# Alternative approach:\n",
    "francine_interval_counts = francine_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).agg({\n",
    "    'matched_geom': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Add count column separately\n",
    "count_series = francine_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).size()\n",
    "francine_interval_counts['count'] = count_series.values\n",
    "\n",
    "# Same for Helene\n",
    "helene_interval_counts = helene_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).agg({\n",
    "    'matched_geom': 'first'\n",
    "}).reset_index()\n",
    "count_series = helene_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).size()\n",
    "helene_interval_counts['count'] = count_series.values\n",
    "\n",
    "# Sort by timestamp to ensure chronological order\n",
    "francine_interval_counts = francine_interval_counts.sort_values('unix_timestamp')\n",
    "helene_interval_counts = helene_interval_counts.sort_values('unix_timestamp')\n",
    "\n",
    "# Calculate cumulative counts\n",
    "francine_interval_counts['cumulative_count'] = francine_interval_counts.groupby(['scale_level', 'matched_name'])['count'].cumsum()\n",
    "helene_interval_counts['cumulative_count'] = helene_interval_counts.groupby(['scale_level', 'matched_name'])['count'].cumsum()\n",
    "\n",
    "# Get unique time bins for iteration\n",
    "francine_time_bins = sorted(francine_gdf['unix_timestamp'].unique())\n",
    "helene_time_bins = sorted(helene_gdf['unix_timestamp'].unique())\n",
    "\n"
   ],
   "id": "95d5ae7c58fc2af4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: DEFINE MASTER GRID CANVAS\n",
    "# ==============================================================================\n",
    "\n",
    "# Configuration\n",
    "TARGET_CRS = 'EPSG:3857'  # Web Mercator\n",
    "CELL_SIZE_M = 1000  # 5 km in meters\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: CREATING MASTER GRID CANVAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Project both datasets to target CRS\n",
    "print(f\"\\nProjecting datasets to {TARGET_CRS}...\")\n",
    "francine_proj = francine_gdf.to_crs(TARGET_CRS)\n",
    "helene_proj = helene_gdf.to_crs(TARGET_CRS)\n",
    "\n",
    "# Also project reference geometries\n",
    "print(\"Projecting reference geometries...\")\n",
    "states_proj = states_gdf.to_crs(TARGET_CRS)\n",
    "counties_proj = counties_gdf.to_crs(TARGET_CRS)\n",
    "cities_proj = cities_gdf.to_crs(TARGET_CRS)\n",
    "# Calculate combined extent from both hurricanes\"\n",
    "print(\"\\nCalculating master extent...\")\n",
    "francine_bounds = francine_proj.total_bounds\n",
    "helene_bounds = helene_proj.total_bounds\n",
    "\n",
    "# Get union of both bounding boxes\n",
    "minx = min(francine_bounds[0], helene_bounds[0])\n",
    "miny = min(francine_bounds[1], helene_bounds[1])\n",
    "maxx = max(francine_bounds[2], helene_bounds[2])\n",
    "maxy = max(francine_bounds[3], helene_bounds[3])\n",
    "#\n",
    "# print(f\"  Master bounds (EPSG:3857):\")\n",
    "# print(f\"    minx: {minx:,.2f}\")\n",
    "# print(f\"    miny: {miny:,.2f}\")\n",
    "# print(f\"    maxx: {maxx:,.2f}\")\n",
    "# print(f\"    maxy: {maxy:,.2f}\")\n",
    "\n",
    "# Calculate grid dimensions\n",
    "width = int(np.ceil((maxx - minx) / CELL_SIZE_M))\n",
    "height = int(np.ceil((maxy - miny) / CELL_SIZE_M))\n",
    "\n",
    "print(f\"\\nGrid Configuration:\")\n",
    "print(f\"  Cell size: {CELL_SIZE_M:,} meters ({CELL_SIZE_M/1000} km)\")\n",
    "print(f\"  Grid dimensions: {width} x {height} cells\")\n",
    "print(f\"  Total cells: {width * height:,}\")\n",
    "\n",
    "# Create master transform\n",
    "master_transform = from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "print(f\"\\nMaster Transform:\")\n",
    "print(f\"  {master_transform}\")\n",
    "\n",
    "# Calculate actual coverage area\n",
    "area_km2 = (width * height * CELL_SIZE_M * CELL_SIZE_M) / 1_000_000\n",
    "print(f\"\\nCoverage area: {area_km2:,.2f} km²\")\n",
    "\n",
    "# Store grid parameters for later use\n",
    "grid_params = {\n",
    "    'crs': TARGET_CRS,\n",
    "    'cell_size': CELL_SIZE_M,\n",
    "    'width': width,\n",
    "    'height': height,\n",
    "    'bounds': (minx, miny, maxx, maxy),\n",
    "    'transform': master_transform\n",
    "}\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"MASTER GRID CANVAS READY ✓\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "# Update lookup dictionaries with projected geometries\n",
    "print(\"\\nUpdating geometry lookups with projected coordinates...\")\n",
    "state_lookup_proj = dict(zip(states_proj['NAME'].str.upper(), states_proj.geometry))\n",
    "county_lookup_proj = dict(zip(counties_proj['NAME'].str.upper(), counties_proj.geometry))\n",
    "cities_lookup_proj = dict(zip(cities_proj['NAME'].str.upper(), cities_proj.geometry))\n",
    "# validation_results = validate_city_matching(francine_gdf, helene_gdf, lookups['city_lookup'], lookups['state_lookup'], lookups['county_lookup'])\n",
    "print(\"Lookup dictionaries updated with projected geometries ✓\")"
   ],
   "id": "5bd77653518eadb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.features import geometry_mask\n",
    "# ==============================================================================\n",
    "# STEP 2: MAIN RASTERIZATION LOOP - TIME ITERATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Create output directories\n",
    "rasters_dir = r\"\\rasters_output\"\n",
    "output_dir = f\"{local_path}{rasters_dir}\"\n",
    "# output_dir = os.path.join(local_path, 'rasters_output')\n",
    "# output_dir = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def create_hierarchical_rasters(data, grid_params, time_bin):\n",
    "    \"\"\"Create hierarchically weighted rasters with automatic parent state inclusion\"\"\"\n",
    "    print(f\"    Creating hierarchical raster for time {time_bin}...\")\n",
    "\n",
    "    output_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "    states_to_include = set()  # Track which states need base layers\n",
    "\n",
    "    # 1. First pass: identify all states that need base layers\n",
    "    state_data = data[data['scale_level'] == 'STATE']\n",
    "    if len(state_data) > 0:\n",
    "        states_to_include.update(state_data['matched_name'].unique())\n",
    "\n",
    "    # Check counties - add their parent states\n",
    "    county_data = data[data['scale_level'] == 'COUNTY']\n",
    "    for county_name in county_data['matched_name'].unique():\n",
    "        if county_name in county_lookup_proj:\n",
    "            # Find parent state by spatial containment\n",
    "            county_geom = county_lookup_proj[county_name]\n",
    "            for state_name, state_geom in state_lookup_proj.items():\n",
    "                if state_geom.contains(county_geom.centroid):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "\n",
    "    # Check cities - add their parent states\n",
    "    city_data = data[data['scale_level'] == 'CITY']\n",
    "    for city_name in city_data['matched_name'].unique():\n",
    "        if city_name in cities_lookup_proj:\n",
    "            city_geom = cities_lookup_proj[city_name]\n",
    "            for state_name, state_geom in state_lookup_proj.items():\n",
    "                if state_geom.contains(city_geom.centroid):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "\n",
    "    # 2. Rasterize all states that need inclusion\n",
    "    for state_name in states_to_include:\n",
    "        if state_name in state_lookup_proj:\n",
    "            state_geom = state_lookup_proj[state_name]\n",
    "            mask = rasterize(\n",
    "                [(state_geom, 1)],\n",
    "                out_shape=(grid_params['height'], grid_params['width']),\n",
    "                transform=grid_params['transform'],\n",
    "                fill=0, dtype=np.float32, all_touched=True\n",
    "            )\n",
    "\n",
    "            # Get tweet count if state was mentioned, else use minimal base\n",
    "            if state_name in state_data['matched_name'].values:\n",
    "                tweet_count = state_data[state_data['matched_name'] == state_name]['count'].sum()\n",
    "            else:\n",
    "                tweet_count = 1  # Minimal base for implied states\n",
    "\n",
    "            base_value = np.log1p(tweet_count) * 2\n",
    "            output_grid += mask * base_value\n",
    "\n",
    "    # 3. Add counties (same as before)\n",
    "    if len(county_data) > 0:\n",
    "        county_counts = county_data.groupby('matched_name')['count'].sum()\n",
    "        for county_name, tweet_count in county_counts.items():\n",
    "            if county_name in county_lookup_proj:\n",
    "                mask = rasterize(\n",
    "                    [(county_lookup_proj[county_name], 1)],\n",
    "                    out_shape=(grid_params['height'], grid_params['width']),\n",
    "                    transform=grid_params['transform'],\n",
    "                    fill=0, dtype=np.float32, all_touched=True\n",
    "                )\n",
    "                output_grid += mask * np.log1p(tweet_count) * 5\n",
    "\n",
    "    # 4. Add cities (same as before)\n",
    "    if len(city_data) > 0:\n",
    "        city_counts = city_data.groupby('matched_name')['count'].sum()\n",
    "        for city_name, tweet_count in city_counts.items():\n",
    "            if city_name in cities_lookup_proj:\n",
    "                mask = rasterize(\n",
    "                    [(cities_lookup_proj[city_name], 1)],\n",
    "                    out_shape=(grid_params['height'], grid_params['width']),\n",
    "                    transform=grid_params['transform'],\n",
    "                    fill=0, dtype=np.float32, all_touched=True\n",
    "                )\n",
    "                output_grid += mask * np.log1p(tweet_count) * 10\n",
    "\n",
    "    # 5. Add facilities\n",
    "    facility_data = data[data['scale_level'] == 'FACILITY']\n",
    "    if len(facility_data) > 0:\n",
    "        output_grid += create_facility_raster(data, grid_params)\n",
    "\n",
    "    return output_grid\n",
    "\n",
    "def process_hurricane(hurricane_name, gdf_proj, interval_counts, time_bins, timestamp_dict):\n",
    "    \"\"\"\n",
    "    Process a single hurricane through all time bins\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"PROCESSING: {hurricane_name.upper()}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print()\n",
    "    print(gdf_proj)\n",
    "    # Create hurricane-specific output directory\n",
    "    hurricane_dir = os.path.join(output_dir, hurricane_name.lower())\n",
    "    os.makedirs(hurricane_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize cumulative grid (persists across time bins)\n",
    "    cumulative_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "\n",
    "    # Loop through each time bin chronologically\n",
    "    for idx, time_bin in enumerate(time_bins):\n",
    "        # print(f\"\\n--- Time Bin {idx+1}/{len(time_bins)}: {time_bin} ---\")\n",
    "\n",
    "        # Filter data for current time bin\n",
    "        current_data = interval_counts[interval_counts['unix_timestamp'] == time_bin]\n",
    "        tweet_count = len(current_data)\n",
    "        # print(f\"  Tweets in this bin: {tweet_count}\")\n",
    "\n",
    "        # WITH THIS:\n",
    "        incremental_grid = create_hierarchical_rasters(current_data, grid_params, time_bin)\n",
    "\n",
    "        # === END PLACEHOLDERS ===\n",
    "\n",
    "        # Update cumulative grid\n",
    "        cumulative_grid += incremental_grid\n",
    "        # Save rasters\n",
    "        save_raster(incremental_grid, hurricane_dir, hurricane_name, time_bin, 'increment', timestamp_dict)\n",
    "        save_raster(cumulative_grid, hurricane_dir, hurricane_name, time_bin, 'cumulative', timestamp_dict)\n",
    "\n",
    "        print(f\"  Incremental max value: {np.max(incremental_grid):.2f}\")\n",
    "        print(f\"  Cumulative max value: {np.max(cumulative_grid):.2f}\")\n",
    "\n",
    "    print(f\"\\n{hurricane_name.upper()} processing complete!\")\n",
    "    print(f\"Output saved to: {hurricane_dir}\")\n",
    "    return\n",
    "\n",
    "# ==============================================================================\n",
    "# PLACEHOLDER FUNCTIONS (TO BE IMPLEMENTED)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_facility_raster(data, grid_params):\n",
    "    \"\"\"Create KDE raster for facility points with strong hotspot multiplier\"\"\"\n",
    "    print(\"    [FACILITY] Creating facility raster...\")\n",
    "\n",
    "    # Initialize empty raster\n",
    "    facility_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "\n",
    "    # Filter for FACILITY-level tweets only\n",
    "    facility_data = data[data['scale_level'] == 'FACILITY']\n",
    "\n",
    "    if len(facility_data) == 0:\n",
    "        print(\"      No facility-level tweets in this time bin\")\n",
    "        return facility_grid\n",
    "\n",
    "    # Group by facility coordinates (using matched_name as proxy) and sum counts\n",
    "    facility_counts = facility_data.groupby('matched_name')['count'].sum()\n",
    "\n",
    "    print(f\"      Processing {len(facility_counts)} unique facilities\")\n",
    "\n",
    "    # HOTSPOT PARAMETERS for facilities\n",
    "    sigma_meters = 2 * grid_params['cell_size']  # 10 km for 5km cells\n",
    "    sigma_pixels = sigma_meters / grid_params['cell_size']  # Convert to pixel units\n",
    "    facility_multiplier = 10  # Make facilities 10x more prominent (strongest hotspots)\n",
    "\n",
    "    # Process each facility\n",
    "    facilities_processed = 0\n",
    "    for facility_name, tweet_count in facility_counts.items():\n",
    "        # Get facility data to extract geometry\n",
    "        facility_rows = facility_data[facility_data['matched_name'] == facility_name]\n",
    "\n",
    "        if len(facility_rows) > 0:\n",
    "            # Get the point geometry (should be from the tweet's geocoded location)\n",
    "            facility_point = facility_rows.iloc[0]['matched_geom']\n",
    "\n",
    "            # Project point to grid CRS if needed\n",
    "            if hasattr(facility_point, 'x') and hasattr(facility_point, 'y'):\n",
    "                # Create GeoSeries to handle projection\n",
    "                point_geoseries = gpd.GeoSeries([facility_point], crs='EPSG:4326')\n",
    "                point_proj = point_geoseries.to_crs(grid_params['crs']).iloc[0]\n",
    "\n",
    "                # Convert point coordinates to pixel indices\n",
    "                px = (point_proj.x - grid_params['bounds'][0]) / grid_params['cell_size']\n",
    "                py = (grid_params['bounds'][3] - point_proj.y) / grid_params['cell_size']\n",
    "\n",
    "                # Check if point is within grid bounds\n",
    "                if 0 <= px < grid_params['width'] and 0 <= py < grid_params['height']:\n",
    "                    # Create point raster with tweet count at location\n",
    "                    point_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "                    point_grid[int(py), int(px)] = tweet_count\n",
    "\n",
    "                    # Apply Gaussian filter to create kernel density\n",
    "                    kernel_grid = gaussian_filter(point_grid, sigma=sigma_pixels, mode='constant', cval=0)\n",
    "\n",
    "                    # FIXED: Only add once with proper multiplier\n",
    "                    facility_grid += kernel_grid * facility_multiplier\n",
    "\n",
    "                    facilities_processed += 1\n",
    "                    effective_value = tweet_count * facility_multiplier\n",
    "                else:\n",
    "                    print(f\"      WARNING: Facility '{facility_name}' outside grid bounds\")\n",
    "            else:\n",
    "                print(f\"      WARNING: Invalid geometry for facility '{facility_name}'\")\n",
    "\n",
    "    print(f\"      Processed {facilities_processed} facilities with sigma={sigma_pixels:.2f} pixels\")\n",
    "\n",
    "    total_value = np.sum(facility_grid)\n",
    "    max_value = np.max(facility_grid)\n",
    "    # print(f\"      Total facility grid value: {total_value:.2f}, Max pixel: {max_value:.2f}\")\n",
    "\n",
    "    return facility_grid\n",
    "\n",
    "def save_raster(grid, output_dir, hurricane_name, time_bin, raster_type, timestamp_dict):\n",
    "    \"\"\"Save raster as GeoTIFF in type-specific subdirectory\"\"\"\n",
    "    # Create subdirectory for raster type\n",
    "    type_dir = os.path.join(output_dir, raster_type)\n",
    "    os.makedirs(type_dir, exist_ok=True)\n",
    "    print('max grid', np.max(grid))\n",
    "    # Convert unix timestamp (microseconds) back to datetime\n",
    "    time_str = timestamp_dict[time_bin].strftime('%Y%m%d_%H%M%S')\n",
    "    # time_str = pd.Timestamp(time_bin, unit='us').strftime('%Y%m%d_%H%M%S')\n",
    "    print([time_str])\n",
    "    filename = f\"{hurricane_name}_tweets_{time_str}.tif\"\n",
    "    filepath = os.path.join(type_dir, filename)\n",
    "    print(grid_params)\n",
    "    with rasterio.open(\n",
    "        filepath, 'w',\n",
    "        driver='GTiff',\n",
    "        height=grid_params['height'],\n",
    "        width=grid_params['width'],\n",
    "        count=1,\n",
    "        dtype=grid.dtype,\n",
    "        crs=grid_params['crs'],\n",
    "        transform=grid_params['transform'],\n",
    "        compress='lzw'\n",
    "    ) as dst:\n",
    "        dst.write(grid, 1)\n",
    "\n",
    "    print(f\"    Saved: {raster_type}/{filename}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECUTE PROCESSING FOR BOTH HURRICANES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING RASTERIZATION PROCESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process Francine\n",
    "process_hurricane('francine', francine_proj, francine_interval_counts, francine_time_bins, francine_timestamp_dict)\n",
    "\n",
    "# Process Helene\n",
    "process_hurricane('helene', helene_proj, helene_interval_counts, helene_time_bins, helene_timestamp_dict)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL PROCESSING COMPLETE! ✓\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "2e1bc16e47f51e00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import arcpy\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Note this is to be inserted into the python command window\n",
    "# Paths\n",
    "gdb_path = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\Tweet_project.gdb\"\n",
    "\n",
    "\n",
    "raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\cumulative\"\n",
    "mosaic_name = \"helene_cumulative_mosaic_v2\"\n",
    "\n",
    "raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\increment\"\n",
    "mosaic_name = \"helene_increment_mosaic_v2\"\n",
    "\n",
    "raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\francine\\cumulative\"\n",
    "mosaic_name = \"francine_cumulative_mosaic_v2\"\n",
    "\n",
    "raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\francine\\increment\"\n",
    "mosaic_name = \"francine_increment_mosaic_v2\"\n",
    "\n",
    "\n",
    "\n",
    "# Create geodatabase if it doesn't exist\n",
    "if not arcpy.Exists(gdb_path):\n",
    "    arcpy.CreateFileGDB_management(os.path.dirname(gdb_path), os.path.basename(gdb_path))\n",
    "\n",
    "# Create mosaic dataset\n",
    "mosaic_path = os.path.join(gdb_path, mosaic_name)\n",
    "if arcpy.Exists(mosaic_path):\n",
    "    arcpy.Delete_management(mosaic_path)\n",
    "\n",
    "arcpy.CreateMosaicDataset_management(gdb_path, mosaic_name, \"PROJCS['WGS_1984_Web_Mercator_Auxiliary_Sphere',GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Mercator_Auxiliary_Sphere'],PARAMETER['False_Easting',0.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',0.0],PARAMETER['Standard_Parallel_1',0.0],PARAMETER['Auxiliary_Sphere_Type',0.0],UNIT['Meter',1.0]]\")\n",
    "\n",
    "print(f\"Created mosaic dataset: {mosaic_path}\")\n",
    "\n",
    "# Add rasters to mosaic\n",
    "arcpy.AddRastersToMosaicDataset_management(\n",
    "    mosaic_path,\n",
    "    \"Raster Dataset\",\n",
    "    raster_folder,\n",
    "    filter=\"*.tif\"\n",
    ")\n",
    "\n",
    "print(\"Added rasters to mosaic dataset\")\n",
    "\n",
    "# Add time field\n",
    "arcpy.AddField_management(mosaic_path, \"date\", \"DATE\")\n",
    "\n",
    "# Calculate time from filename\n",
    "with arcpy.da.UpdateCursor(mosaic_path, [\"Name\", \"date\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        filename = row[0]\n",
    "        # Remove .tif extension and split\n",
    "        parts = filename.replace(\".tif\", \"\").split(\"_\")\n",
    "\n",
    "        # Join last two parts to get full timestamp: 20240926 + 080000\n",
    "        time_str = parts[-2] + parts[-1]  # Combines date and time\n",
    "\n",
    "        # Parse: 20240926080000 -> datetime\n",
    "        dt = datetime.strptime(time_str, \"%Y%m%d%H%M%S\")\n",
    "        print(f\"{filename} -> {time_str} -> {dt}\")\n",
    "        row[1] = dt\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Time field populated\")\n",
    "\n",
    "# Configure mosaic properties\n",
    "arcpy.SetMosaicDatasetProperties_management(\n",
    "    mosaic_path,\n",
    "    start_time_field=\"date\"\n",
    ")\n",
    "\n",
    "print(\"Mosaic dataset configured with time dimension\")\n",
    "\n",
    "print(f\"\\nMosaic dataset complete: {mosaic_path}\")\n",
    "print(\"To apply symbology in ArcGIS Pro:\")\n",
    "print(f\"1. Add mosaic to map: {mosaic_path}\")\n",
    "print(f\"2. Right-click layer > Symbology > Import\")\n",
    "print(f\"3. Select: {symbology_file}\")\n",
    "print(\"4. Enable time slider to animate cumulative growth\")"
   ],
   "id": "80b96b516fb40db0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
