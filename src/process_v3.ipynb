{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# !pip install fuzzywuzzy python-Levenshtein geopandas pandas numpy matplotlib",
   "id": "203a6780cd79d505",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# This cell handles the initial loading and preparation of the hurricane tweet data.\n",
    "# Key steps include:\n",
    "# 1. Importing necessary libraries for data manipulation, file paths, and time handling.\n",
    "# 2. Constructing file paths to the GeoJSON data for Hurricanes Francine and Helene.\n",
    "# 3. Loading the spatial data into GeoDataFrames.\n",
    "# 4. Standardizing all timestamps to Coordinated Universal Time (UTC).\n",
    "# 5. Aggregating the data into discrete 4-hour time bins for temporal analysis.\n",
    "# 6. Creating various time-related columns (Unix timestamps, readable labels) for later use.\n",
    "# =============================================================================\n",
    "\n",
    "# Import core libraries\n",
    "import geopandas as gpd  # Used for working with geospatial data.\n",
    "import pandas as pd      # Used for data manipulation and analysis in DataFrames.\n",
    "import os                # Provides a way of using operating system dependent functionality, like file paths.\n",
    "from datetime import datetime, timezone # Used for handling date and time objects.\n",
    "\n",
    "# --- 1. Load GeoJSON files ---\n",
    "# Get the parent directory of the current working directory to build relative paths.\n",
    "# This makes the script more portable as it doesn't rely on a hardcoded absolute path.\n",
    "local_path = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Define the relative paths to the GeoJSON files for each hurricane.\n",
    "francine_dir = r\"\\data\\geojson\\francine.geojson\"\n",
    "helene_dir = r\"\\data\\geojson\\helene.geojson\"\n",
    "\n",
    "# Combine the base path and relative directory to create full, absolute paths to the files.\n",
    "francine_path = f\"{local_path}{francine_dir}\"\n",
    "helene_path = f\"{local_path}{helene_dir}\"\n",
    "\n",
    "# --- 2. Load data into GeoDataFrames ---\n",
    "# A GeoDataFrame is a pandas DataFrame with a special 'geometry' column that allows for spatial operations.\n",
    "francine_gdf = gpd.read_file(francine_path)\n",
    "helene_gdf = gpd.read_file(helene_path)\n",
    "\n",
    "# --- 3. Standardize timestamps to UTC ---\n",
    "# Convert the original 'time' column into a pandas datetime object.\n",
    "# Setting `utc=True` ensures all timestamps are in a single, unambiguous timezone (UTC).\n",
    "# This is crucial for accurate temporal comparisons and binning.\n",
    "francine_gdf['timestamp'] = pd.to_datetime(francine_gdf['time'], utc=True)\n",
    "helene_gdf['timestamp'] = pd.to_datetime(helene_gdf['time'], utc=True)\n",
    "\n",
    "# --- 4. Group data into 4-hour time bins ---\n",
    "# The `dt.floor('4h')` function rounds each timestamp *down* to the nearest 4-hour interval.\n",
    "# For example, 09:35 becomes 08:00, 15:59 becomes 12:00. This aggregates tweets into discrete time windows.\n",
    "francine_gdf['time_bin'] = francine_gdf['timestamp'].dt.floor('4h')\n",
    "helene_gdf['time_bin'] = helene_gdf['timestamp'].dt.floor('4h')\n",
    "\n",
    "# --- 5. Create Unix timestamps and lookup dictionaries ---\n",
    "# Convert the binned datetime objects into Unix timestamps (as an integer).\n",
    "# The `// 1000` division is likely to convert from nanoseconds or microseconds to seconds, a more standard Unix format.\n",
    "francine_gdf['unix_timestamp'] = francine_gdf['time_bin'].astype('int64') // 1000\n",
    "helene_gdf['unix_timestamp'] = helene_gdf['time_bin'].astype('int64') // 1000\n",
    "\n",
    "# Create dictionaries to map the numeric Unix timestamp back to its original datetime object.\n",
    "# This provides a quick way to retrieve the readable time bin later in the script without recalculating it.\n",
    "helene_timestamp_dict = dict(zip(helene_gdf['unix_timestamp'], helene_gdf['time_bin']))\n",
    "francine_timestamp_dict = dict(zip(francine_gdf['unix_timestamp'], francine_gdf['time_bin']))\n",
    "\n",
    "# --- 6. Create readable labels for file naming ---\n",
    "# The `dt.strftime` function formats the datetime object into a specific string format.\n",
    "# Here, '%Y%m%d_%H%M' creates a clean, sortable label like '20240926_0800', which is ideal for filenames.\n",
    "francine_gdf['bin_label'] = francine_gdf['time_bin'].dt.strftime('%Y%m%d_%H%M')\n",
    "helene_gdf['bin_label'] = helene_gdf['time_bin'].dt.strftime('%Y%m%d_%H%M')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load reference shapefiles\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "\n",
    "states_dir = r\"\\data\\shape_files\\cb_2023_us_state_20m.shp\"\n",
    "counties_dir = r\"\\data\\shape_files\\cb_2023_us_county_20m.shp\"\n",
    "cities_dir = r\"\\data\\shape_files\\US_Cities.shp\"\n",
    "states_path = f\"{local_path}{states_dir}\"\n",
    "counties_path = f\"{local_path}{counties_dir}\"\n",
    "cities_path = f\"{local_path}{cities_dir}\"\n",
    "\n",
    "\n",
    "# Load spatial reference data\n",
    "states_gdf = gpd.read_file(states_path)\n",
    "counties_gdf = gpd.read_file(counties_path)\n",
    "cities_gdf = gpd.read_file(cities_path)\n",
    "\n",
    "# =============================================================================\n",
    "# MULTI-LEVEL GEOGRAPHIC MATCHING (SINGLE BEST PER ENTITY: STATE>COUNTY>CITY)\n",
    "# with conservative guards against false positives (e.g., FRANCINE→FRANKLIN)\n",
    "# =============================================================================\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "\n",
    "# --- Safeguards ---------------------------------------------------------------\n",
    "BLOCKLIST_EXACT = {\n",
    "    # hurricane/event terms and generic weather tokens to ignore unless exact place\n",
    "    'FRANCINE', 'HELENE', 'HURRICANE', 'STORM', 'TROPICAL', 'TS', 'CYCLONE',\n",
    "    'FLOOD', 'RAIN', 'WIND', 'SURGE', 'LANDFALL', 'EYE', 'TRACK', 'BAND'\n",
    "}\n",
    "\n",
    "def is_blocklisted(entity: str) -> bool:\n",
    "    e = (entity or '').strip().upper()\n",
    "    return e in BLOCKLIST_EXACT\n",
    "\n",
    "def preprocess_place_name(name):\n",
    "    \"\"\"Standardize place names for better matching.\"\"\"\n",
    "    if pd.isna(name) or name == 'NAN':\n",
    "        return None\n",
    "    name = str(name).upper().strip()\n",
    "    name = re.sub(r'\\bST\\.?\\b', 'SAINT', name)\n",
    "    name = re.sub(r'\\bMT\\.?\\b', 'MOUNT', name)\n",
    "    name = re.sub(r'\\bFT\\.?\\b', 'FORT', name)\n",
    "    name = re.sub(r'\\bN\\.?\\b', 'NORTH', name)\n",
    "    name = re.sub(r'\\bS\\.?\\b', 'SOUTH', name)\n",
    "    name = re.sub(r'\\bE\\.?\\b', 'EAST', name)\n",
    "    name = re.sub(r'\\bW\\.?\\b', 'WEST', name)\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    return name.strip()\n",
    "\n",
    "def parse_gpe_entities(gpe_string):\n",
    "    \"\"\"Split the GPE field into potential place names.\"\"\"\n",
    "    if not gpe_string or pd.isna(gpe_string) or str(gpe_string).strip() == '':\n",
    "        return []\n",
    "    gpe_string = str(gpe_string).strip()\n",
    "    entities = []\n",
    "    for part in [p.strip() for p in gpe_string.split(',')]:\n",
    "        if not part:\n",
    "            continue\n",
    "        for sub in re.split(r'[;&|]', part):\n",
    "            sub = preprocess_place_name(sub)\n",
    "            if sub and len(sub) > 1:\n",
    "                entities.append(sub)\n",
    "    # remove dups\n",
    "    seen, clean = set(), []\n",
    "    for e in entities:\n",
    "        if e not in seen:\n",
    "            clean.append(e)\n",
    "            seen.add(e)\n",
    "    return clean\n",
    "\n",
    "def create_hierarchical_lookups(states_gdf, counties_gdf, cities_gdf):\n",
    "    \"\"\"Build dictionaries for all levels.\"\"\"\n",
    "    print(\"\\nCreating hierarchical lookup dictionaries...\")\n",
    "\n",
    "    state_lookup, state_abbrev_to_name, state_name_to_abbrev = {}, {}, {}\n",
    "    for _, row in states_gdf.iterrows():\n",
    "        sname = preprocess_place_name(row['NAME'])\n",
    "        if not sname:\n",
    "            continue\n",
    "        state_lookup[sname] = row.geometry\n",
    "        if 'STUSPS' in row:\n",
    "            abbr = str(row['STUSPS']).upper()\n",
    "            state_abbrev_to_name[abbr] = sname\n",
    "            state_name_to_abbrev[sname] = abbr\n",
    "            state_lookup[abbr] = row.geometry\n",
    "\n",
    "    county_lookup, county_by_state = {}, {}\n",
    "    for _, row in counties_gdf.iterrows():\n",
    "        cname = preprocess_place_name(row['NAME'])\n",
    "        sfips = row.get('STATEFP', '')\n",
    "        if not cname:\n",
    "            continue\n",
    "        county_lookup[cname] = row.geometry\n",
    "        sname = None\n",
    "        if 'STATE_NAME' in row:\n",
    "            sname = preprocess_place_name(row['STATE_NAME'])\n",
    "        else:\n",
    "            for _, srow in states_gdf.iterrows():\n",
    "                if srow.get('STATEFP', '') == sfips:\n",
    "                    sname = preprocess_place_name(srow['NAME'])\n",
    "                    break\n",
    "        if sname:\n",
    "            county_by_state.setdefault(sname, {})[cname] = row.geometry\n",
    "\n",
    "    city_lookup, city_by_state = {}, {}\n",
    "    for _, row in cities_gdf.iterrows():\n",
    "        cname = preprocess_place_name(row['NAME'])\n",
    "        stabbr = str(row.get('ST', '')).upper()\n",
    "        if not cname:\n",
    "            continue\n",
    "        city_lookup[cname] = row.geometry\n",
    "        if stabbr in state_abbrev_to_name:\n",
    "            sfull = state_abbrev_to_name[stabbr]\n",
    "            city_by_state.setdefault(sfull, {})[cname] = row.geometry\n",
    "\n",
    "    return {\n",
    "        'state_lookup': state_lookup,\n",
    "        'county_lookup': county_lookup,\n",
    "        'city_lookup': city_lookup,\n",
    "        'county_by_state': county_by_state,\n",
    "        'city_by_state': city_by_state,\n",
    "        'state_abbrev_to_name': state_abbrev_to_name,\n",
    "        'state_name_to_abbrev': state_name_to_abbrev\n",
    "    }\n",
    "\n",
    "# --- Smart fuzzy matching (with conservative guards) --------------------------\n",
    "def _best_name_by_ratio(entity, names):\n",
    "    \"\"\"Get best candidate name by fuzz.ratio; returns (name, ratio) or (None,0).\"\"\"\n",
    "    if not names:\n",
    "        return (None, 0)\n",
    "    match = process.extractOne(entity, names, scorer=fuzz.ratio)\n",
    "    return (match[0], match[1]) if match else (None, 0)\n",
    "\n",
    "def fuzzy_match_best(entity, candidates, base_threshold):\n",
    "    \"\"\"\n",
    "    Return (name, geom, score) for the best candidate meeting conservative checks:\n",
    "      - Blocklisted tokens: only allow EXACT match (no fuzzy).\n",
    "      - Single-token with len>=6: ONLY exact match (prevents FRANCINE→FRANKLIN).\n",
    "      - Otherwise require BOTH ratio and token_set_ratio >= threshold.\n",
    "    \"\"\"\n",
    "    if not entity or not candidates:\n",
    "        return None, None, 0\n",
    "\n",
    "    # Exact fast-path\n",
    "    if entity in candidates:\n",
    "        return entity, candidates[entity], 100\n",
    "\n",
    "    # Blocklist: refuse fuzzy for these tokens\n",
    "    if is_blocklisted(entity):\n",
    "        return None, None, 0\n",
    "\n",
    "    # Single-token long strings: exact only\n",
    "    if ' ' not in entity and len(entity) >= 6:\n",
    "        return None, None, 0\n",
    "\n",
    "    # Compute best by ratio, then validate with token_set_ratio\n",
    "    names = list(candidates.keys())\n",
    "    best_name, ratio_score = _best_name_by_ratio(entity, names)\n",
    "    if not best_name:\n",
    "        return None, None, 0\n",
    "\n",
    "    set_score = fuzz.token_set_ratio(entity, best_name)\n",
    "\n",
    "    # Dynamic tightening for very short tokens (reduce false hits)\n",
    "    threshold = base_threshold\n",
    "    if len(entity) <= 3:\n",
    "        threshold = max(threshold, 95)\n",
    "    elif len(entity) <= 5:\n",
    "        threshold = max(threshold, 90)\n",
    "\n",
    "    if ratio_score >= threshold and set_score >= threshold:\n",
    "        return best_name, candidates[best_name], int((ratio_score + set_score) / 2)\n",
    "\n",
    "    return None, None, 0\n",
    "\n",
    "def resolve_best_for_entity(entity, lookups, state_context):\n",
    "    \"\"\"\n",
    "    Pick ONE best match with priority: STATE > COUNTY > CITY.\n",
    "    COUNTY/CITY biased by any known states in state_context.\n",
    "    \"\"\"\n",
    "    st_lu = lookups['state_lookup']\n",
    "    co_lu = lookups['county_lookup']\n",
    "    ci_lu = lookups['city_lookup']\n",
    "    co_by_state = lookups['county_by_state']\n",
    "    ci_by_state = lookups['city_by_state']\n",
    "\n",
    "    # 1) STATE (exact or USPS abbrev or conservative fuzzy)\n",
    "    n, g, sc = fuzzy_match_best(entity, st_lu, 85)  # states: higher threshold\n",
    "    if sc > 0:\n",
    "        return ('STATE', n, g, sc)\n",
    "\n",
    "    # 2) COUNTY (prefer in-state)\n",
    "    best = (None, None, 0)\n",
    "    for s in state_context:\n",
    "        if s in co_by_state:\n",
    "            n2, g2, sc2 = fuzzy_match_best(entity, co_by_state[s], 85)\n",
    "            if sc2 > best[2]:\n",
    "                best = (n2, g2, sc2)\n",
    "    if best[2] == 0:\n",
    "        n2, g2, sc2 = fuzzy_match_best(entity, co_lu, 90)  # global county stricter\n",
    "        if sc2 > best[2]:\n",
    "            best = (n2, g2, sc2)\n",
    "    if best[2] > 0:\n",
    "        return ('COUNTY', best[0], best[1], best[2])\n",
    "\n",
    "    # 3) CITY (prefer in-state)\n",
    "    best = (None, None, 0)\n",
    "    for s in state_context:\n",
    "        if s in ci_by_state:\n",
    "            n3, g3, sc3 = fuzzy_match_best(entity, ci_by_state[s], 85)\n",
    "            if sc3 > best[2]:\n",
    "                best = (n3, g3, sc3)\n",
    "    if best[2] == 0:\n",
    "        n3, g3, sc3 = fuzzy_match_best(entity, ci_lu, 90)  # global city stricter\n",
    "        if sc3 > best[2]:\n",
    "            best = (n3, g3, sc3)\n",
    "    if best[2] > 0:\n",
    "        return ('CITY', best[0], best[1], best[2])\n",
    "\n",
    "    return None\n",
    "\n",
    "def find_all_geographic_matches_single_per_entity(entities, lookups):\n",
    "    \"\"\"Return one best match per entity, using STATE>COUNTY>CITY and state context.\"\"\"\n",
    "    if not entities:\n",
    "        return []\n",
    "\n",
    "    # Build state context from the entities themselves (helps county/city later).\n",
    "    state_context = set()\n",
    "    for e in entities:\n",
    "        n, g, sc = fuzzy_match_best(e, lookups['state_lookup'], 85)\n",
    "        if sc > 0 and n:\n",
    "            state_context.add(n)\n",
    "\n",
    "    results, seen = [], set()\n",
    "    for e in entities:\n",
    "        m = resolve_best_for_entity(e, lookups, state_context)\n",
    "        if not m:\n",
    "            continue\n",
    "        key = (m[0], m[1])  # (scale, name)\n",
    "        if key not in seen:\n",
    "            results.append(m)\n",
    "            seen.add(key)\n",
    "    return results\n",
    "\n",
    "def multi_level_assign_scale_levels(row, lookups):\n",
    "    \"\"\"Return list of matches [(scale,name,geom,score), ...] for this tweet.\"\"\"\n",
    "    gpe = str(row.get('GPE', '')).strip()\n",
    "    fac = str(row.get('FAC', '')).strip()\n",
    "    matches = []\n",
    "\n",
    "    entities = parse_gpe_entities(gpe)\n",
    "    if entities:\n",
    "        geo_matches = find_all_geographic_matches_single_per_entity(entities, lookups)\n",
    "        matches.extend(geo_matches)\n",
    "\n",
    "    if fac and fac not in ['nan', 'NAN', '']:\n",
    "        matches.append(('FACILITY', preprocess_place_name(fac), row.geometry, 100))\n",
    "\n",
    "    if not matches:\n",
    "        matches.append(('UNMATCHED', None, row.geometry, 0))\n",
    "    return matches\n",
    "\n",
    "def expand_tweets_by_matches(gdf, lookups, dataset_name):\n",
    "    print(f\"\\nExpanding {dataset_name} tweets by geographic matches...\")\n",
    "    expanded_rows = []\n",
    "    for i, row in gdf.iterrows():\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        for scale, name, geom, score in multi_level_assign_scale_levels(row, lookups):\n",
    "            new_row = row.copy()\n",
    "            new_row['scale_level'] = scale\n",
    "            new_row['matched_name'] = name\n",
    "            new_row['matched_geom'] = geom\n",
    "            new_row['match_score'] = score\n",
    "            new_row['original_index'] = i\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    expanded_gdf = gpd.GeoDataFrame(expanded_rows, crs=gdf.crs)\n",
    "    print(\"  Sample multi-level matches:\")\n",
    "    multi = expanded_gdf.groupby('original_index').size()\n",
    "    idxs = multi[multi > 1].head(5).index\n",
    "    for j in idxs:\n",
    "        t = expanded_gdf[expanded_gdf['original_index'] == j]\n",
    "        gpe = t.iloc[0]['GPE']\n",
    "        summ = ', '.join([f\"{r['scale_level']}:{r['matched_name']}\" for _, r in t.iterrows()])\n",
    "        print(f\"    '{gpe}' → {summ}\")\n",
    "    return expanded_gdf\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTE MULTI-LEVEL FUZZY MATCHING\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-LEVEL GEOGRAPHIC MATCHING (STATE>COUNTY>CITY, conservative)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lookups = create_hierarchical_lookups(states_gdf, counties_gdf, cities_gdf)\n",
    "francine_gdf = expand_tweets_by_matches(francine_gdf, lookups, \"FRANCINE\")\n",
    "helene_gdf   = expand_tweets_by_matches(helene_gdf, lookups, \"HELENE\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-LEVEL FUZZY MATCHING COMPLETE ✓\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGuards active: blocklist, exact-only for long single tokens, dual-score thresholds.\")\n"
   ],
   "id": "102cf233829866a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # Group tweets by 4-hour intervals and scale level\n",
    "# Using unix_timestamp for unambiguous temporal grouping\n",
    "\n",
    "# Alternative approach:\n",
    "francine_interval_counts = francine_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).agg({\n",
    "    'matched_geom': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Add count column separately\n",
    "count_series = francine_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).size()\n",
    "francine_interval_counts['count'] = count_series.values\n",
    "\n",
    "# Same for Helene\n",
    "helene_interval_counts = helene_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).agg({\n",
    "    'matched_geom': 'first'\n",
    "}).reset_index()\n",
    "count_series = helene_gdf.groupby(['unix_timestamp', 'scale_level', 'matched_name']).size()\n",
    "helene_interval_counts['count'] = count_series.values\n",
    "\n",
    "# Sort by timestamp to ensure chronological order\n",
    "francine_interval_counts = francine_interval_counts.sort_values('unix_timestamp')\n",
    "helene_interval_counts = helene_interval_counts.sort_values('unix_timestamp')\n",
    "\n",
    "# Calculate cumulative counts\n",
    "francine_interval_counts['cumulative_count'] = francine_interval_counts.groupby(['scale_level', 'matched_name'])['count'].cumsum()\n",
    "helene_interval_counts['cumulative_count'] = helene_interval_counts.groupby(['scale_level', 'matched_name'])['count'].cumsum()\n",
    "\n",
    "# Get unique time bins for iteration\n",
    "francine_time_bins = sorted(francine_gdf['unix_timestamp'].unique())\n",
    "helene_time_bins = sorted(helene_gdf['unix_timestamp'].unique())\n",
    "\n"
   ],
   "id": "95d5ae7c58fc2af4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: DEFINE MASTER GRID CANVAS\n",
    "# ==============================================================================\n",
    "\n",
    "# Configuration\n",
    "TARGET_CRS = 'EPSG:3857'  # Web Mercator\n",
    "CELL_SIZE_M = 1000  # 5 km in meters\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: CREATING MASTER GRID CANVAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Project both datasets to target CRS\n",
    "print(f\"\\nProjecting datasets to {TARGET_CRS}...\")\n",
    "francine_proj = francine_gdf.to_crs(TARGET_CRS)\n",
    "helene_proj = helene_gdf.to_crs(TARGET_CRS)\n",
    "\n",
    "# Also project reference geometries\n",
    "print(\"Projecting reference geometries...\")\n",
    "states_proj = states_gdf.to_crs(TARGET_CRS)\n",
    "counties_proj = counties_gdf.to_crs(TARGET_CRS)\n",
    "cities_proj = cities_gdf.to_crs(TARGET_CRS)\n",
    "# Calculate combined extent from both hurricanes\"\n",
    "print(\"\\nCalculating master extent...\")\n",
    "francine_bounds = francine_proj.total_bounds\n",
    "helene_bounds = helene_proj.total_bounds\n",
    "\n",
    "# Get union of both bounding boxes\n",
    "minx = min(francine_bounds[0], helene_bounds[0])\n",
    "miny = min(francine_bounds[1], helene_bounds[1])\n",
    "maxx = max(francine_bounds[2], helene_bounds[2])\n",
    "maxy = max(francine_bounds[3], helene_bounds[3])\n",
    "#\n",
    "# print(f\"  Master bounds (EPSG:3857):\")\n",
    "# print(f\"    minx: {minx:,.2f}\")\n",
    "# print(f\"    miny: {miny:,.2f}\")\n",
    "# print(f\"    maxx: {maxx:,.2f}\")\n",
    "# print(f\"    maxy: {maxy:,.2f}\")\n",
    "\n",
    "# Calculate grid dimensions\n",
    "width = int(np.ceil((maxx - minx) / CELL_SIZE_M))\n",
    "height = int(np.ceil((maxy - miny) / CELL_SIZE_M))\n",
    "\n",
    "print(f\"\\nGrid Configuration:\")\n",
    "print(f\"  Cell size: {CELL_SIZE_M:,} meters ({CELL_SIZE_M/1000} km)\")\n",
    "print(f\"  Grid dimensions: {width} x {height} cells\")\n",
    "print(f\"  Total cells: {width * height:,}\")\n",
    "\n",
    "# Create master transform\n",
    "master_transform = from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "print(f\"\\nMaster Transform:\")\n",
    "print(f\"  {master_transform}\")\n",
    "\n",
    "# Calculate actual coverage area\n",
    "area_km2 = (width * height * CELL_SIZE_M * CELL_SIZE_M) / 1_000_000\n",
    "print(f\"\\nCoverage area: {area_km2:,.2f} km²\")\n",
    "\n",
    "# Store grid parameters for later use\n",
    "grid_params = {\n",
    "    'crs': TARGET_CRS,\n",
    "    'cell_size': CELL_SIZE_M,\n",
    "    'width': width,\n",
    "    'height': height,\n",
    "    'bounds': (minx, miny, maxx, maxy),\n",
    "    'transform': master_transform\n",
    "}\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"MASTER GRID CANVAS READY ✓\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "# Update lookup dictionaries with projected geometries\n",
    "print(\"\\nUpdating geometry lookups with projected coordinates...\")\n",
    "state_lookup_proj = dict(zip(states_proj['NAME'].str.upper(), states_proj.geometry))\n",
    "county_lookup_proj = dict(zip(counties_proj['NAME'].str.upper(), counties_proj.geometry))\n",
    "cities_lookup_proj = dict(zip(cities_proj['NAME'].str.upper(), cities_proj.geometry))\n",
    "# validation_results = validate_city_matching(francine_gdf, helene_gdf, lookups['city_lookup'], lookups['state_lookup'], lookups['county_lookup'])\n",
    "print(\"Lookup dictionaries updated with projected geometries ✓\")"
   ],
   "id": "5bd77653518eadb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "# Removed: from KDEpy import FFTKDE (using scipy.stats.gaussian_kde instead)\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: MAIN RASTERIZATION LOOP - TIME ITERATION (UNCHANGED CORE, BONUS ADDED)\n",
    "# ==============================================================================\n",
    "\n",
    "# Create output directories\n",
    "rasters_dir = r\"\\rasters_output\"\n",
    "output_dir = f\"{local_path}{rasters_dir}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def create_hierarchical_rasters(data, grid_params, time_bin):\n",
    "    \"\"\"Create hierarchically weighted rasters with KDE heat map for cities\"\"\"\n",
    "    print(f\"    Creating hierarchical raster for time {time_bin}...\")\n",
    "\n",
    "    output_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "    states_to_include = set()\n",
    "\n",
    "    # 1. States (identify base layers to include)\n",
    "    state_data = data[data['scale_level'] == 'STATE']\n",
    "    if len(state_data) > 0:\n",
    "        states_to_include.update(state_data['matched_name'].unique())\n",
    "\n",
    "    # 1b. Counties -> parent states\n",
    "    county_data = data[data['scale_level'] == 'COUNTY']\n",
    "    for county_name in county_data['matched_name'].unique():\n",
    "        if county_name in county_lookup_proj:\n",
    "            county_geom = county_lookup_proj[county_name]\n",
    "            for state_name, state_geom in state_lookup_proj.items():\n",
    "                if state_geom.contains(county_geom.centroid):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "\n",
    "    # 1c. Cities -> parent states\n",
    "    city_data = data[data['scale_level'] == 'CITY']\n",
    "    for city_name in city_data['matched_name'].unique():\n",
    "        if city_name in cities_lookup_proj:\n",
    "            city_geom = cities_lookup_proj[city_name]\n",
    "            for state_name, state_geom in state_lookup_proj.items():\n",
    "                if state_geom.contains(city_geom.centroid):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "\n",
    "    # 2. Rasterize states\n",
    "    for state_name in states_to_include:\n",
    "        if state_name in state_lookup_proj:\n",
    "            state_geom = state_lookup_proj[state_name]\n",
    "            mask = rasterize(\n",
    "                [(state_geom, 1)],\n",
    "                out_shape=(grid_params['height'], grid_params['width']),\n",
    "                transform=grid_params['transform'],\n",
    "                fill=0, dtype=np.float32, all_touched=True\n",
    "            )\n",
    "            if state_name in state_data['matched_name'].values:\n",
    "                tweet_count = state_data[state_data['matched_name'] == state_name]['count'].sum()\n",
    "            else:\n",
    "                tweet_count = 1  # minimal base\n",
    "            base_value = np.log1p(tweet_count) * 2\n",
    "            output_grid += mask * base_value\n",
    "\n",
    "    # 3. Add counties (fill)\n",
    "    if len(county_data) > 0:\n",
    "        county_counts = county_data.groupby('matched_name')['count'].sum()\n",
    "        for county_name, tweet_count in county_counts.items():\n",
    "            if county_name in county_lookup_proj:\n",
    "                mask = rasterize(\n",
    "                    [(county_lookup_proj[county_name], 1)],\n",
    "                    out_shape=(grid_params['height'], grid_params['width']),\n",
    "                    transform=grid_params['transform'],\n",
    "                    fill=0, dtype=np.float32, all_touched=True\n",
    "                )\n",
    "                output_grid += mask * np.log1p(tweet_count) * 5\n",
    "\n",
    "    # 4. Add cities as KDE heat map (UPDATED - uses KDE instead of polygon fill)\n",
    "    if len(city_data) > 0:\n",
    "        print(f\"      Adding city heat map layer...\")\n",
    "        xy, w = _city_centroids_and_weights(city_data)\n",
    "        if xy is not None:\n",
    "            # Use KDE with 10km bandwidth\n",
    "            city_kde = _evaluate_fftkde_on_grid(xy, w, grid_params, bw_meters=10000.0)\n",
    "            # Add to output with multiplier for visibility\n",
    "            output_grid += city_kde * 10  # Multiplier to make KDE visible (KDE max ~0.25, need ~25 to compete with log values)\n",
    "        else:\n",
    "            print(f\"      No valid city centroids for KDE\")\n",
    "\n",
    "    # 5. Facilities (unchanged)\n",
    "    facility_data = data[data['scale_level'] == 'FACILITY']\n",
    "    if len(facility_data) > 0:\n",
    "        output_grid += create_facility_raster(data, grid_params)\n",
    "\n",
    "    return output_grid\n",
    "\n",
    "# ==============================================================================\n",
    "# BONUS: KDEpy (FFTKDE) city-centroid KDE per-bin and cumulative\n",
    "# ==============================================================================\n",
    "\n",
    "def _city_centroids_and_weights(city_data):\n",
    "    \"\"\"\n",
    "    Return (N,2) array of centroid coordinates (already projected) and weights.\n",
    "    Falls back to .centroid for polygons; passes through points. Skips invalids.\n",
    "    \"\"\"\n",
    "    xs, ys, ws = [], [], []\n",
    "    if len(city_data) == 0:\n",
    "        return None, None\n",
    "\n",
    "    city_counts = city_data.groupby('matched_name')['count'].sum()\n",
    "\n",
    "    for city_name, w in city_counts.items():\n",
    "        geom = cities_lookup_proj.get(city_name)\n",
    "        if geom is None:\n",
    "            continue\n",
    "        if isinstance(geom, (Polygon, MultiPolygon)):\n",
    "            c = geom.centroid\n",
    "        elif isinstance(geom, Point):\n",
    "            c = geom\n",
    "        else:\n",
    "            c = getattr(geom, 'centroid', None)\n",
    "            if c is None:\n",
    "                continue\n",
    "        xs.append(c.x)\n",
    "        ys.append(c.y)\n",
    "        ws.append(float(w))\n",
    "    if len(xs) == 0:\n",
    "        return None, None\n",
    "    xy = np.column_stack([np.array(xs, dtype='float64'),\n",
    "                          np.array(ys, dtype='float64')])\n",
    "    w = np.array(ws, dtype='float64')\n",
    "    return xy, w\n",
    "\n",
    "\n",
    "def _evaluate_fftkde_on_grid(xy, weights, grid_params, bw_meters=10000.0):\n",
    "    \"\"\"\n",
    "    Create a kernel density heat map like ArcGIS Pro's point density tool.\n",
    "\n",
    "    Takes city centroid points with tweet counts and creates a smooth density\n",
    "    surface with the specified search radius (bandwidth).\n",
    "\n",
    "    Returns float32 array aligned to raster orientation (row 0 = top).\n",
    "    \"\"\"\n",
    "    from scipy.stats import gaussian_kde\n",
    "\n",
    "    H, W = grid_params['height'], grid_params['width']\n",
    "    xmin, ymin, xmax, ymax = grid_params['bounds']\n",
    "    cs = grid_params['cell_size']\n",
    "\n",
    "    # Build grid cell centers\n",
    "    xs = xmin + (np.arange(W) + 0.5) * cs\n",
    "    ys = ymin + (np.arange(H) + 0.5) * cs\n",
    "    X, Y = np.meshgrid(xs, ys, indexing='xy')\n",
    "    positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "\n",
    "    # Prepare data for scipy: shape (2, n_points)\n",
    "    data = xy.T\n",
    "\n",
    "    # Calculate bandwidth to achieve desired search radius in meters\n",
    "    data_std = np.sqrt((np.var(data[0, :]) + np.var(data[1, :])) / 2)\n",
    "    bw_method = bw_meters / data_std if data_std > 0 else 0.1\n",
    "\n",
    "    try:\n",
    "        # Fit gaussian KDE with weights\n",
    "        kde = gaussian_kde(data, bw_method=bw_method, weights=weights)\n",
    "\n",
    "        # Evaluate - returns probability density (per square meter)\n",
    "        z_flat = kde(positions)\n",
    "\n",
    "        # Convert from density (per m²) to total count in each cell\n",
    "        # Multiply by: (1) cell area and (2) sum of weights\n",
    "        cell_area = cs * cs  # square meters\n",
    "        total_weight = np.sum(weights)\n",
    "        z_flat = z_flat * cell_area * total_weight\n",
    "\n",
    "        # Reshape and flip\n",
    "        z = z_flat.reshape(H, W)\n",
    "        z = np.flipud(z).astype('float32')\n",
    "\n",
    "        print(f\"      City KDE: min={z.min():.3f}, max={z.max():.3f}, mean={z.mean():.3f}\")\n",
    "\n",
    "        return z\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"      ERROR: City KDE failed: {e}\")\n",
    "        return np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "def process_hurricane(hurricane_name, gdf_proj, interval_counts, time_bins, timestamp_dict):\n",
    "    \"\"\"\n",
    "    Process a single hurricane through all time bins\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"PROCESSING: {hurricane_name.upper()}\")\n",
    "    print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "    # hurricane output root\n",
    "    hurricane_dir = os.path.join(output_dir, hurricane_name.lower())\n",
    "    os.makedirs(hurricane_dir, exist_ok=True)\n",
    "\n",
    "    # Main cumulative grid (includes states, counties, cities KDE, facilities)\n",
    "    cumulative_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "\n",
    "    for idx, time_bin in enumerate(time_bins):\n",
    "        # Filter this bin\n",
    "        current_data = interval_counts[interval_counts['unix_timestamp'] == time_bin]\n",
    "\n",
    "        # Create hierarchical raster (now includes city KDE)\n",
    "        incremental_grid = create_hierarchical_rasters(current_data, grid_params, time_bin)\n",
    "\n",
    "        # Update cumulative\n",
    "        cumulative_grid += incremental_grid\n",
    "\n",
    "        # Save rasters\n",
    "        save_raster(incremental_grid, hurricane_dir, hurricane_name, time_bin, 'increment', timestamp_dict)\n",
    "        save_raster(cumulative_grid, hurricane_dir, hurricane_name, time_bin, 'cumulative', timestamp_dict)\n",
    "\n",
    "        print(f\"  Incremental max:       {np.max(incremental_grid):.3f}\")\n",
    "        print(f\"  Cumulative max:        {np.max(cumulative_grid):.3f}\")\n",
    "\n",
    "    print(f\"\\n{hurricane_name.upper()} processing complete!\")\n",
    "    print(f\"Output saved to: {hurricane_dir}\")\n",
    "    return\n",
    "\n",
    "# ==============================================================================\n",
    "# PLACEHOLDER FUNCTIONS (UNCHANGED)\n",
    "# ==============================================================================\n",
    "\n",
    "def create_facility_raster(data, grid_params):\n",
    "    \"\"\"Create KDE raster for facility points with strong hotspot multiplier\"\"\"\n",
    "    print(\"    [FACILITY] Creating facility raster...\")\n",
    "\n",
    "    facility_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "    facility_data = data[data['scale_level'] == 'FACILITY']\n",
    "    if len(facility_data) == 0:\n",
    "        print(\"      No facility-level tweets in this time bin\")\n",
    "        return facility_grid\n",
    "\n",
    "    facility_counts = facility_data.groupby('matched_name')['count'].sum()\n",
    "    sigma_meters = 2 * grid_params['cell_size']\n",
    "    sigma_pixels = sigma_meters / grid_params['cell_size']\n",
    "    facility_multiplier = 10\n",
    "\n",
    "    facilities_processed = 0\n",
    "    for facility_name, tweet_count in facility_counts.items():\n",
    "        rows = facility_data[facility_data['matched_name'] == facility_name]\n",
    "        if len(rows) == 0:\n",
    "            continue\n",
    "        facility_point = rows.iloc[0]['matched_geom']\n",
    "        if hasattr(facility_point, 'x') and hasattr(facility_point, 'y'):\n",
    "            point_geoseries = gpd.GeoSeries([facility_point], crs='EPSG:4326')\n",
    "            point_proj = point_geoseries.to_crs(grid_params['crs']).iloc[0]\n",
    "            px = (point_proj.x - grid_params['bounds'][0]) / grid_params['cell_size']\n",
    "            py = (grid_params['bounds'][3] - point_proj.y) / grid_params['cell_size']\n",
    "            if 0 <= px < grid_params['width'] and 0 <= py < grid_params['height']:\n",
    "                point_grid = np.zeros((grid_params['height'], grid_params['width']), dtype=np.float32)\n",
    "                point_grid[int(py), int(px)] = tweet_count\n",
    "                # simple gaussian via pixels (kept as-is)\n",
    "                from scipy.ndimage import gaussian_filter\n",
    "                kernel_grid = gaussian_filter(point_grid, sigma=sigma_pixels, mode='constant', cval=0)\n",
    "                facility_grid += kernel_grid * facility_multiplier\n",
    "                facilities_processed += 1\n",
    "            else:\n",
    "                print(f\"      WARNING: Facility '{facility_name}' outside grid bounds\")\n",
    "        else:\n",
    "            print(f\"      WARNING: Invalid geometry for facility '{facility_name}'\")\n",
    "\n",
    "    print(f\"      Processed {facilities_processed} facilities with sigma={sigma_pixels:.2f} pixels\")\n",
    "    return facility_grid\n",
    "\n",
    "\n",
    "def save_raster(grid, output_dir, hurricane_name, time_bin, raster_type, timestamp_dict):\n",
    "    \"\"\"Save raster as GeoTIFF with embedded time metadata\"\"\"\n",
    "    type_dir = os.path.join(output_dir, raster_type)\n",
    "    os.makedirs(type_dir, exist_ok=True)\n",
    "\n",
    "    time_str = timestamp_dict[time_bin].strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"{hurricane_name}_tweets_{time_str}.tif\"\n",
    "    filepath = os.path.join(type_dir, filename)\n",
    "\n",
    "    with rasterio.open(\n",
    "        filepath, 'w',\n",
    "        driver='GTiff',\n",
    "        height=grid_params['height'],\n",
    "        width=grid_params['width'],\n",
    "        count=1,\n",
    "        dtype=grid.dtype,\n",
    "        crs=grid_params['crs'],\n",
    "        transform=grid_params['transform'],\n",
    "        compress='lzw'\n",
    "    ) as dst:\n",
    "        dst.write(grid, 1)\n",
    "\n",
    "        # *** EMBED TIME METADATA ***\n",
    "        dst.update_tags(1,\n",
    "            time_bin=str(time_bin),\n",
    "            timestamp=time_str,\n",
    "            datetime_iso=timestamp_dict[time_bin].isoformat()\n",
    "        )\n",
    "        dst.set_band_description(1, f\"Tweet density - {time_str}\")\n",
    "\n",
    "    print(f\"    Saved: {raster_type}/{filename}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECUTE PROCESSING FOR BOTH HURRICANES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING RASTERIZATION PROCESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "process_hurricane('francine', francine_proj, francine_interval_counts, francine_time_bins, francine_timestamp_dict)\n",
    "process_hurricane('helene', helene_proj, helene_interval_counts, helene_time_bins, helene_timestamp_dict)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL PROCESSING COMPLETE! ✓\")\n",
    "print(\"=\" * 60)\n"
   ],
   "id": "2e1bc16e47f51e00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import rasterio\n",
    "path = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\cumulative\\helene_tweets_20240927_000000.tif\"\n",
    "with rasterio.open(path) as src:\n",
    "    tags = src.tags(1)\n",
    "    print(tags)  # Returns '20240926_000000'"
   ],
   "id": "20e0a59b6c5a69e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import arcpy\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Note this is to be inserted into the python command window\n",
    "# Paths\n",
    "gdb_path = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\Tweet_project.gdb\"\n",
    "\n",
    "\n",
    "raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\cumulative\"\n",
    "mosaic_name = \"helene_cumulative_mosaic_v4\"\n",
    "\n",
    "# raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\helene\\increment\"\n",
    "# mosaic_name = \"helene_increment_mosaic_v2\"\n",
    "#\n",
    "# raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\francine\\cumulative\"\n",
    "# mosaic_name = \"francine_cumulative_mosaic_v2\"\n",
    "#\n",
    "# raster_folder = r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\rasters_output\\francine\\increment\"\n",
    "# mosaic_name = \"francine_increment_mosaic_v2\"\n",
    "\n",
    "\n",
    "\n",
    "# Create geodatabase if it doesn't exist\n",
    "if not arcpy.Exists(gdb_path):\n",
    "    arcpy.CreateFileGDB_management(os.path.dirname(gdb_path), os.path.basename(gdb_path))\n",
    "\n",
    "# Create mosaic dataset\n",
    "mosaic_path = os.path.join(gdb_path, mosaic_name)\n",
    "if arcpy.Exists(mosaic_path):\n",
    "    arcpy.Delete_management(mosaic_path)\n",
    "\n",
    "arcpy.CreateMosaicDataset_management(gdb_path, mosaic_name, \"PROJCS['WGS_1984_Web_Mercator_Auxiliary_Sphere',GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Mercator_Auxiliary_Sphere'],PARAMETER['False_Easting',0.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',0.0],PARAMETER['Standard_Parallel_1',0.0],PARAMETER['Auxiliary_Sphere_Type',0.0],UNIT['Meter',1.0]]\")\n",
    "\n",
    "print(f\"Created mosaic dataset: {mosaic_path}\")\n",
    "\n",
    "# Add rasters to mosaic\n",
    "arcpy.AddRastersToMosaicDataset_management(\n",
    "    mosaic_path,\n",
    "    \"Raster Dataset\",\n",
    "    raster_folder,\n",
    "    filter=\"*.tif\"\n",
    ")\n",
    "\n",
    "print(\"Added rasters to mosaic dataset\")\n",
    "\n",
    "# Add time field\n",
    "arcpy.AddField_management(mosaic_path, \"date\", \"DATE\")\n",
    "\n",
    "# Calculate time from filename\n",
    "with arcpy.da.UpdateCursor(mosaic_path, [\"Name\", \"date\"]) as cursor:\n",
    "    for row in cursor:\n",
    "        filename = row[0]\n",
    "        # Remove .tif extension and split\n",
    "        parts = filename.replace(\".tif\", \"\").split(\"_\")\n",
    "\n",
    "        # Join last two parts to get full timestamp: 20240926 + 080000\n",
    "        time_str = parts[-2] + parts[-1]  # Combines date and time\n",
    "\n",
    "        # Parse: 20240926080000 -> datetime\n",
    "        dt = datetime.strptime(time_str, \"%Y%m%d%H%M%S\")\n",
    "        print(f\"{filename} -> {time_str} -> {dt}\")\n",
    "        row[1] = dt\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"Time field populated\")\n",
    "\n",
    "# Configure mosaic properties\n",
    "arcpy.SetMosaicDatasetProperties_management(\n",
    "    mosaic_path,\n",
    "    start_time_field=\"date\"\n",
    ")\n",
    "\n",
    "print(\"Mosaic dataset configured with time dimension\")\n",
    "\n",
    "print(f\"\\nMosaic dataset complete: {mosaic_path}\")\n",
    "print(\"To apply symbology in ArcGIS Pro:\")\n",
    "print(f\"1. Add mosaic to map: {mosaic_path}\")\n",
    "print(f\"2. Right-click layer > Symbology > Import\")\n",
    "print(f\"3. Select: {symbology_file}\")\n",
    "print(\"4. Enable time slider to animate cumulative growth\")"
   ],
   "id": "80b96b516fb40db0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "space_time_cube_generator",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# SPACE-TIME CUBE GENERATION (NetCDF Format)\n",
    "# Creates 3D cubes from raster time series without ArcPy\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from netCDF4 import Dataset\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def create_space_time_cube(raster_folder, output_nc_path, hurricane_name, cube_type):\n",
    "    \"\"\"\n",
    "    Convert a folder of time-sequenced rasters into a space-time cube (netCDF).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    raster_folder : str\n",
    "        Path to folder containing GeoTIFF rasters\n",
    "    output_nc_path : str\n",
    "        Output path for netCDF file\n",
    "    hurricane_name : str\n",
    "        Name of hurricane (e.g., 'helene')\n",
    "    cube_type : str\n",
    "        Type of cube ('increment' or 'cumulative')\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating space-time cube for {hurricane_name} ({cube_type})...\")\n",
    "\n",
    "    # Get all raster files sorted by timestamp\n",
    "    raster_files = sorted(glob.glob(os.path.join(raster_folder, '*.tif')))\n",
    "\n",
    "    if len(raster_files) == 0:\n",
    "        print(f\"ERROR: No rasters found in {raster_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Found {len(raster_files)} rasters\")\n",
    "\n",
    "    # Read first raster to get dimensions and metadata\n",
    "    with rasterio.open(raster_files[0]) as src:\n",
    "        height = src.height\n",
    "        width = src.width\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "\n",
    "    print(f\"  Grid dimensions: {width} x {height}\")\n",
    "\n",
    "    # Parse timestamps from filenames\n",
    "    timestamps = []\n",
    "    for f in raster_files:\n",
    "        # Extract timestamp from filename: hurricane_tweets_YYYYMMDD_HHMMSS.tif\n",
    "        basename = os.path.basename(f)\n",
    "        parts = basename.replace('.tif', '').split('_')\n",
    "        time_str = parts[-2] + parts[-1]  # YYYYMMDDHHMMSS\n",
    "        dt = datetime.strptime(time_str, '%Y%m%d%H%M%S')\n",
    "        timestamps.append(dt)\n",
    "\n",
    "    # Create netCDF file\n",
    "    print(f\"  Creating netCDF file: {output_nc_path}\")\n",
    "    nc = Dataset(output_nc_path, 'w', format='NETCDF4')\n",
    "\n",
    "    # Create dimensions\n",
    "    nc.createDimension('x', width)\n",
    "    nc.createDimension('y', height)\n",
    "    nc.createDimension('time', len(raster_files))\n",
    "\n",
    "    # Create coordinate variables\n",
    "    x_var = nc.createVariable('x', 'f8', ('x',))\n",
    "    y_var = nc.createVariable('y', 'f8', ('y',))\n",
    "    time_var = nc.createVariable('time', 'f8', ('time',))\n",
    "\n",
    "    # Create data variable\n",
    "    data_var = nc.createVariable('tweet_density', 'f4', ('time', 'y', 'x'),\n",
    "                                   fill_value=-9999, zlib=True, complevel=4)\n",
    "\n",
    "    # Set coordinate values\n",
    "    x_coords = np.arange(width) * transform.a + transform.c  # X coordinates\n",
    "    y_coords = np.arange(height) * transform.e + transform.f  # Y coordinates\n",
    "    x_var[:] = x_coords\n",
    "    y_var[:] = y_coords\n",
    "\n",
    "    # Convert timestamps to numeric (days since epoch)\n",
    "    epoch = datetime(1970, 1, 1)\n",
    "    time_values = [(t - epoch).total_seconds() / 86400.0 for t in timestamps]\n",
    "    time_var[:] = time_values\n",
    "\n",
    "    # Add metadata\n",
    "    nc.title = f'{hurricane_name.capitalize()} Hurricane Tweet Activity ({cube_type})'\n",
    "    nc.institution = 'Generated from tweet geolocation data'\n",
    "    nc.source = 'Twitter/X social media data'\n",
    "    nc.hurricane = hurricane_name\n",
    "    nc.cube_type = cube_type\n",
    "    nc.Conventions = 'CF-1.6'\n",
    "\n",
    "    # Variable metadata\n",
    "    x_var.units = 'meters'\n",
    "    x_var.long_name = 'x coordinate (EPSG:3857)'\n",
    "    x_var.standard_name = 'projection_x_coordinate'\n",
    "\n",
    "    y_var.units = 'meters'\n",
    "    y_var.long_name = 'y coordinate (EPSG:3857)'\n",
    "    y_var.standard_name = 'projection_y_coordinate'\n",
    "\n",
    "    time_var.units = 'days since 1970-01-01 00:00:00'\n",
    "    time_var.long_name = 'time'\n",
    "    time_var.standard_name = 'time'\n",
    "    time_var.calendar = 'gregorian'\n",
    "\n",
    "    data_var.units = 'tweet_activity_index'\n",
    "    data_var.long_name = 'Tweet Activity Density'\n",
    "    data_var.description = f'{cube_type.capitalize()} tweet activity from hierarchical rasterization (states, counties, cities KDE, facilities)'\n",
    "\n",
    "    # Load and write raster data\n",
    "    print(f\"  Loading raster data...\")\n",
    "    for i, raster_path in enumerate(raster_files):\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            data = src.read(1)  # Read first band\n",
    "            data_var[i, :, :] = data\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"    Loaded {i + 1}/{len(raster_files)} time slices\")\n",
    "\n",
    "    # Close netCDF file\n",
    "    nc.close()\n",
    "\n",
    "    print(f\"  ✓ Space-time cube created successfully!\")\n",
    "    print(f\"    Dimensions: {width} x {height} x {len(raster_files)} (X x Y x Time)\")\n",
    "    print(f\"    Time range: {timestamps[0]} to {timestamps[-1]}\")\n",
    "    print(f\"    Output: {output_nc_path}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# GENERATE SPACE-TIME CUBES FOR ALL DATASETS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING SPACE-TIME CUBES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create output directory for cubes\n",
    "cubes_dir = os.path.join(local_path, 'space_time_cubes')\n",
    "os.makedirs(cubes_dir, exist_ok=True)\n",
    "\n",
    "# Generate cubes for each hurricane and type\n",
    "datasets = [\n",
    "    ('helene', 'cumulative'),\n",
    "    ('helene', 'increment'),\n",
    "    ('francine', 'cumulative'),\n",
    "    ('francine', 'increment')\n",
    "]\n",
    "\n",
    "for hurricane, cube_type in datasets:\n",
    "    raster_folder = os.path.join(output_dir, hurricane, cube_type)\n",
    "    output_nc = os.path.join(cubes_dir, f'{hurricane}_{cube_type}_cube.nc')\n",
    "\n",
    "    if os.path.exists(raster_folder):\n",
    "        create_space_time_cube(raster_folder, output_nc, hurricane, cube_type)\n",
    "    else:\n",
    "        print(f\"\\nWARNING: Folder not found: {raster_folder}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SPACE-TIME CUBE GENERATION COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOutput directory: {cubes_dir}\")\n",
    "print(\"\\nYou can now:\")\n",
    "print(\"  1. Open these .nc files in ArcGIS Pro as multidimensional rasters\")\n",
    "print(\"  2. Use Python libraries like xarray to analyze them\")\n",
    "print(\"  3. Visualize with tools like Panoply or ncview\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
