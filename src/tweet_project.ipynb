{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:09.190604Z",
     "start_time": "2025-09-12T20:27:09.189025Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:09.200720Z",
     "start_time": "2025-09-12T20:27:09.194308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def get_project_root():\n",
    "    \"\"\"\n",
    "    Gets the absolute path to the project's root folder.\n",
    "    This version works in a Jupyter Notebook assuming the notebook is in a '/src' subdirectory.\n",
    "    \"\"\"\n",
    "    # Get the current working directory of the notebook\n",
    "    notebook_path = os.getcwd()\n",
    "\n",
    "    # If the notebook is in 'src', go up one level to the project root\n",
    "    if os.path.basename(notebook_path) == 'src':\n",
    "        return os.path.dirname(notebook_path)\n",
    "\n",
    "    # Otherwise, assume the notebook is already in the project root\n",
    "    return notebook_path\n",
    "def get_data_file_path(*path_segments):\n",
    "    \"\"\"Builds a full path to any data file from the project root.\"\"\"\n",
    "    project_root = get_project_root()\n",
    "    return os.path.join(project_root, *path_segments)\n",
    "\n",
    "\n",
    "def get_geojson(label):\n",
    "    \"\"\"Get path to helene.geojson\"\"\"\n",
    "    geojson = get_data_file_path('data', 'geojson', f'{label}.geojson')\n",
    "    return gpd.read_file(geojson)\n",
    "\n",
    "\n",
    "def get_cities():\n",
    "    \"\"\"\n",
    "    Load US cities data, starting with GeoJSON and supplementing with CSV data.\n",
    "    Combines both sources to maximize data coverage.\n",
    "    \"\"\"\n",
    "    cities_gdf = None\n",
    "\n",
    "    # Try to load GeoJSON first\n",
    "    try:\n",
    "        geojson_path = get_data_file_path('data', 'geojson', f'us_cities.geojson')\n",
    "        cities_gdf = gpd.read_file(geojson_path, dtype={13: str})\n",
    "\n",
    "        if cities_gdf.crs is None:\n",
    "            cities_gdf = cities_gdf.set_crs(\"EPSG:4326\")\n",
    "\n",
    "        print(f\"✓ Loaded {len(cities_gdf)} cities from GeoJSON\")\n",
    "\n",
    "    except (FileNotFoundError, Exception) as e:\n",
    "        print(f\"⚠ GeoJSON loading failed ({e})\")\n",
    "\n",
    "    # Load CSV data\n",
    "    try:\n",
    "        df_path = get_data_file_path('data', 'tables', 'cities1000.csv')\n",
    "        df = pd.read_csv(df_path, dtype={13: str})\n",
    "        csv_cities_df = df[\n",
    "            (df['country_code'] == 'US') &\n",
    "            (df['feature_class'] == 'P') &\n",
    "            (df['population'].notna()) &\n",
    "            (df['latitude'].notna()) &\n",
    "            (df['longitude'].notna())\n",
    "            ].reset_index(drop=True)\n",
    "\n",
    "        csv_cities_gdf = gpd.GeoDataFrame(\n",
    "            csv_cities_df,\n",
    "            geometry=gpd.points_from_xy(csv_cities_df.longitude, csv_cities_df.latitude),\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "\n",
    "        print(f\"✓ Loaded {len(csv_cities_gdf)} cities from CSV\")\n",
    "\n",
    "        # If we have both, supplement GeoJSON with missing CSV cities\n",
    "        if cities_gdf is not None:\n",
    "            # Find missing cities (by geonameid if available, otherwise by name)\n",
    "            if 'geonameid' in cities_gdf.columns and 'geonameid' in csv_cities_gdf.columns:\n",
    "                existing_ids = set(cities_gdf['geonameid'])\n",
    "                missing_cities = csv_cities_gdf[~csv_cities_gdf['geonameid'].isin(existing_ids)]\n",
    "            else:\n",
    "                # Fall back to name-based comparison\n",
    "                existing_names = set(cities_gdf['name']) if 'name' in cities_gdf.columns else set()\n",
    "                missing_cities = csv_cities_gdf[~csv_cities_gdf['name'].isin(existing_names)]\n",
    "\n",
    "            if len(missing_cities) > 0:\n",
    "                # Combine datasets\n",
    "                cities_gdf = pd.concat([cities_gdf, missing_cities], ignore_index=True)\n",
    "                print(f\"✓ Added {len(missing_cities)} supplemental cities from CSV\")\n",
    "        else:\n",
    "\n",
    "            # If GeoJSON failed, use CSV only\n",
    "            cities_gdf = csv_cities_gdf\n",
    "\n",
    "    except Exception as e:\n",
    "        if cities_gdf is None:\n",
    "            raise Exception(f\"Failed to load both GeoJSON and CSV: {e}\")\n",
    "        print(f\"⚠ CSV supplementation failed: {e}\")\n",
    "\n",
    "    print(f\"✓ Total cities loaded: {len(cities_gdf)}\")\n",
    "    return cities_gdf\n",
    "\n",
    "\n",
    "def get_states():\n",
    "    gdf_path = get_data_file_path('data', 'shape_files', \"cb_2023_us_state_20m.shp\")\n",
    "    return gpd.read_file(gdf_path)\n",
    "\n",
    "\n",
    "def get_counties():\n",
    "    gdf_path = get_data_file_path('data', 'shape_files', \"cb_2023_us_county_20m.shp\")\n",
    "    return gpd.read_file(gdf_path)\n",
    "\n"
   ],
   "id": "1c82b3be70317e6a",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:10.767188Z",
     "start_time": "2025-09-12T20:27:09.204260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tweets_gdf_helene = get_geojson('helene').to_crs(\"EPSG:4326\")\n",
    "us_cities_gdf_helene = get_cities().to_crs(\"EPSG:4326\")\n",
    "us_states_gdf_helene = get_states().to_crs(\"EPSG:4326\")\n",
    "us_counties_gdf_helene = get_counties().to_crs(\"EPSG:4326\")\n",
    "\n",
    "tweets_gdf_francine = get_geojson('francine').to_crs(\"EPSG:4326\")\n",
    "us_cities_gdf_francine = get_cities().to_crs(\"EPSG:4326\")\n",
    "us_states_gdf_francine = get_states().to_crs(\"EPSG:4326\")\n",
    "us_counties_gdf_francine = get_counties().to_crs(\"EPSG:4326\")"
   ],
   "id": "5a0bb6165ca50826",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\OneDrive\\Music\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:198: RuntimeWarning: driver GeoJSON does not support open option DTYPE\n",
      "  return ogr_read(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 7471 cities from GeoJSON\n",
      "✓ Loaded 17244 cities from CSV\n",
      "✓ Added 9773 supplemental cities from CSV\n",
      "✓ Total cities loaded: 17244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\OneDrive\\Music\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:198: RuntimeWarning: driver GeoJSON does not support open option DTYPE\n",
      "  return ogr_read(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 7471 cities from GeoJSON\n",
      "✓ Loaded 17244 cities from CSV\n",
      "✓ Added 9773 supplemental cities from CSV\n",
      "✓ Total cities loaded: 17244\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:10.943736Z",
     "start_time": "2025-09-12T20:27:10.843392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "projected_crs = \"EPSG:3857\"\n",
    "\n",
    "tweets_with_states_helene = gpd.sjoin(tweets_gdf_helene, us_states_gdf_helene, predicate='within', lsuffix='_tweet', rsuffix='_state')\n",
    "tweets_with_counties_helene = gpd.sjoin(tweets_with_states_helene, us_counties_gdf_helene, predicate='within', lsuffix='_tweet',\n",
    "                                 rsuffix='_county')\n",
    "tweets_proj_helene = tweets_with_counties_helene.to_crs(projected_crs)\n",
    "cities_proj_helene = us_cities_gdf_helene.to_crs(projected_crs)\n",
    "tweets_with_cities_proj_helene = gpd.sjoin_nearest(\n",
    "    tweets_proj_helene, \n",
    "    cities_proj_helene, \n",
    "    max_distance=15000,  # <-- IMPORTANT: Distance is now in meters\n",
    "    distance_col='distance_to_city')\n",
    "\n",
    "tweets_with_states_francine = gpd.sjoin(tweets_gdf_francine, us_states_gdf_francine, predicate='within', lsuffix='_tweet', rsuffix='_state')\n",
    "tweets_with_counties_francine = gpd.sjoin(tweets_with_states_francine, us_counties_gdf_francine, predicate='within', lsuffix='_tweet',\n",
    "                                 rsuffix='_county')\n",
    "tweets_proj_francine = tweets_with_counties_francine.to_crs(projected_crs)\n",
    "cities_proj_francine = us_cities_gdf_francine.to_crs(projected_crs)\n",
    "tweets_with_cities_proj_francine = gpd.sjoin_nearest(\n",
    "    tweets_proj_francine, \n",
    "    cities_proj_francine, \n",
    "    max_distance=15000,  # <-- IMPORTANT: Distance is now in meters\n",
    "    distance_col='distance_to_city')"
   ],
   "id": "c946e5465519db8d",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:10.953968Z",
     "start_time": "2025-09-12T20:27:10.947598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_and_select_columns(tweets_with_cities):\n",
    "    \"\"\"Select and rename essential columns\"\"\"\n",
    "    cleaned = tweets_with_cities[[\n",
    "        'FAC', 'LOC', 'GPE', 'time', 'Latitude', 'Longitude',\n",
    "        'STUSPS__tweet', 'NAME__tweet', 'NAME__county', 'GEOID__county',\n",
    "        'name', 'geonameid', 'population'\n",
    "    ]].copy()\n",
    "\n",
    "    cleaned = cleaned.rename(columns={\n",
    "        'STUSPS__tweet': 'state_code',\n",
    "        'NAME__tweet': 'state_name',\n",
    "        'NAME__county': 'county_name',\n",
    "        'GEOID__county': 'county_fips',\n",
    "        'name': 'city_name',\n",
    "        'geonameid': 'city_id'\n",
    "    })\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "final_tweets_helene = clean_and_select_columns(tweets_with_cities_proj_helene)\n",
    "final_tweets_francine = clean_and_select_columns(tweets_with_cities_proj_francine)"
   ],
   "id": "d8b79bf129e58517",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:32.100213Z",
     "start_time": "2025-09-12T20:27:10.959249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_temporal_aggregations(tweets_df, time_bins):\n",
    "    \"\"\"Create aggregated counts for each time bin\"\"\"\n",
    "    temporal_data = {}\n",
    "\n",
    "    for bin_time in time_bins:\n",
    "        bin_tweets = tweets_df[tweets_df['bin'] == bin_time]\n",
    "\n",
    "        state_counts = bin_tweets.groupby('state_code').size().reset_index(name='tweet_count')\n",
    "        county_counts = bin_tweets.groupby('county_fips').size().reset_index(name='tweet_count')\n",
    "        city_counts = bin_tweets.groupby('city_id').size().reset_index(name='tweet_count')\n",
    "\n",
    "        temporal_data[bin_time] = {\n",
    "            'states': state_counts,\n",
    "            'counties': county_counts,\n",
    "            'cities': city_counts\n",
    "        }\n",
    "    return temporal_data\n",
    "\n",
    "def _get_join_cols(level_name: str):\n",
    "    \"\"\"\n",
    "    Return (join_col_on_geometry, data_col_on_temporal_counts)\n",
    "    \"\"\"\n",
    "    if level_name == 'states':\n",
    "        return 'STUSPS', 'state_code'\n",
    "    elif level_name == 'counties':\n",
    "        return 'GEOID', 'county_fips'\n",
    "    elif level_name == 'cities':\n",
    "        return 'geonameid', 'city_id'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown level_name: {level_name}\")\n",
    "\n",
    "def create_long_format_shapefile(temporal_data, gdf, output_path, level_name='states'):\n",
    "    \"\"\"\n",
    "    Option 2: Create shapefile with repeated geometries (long format)\n",
    "    Each geometry appears once per time period\n",
    "\n",
    "    Args:\n",
    "        temporal_data: Your temporal aggregation data\n",
    "        gdf: GeoDataFrame (states or counties)\n",
    "        output_path: Where to save the shapefile\n",
    "        level_name: 'states' or 'counties'\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "\n",
    "    # Get the appropriate join columns\n",
    "    join_col, data_col = _get_join_cols(level_name)\n",
    "    # For each time period, create records\n",
    "    for bin_time, counts_data in temporal_data.items():\n",
    "        time_counts = counts_data[level_name]\n",
    "\n",
    "        # Merge with GeoDataFrame\n",
    "        merged = gdf.merge(\n",
    "            time_counts,\n",
    "            left_on=join_col,\n",
    "            right_on=data_col,\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Fill NaN tweet counts with 0\n",
    "        merged['tweet_count'] = merged['tweet_count'].fillna(0)\n",
    "\n",
    "        # Add timestamp columns\n",
    "        merged['timestamp'] = bin_time\n",
    "        merged['time_str'] = bin_time.strftime('%Y-%m-%d %H:%M')\n",
    "        merged['unix_time'] = int(bin_time.timestamp())\n",
    "\n",
    "        # Keep essential columns + geometry\n",
    "        essential_cols = [join_col, 'NAME', 'geometry', 'timestamp', 'time_str', 'unix_time', 'tweet_count']\n",
    "        if level_name == 'counties':\n",
    "            essential_cols.append('STATEFP')  # State FIPS for counties\n",
    "\n",
    "        # Filter to existing columns\n",
    "        available_cols = [col for col in essential_cols if col in merged.columns]\n",
    "        merged_clean = merged[available_cols].copy()\n",
    "\n",
    "        all_records.append(merged_clean)\n",
    "\n",
    "    # Combine all time periods\n",
    "    result_gdf = pd.concat(all_records, ignore_index=True)\n",
    "\n",
    "    # Clean column names for shapefile\n",
    "    result_gdf = clean_shapefile_columns(result_gdf)\n",
    "    # result_gdf = result_gdf[result_gdf['tweets'] > 0]\n",
    "    # Save as shapefile\n",
    "    result_gdf.to_file(output_path)\n",
    "    print(f\"Long format shapefile saved: {output_path}\")\n",
    "    print(f\"Total records: {len(result_gdf)} (geometries × time periods)\")\n",
    "\n",
    "    return result_gdf\n",
    "def clean_shapefile_columns(gdf):\n",
    "    \"\"\"\n",
    "    Clean column names to be shapefile-compatible\n",
    "    Shapefiles have 10-character field name limits\n",
    "    \"\"\"\n",
    "    result = gdf.copy()\n",
    "\n",
    "    # Rename long column names\n",
    "    rename_dict = {}\n",
    "    for col in result.columns:\n",
    "        if col == 'geometry':\n",
    "            continue\n",
    "        if len(col) > 10:\n",
    "            # Create shortened version\n",
    "            if 'tweet_count' in col:\n",
    "                rename_dict[col] = 'tweets'\n",
    "            elif 'timestamp' in col:\n",
    "                rename_dict[col] = 'time_stamp'\n",
    "            elif col.startswith('t_'):\n",
    "                rename_dict[col] = col[:10]  # Keep first 10 chars\n",
    "            else:\n",
    "                rename_dict[col] = col[:10]\n",
    "\n",
    "    if rename_dict:\n",
    "        result = result.rename(columns=rename_dict)\n",
    "\n",
    "    return result\n",
    "def convert_temporal_data_to_shapefiles(final_tweets, us_states_gdf, us_counties_gdf,us_cities_gdf, label):\n",
    "    \"\"\"\n",
    "    Main function to convert all your temporal data to shapefiles\n",
    "    \"\"\"\n",
    "    # Prepare temporal data (same as your existing code)\n",
    "    final_tweets['time'] = pd.to_datetime(final_tweets['time'])\n",
    "    final_tweets['bin'] = final_tweets['time'].dt.floor('4h')\n",
    "    time_bins = sorted(final_tweets['bin'].unique())\n",
    "    temporal_data = create_temporal_aggregations(final_tweets, time_bins)\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = 'temporal_shapefiles_v2'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    states_long = create_long_format_shapefile(\n",
    "        temporal_data, us_states_gdf,\n",
    "        os.path.join(output_dir, f'{label}_states_long_format.shp'),\n",
    "        'states'\n",
    "    )\n",
    "\n",
    "    counties_long = create_long_format_shapefile(\n",
    "        temporal_data, us_counties_gdf,\n",
    "        os.path.join(output_dir, f'{label}_counties_long_format.shp'),\n",
    "        'counties'\n",
    "    )\n",
    "    cities_long   = create_long_format_shapefile(temporal_data, us_cities_gdf,\n",
    "                       os.path.join(output_dir, f'{label}_cities_long_v2.shp'),   'cities')\n",
    "convert_temporal_data_to_shapefiles(final_tweets_helene, us_states_gdf_helene, us_counties_gdf_helene, us_cities_gdf_helene, 'helene')\n",
    "convert_temporal_data_to_shapefiles(final_tweets_francine, us_states_gdf_francine, us_counties_gdf_francine, us_cities_gdf_francine, 'francine')\n"
   ],
   "id": "208e8dd5e1c931a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long format shapefile saved: temporal_shapefiles_v2\\helene_states_long_format.shp\n",
      "Total records: 572 (geometries × time periods)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\OneDrive\\Music\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field timestamp create as date field, though DateTime requested.\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\OneDrive\\Music\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field timestamp create as date field, though DateTime requested.\n",
      "  ogr_write(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long format shapefile saved: temporal_shapefiles_v2\\helene_counties_long_format.shp\n",
      "Total records: 35442 (geometries × time periods)\n",
      "Long format shapefile saved: temporal_shapefiles_v2\\helene_cities_long_v2.shp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\OneDrive\\Music\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field timestamp create as date field, though DateTime requested.\n",
      "  ogr_write(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total records: 189684 (geometries × time periods)\n",
      "Long format shapefile saved: temporal_shapefiles_v2\\francine_states_long_format.shp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\OneDrive\\Music\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field timestamp create as date field, though DateTime requested.\n",
      "  ogr_write(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total records: 2080 (geometries × time periods)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\OneDrive\\Music\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field timestamp create as date field, though DateTime requested.\n",
      "  ogr_write(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long format shapefile saved: temporal_shapefiles_v2\\francine_counties_long_format.shp\n",
      "Total records: 128880 (geometries × time periods)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\OneDrive\\Music\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field timestamp create as date field, though DateTime requested.\n",
      "  ogr_write(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long format shapefile saved: temporal_shapefiles_v2\\francine_cities_long_v2.shp\n",
      "Total records: 689760 (geometries × time periods)\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T20:27:32.193674Z",
     "start_time": "2025-09-12T20:27:32.191997Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3f20ea78419f0001",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
