{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T02:06:39.646752Z",
     "start_time": "2025-11-03T02:06:38.417940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "STEP 0: Parse Hurricane GeoJSON → 4-hour binned count CSVs\n",
    "=========================================================\n",
    "\n",
    "This script:\n",
    "  1. Loads your helene.geojson (and any other event GeoJSON)\n",
    "  2. Extracts GPE (state/county/city mentions), timestamps, coordinates\n",
    "  3. Bins events into 4-hour windows\n",
    "  4. Counts mentions per GPE per bin\n",
    "  5. Outputs CSVs ready to join to shapefiles in QGIS\n",
    "\n",
    "Dependencies: pandas, geopandas (pip install pandas geopandas)\n",
    "\n",
    "Input:  helene.geojson, francine.geojson (if available)\n",
    "Output:\n",
    "  - counts_helene_*.csv (per bin with GPE → count)\n",
    "  - bins_summary.csv (metadata on all bins)\n",
    "  - geometries_helene_*.shp (optional: points per bin for sanity check)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "INPUT_GEOJSONS = [\n",
    "    \"helene.geojson\",\n",
    "    \"francine.geojson\",  # uncomment if you have this\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = \"./counts_output\"\n",
    "BIN_HOURS = 4\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Parse a single GeoJSON into a GeoDataFrame\n",
    "# ============================================================================\n",
    "\n",
    "def parse_geojson(filepath):\n",
    "    \"\"\"\n",
    "    Load a GeoJSON and extract: event, time, GPE, coordinates.\n",
    "\n",
    "    Returns: GeoDataFrame with columns:\n",
    "      - event (str): name of event (e.g., 'helene')\n",
    "      - time (datetime): parsed timestamp\n",
    "      - GPE (str): Geopolitical Entity (state/county/city from the data)\n",
    "      - geometry (Point): lon/lat\n",
    "    \"\"\"\n",
    "    print(f\"\\n[*] Parsing {filepath}...\")\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    event_name = Path(filepath).stem  # 'helene' from 'helene.geojson'\n",
    "    rows = []\n",
    "    errors = 0\n",
    "\n",
    "    for i, feat in enumerate(data.get(\"features\", [])):\n",
    "        try:\n",
    "            props = feat.get(\"properties\", {})\n",
    "            geom = feat.get(\"geometry\", {})\n",
    "\n",
    "            # Extract GPE (state/county/city name)\n",
    "            gpe = props.get(\"GPE\", \"\").strip()\n",
    "            if not gpe:\n",
    "                continue  # Skip if no GPE\n",
    "\n",
    "            # Parse timestamp\n",
    "            time_str = props.get(\"time\", \"\")\n",
    "            if not time_str:\n",
    "                errors += 1\n",
    "                continue\n",
    "\n",
    "            # Handle ISO format with timezone\n",
    "            try:\n",
    "                time_dt = pd.to_datetime(time_str)\n",
    "            except:\n",
    "                errors += 1\n",
    "                continue\n",
    "\n",
    "            # Extract coordinates\n",
    "            if geom.get(\"type\") == \"Point\":\n",
    "                coords = geom.get(\"coordinates\", [None, None])\n",
    "                lon, lat = coords[0], coords[1]\n",
    "            else:\n",
    "                # If not a point, skip\n",
    "                errors += 1\n",
    "                continue\n",
    "\n",
    "            if lon is None or lat is None:\n",
    "                errors += 1\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"event\": event_name,\n",
    "                \"time\": time_dt,\n",
    "                \"GPE\": gpe,\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lon,\n",
    "                \"geometry\": Point(lon, lat)\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "            if i < 5:  # Only print first few errors\n",
    "                print(f\"  Warning: row {i} failed: {e}\")\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(rows, crs=\"EPSG:4326\")\n",
    "    print(f\"  ✓ Loaded {len(gdf)} features, {errors} errors\")\n",
    "\n",
    "    return gdf\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Bin and count mentions\n",
    "# ============================================================================\n",
    "\n",
    "def bin_and_count(gdf, bin_hours=4):\n",
    "    \"\"\"\n",
    "    Bin GeoDataFrame by time, count mentions per GPE per bin.\n",
    "\n",
    "    Returns: dict of (event, bin_start, bin_end) → DataFrame with (GPE, count)\n",
    "    \"\"\"\n",
    "    print(f\"\\n[*] Binning into {bin_hours}-hour windows...\")\n",
    "\n",
    "    # Add bin column\n",
    "    gdf[\"bin\"] = gdf[\"time\"].dt.floor(f\"{bin_hours}H\")\n",
    "\n",
    "    # Group by event + bin, then count GPE mentions\n",
    "    bin_dict = {}\n",
    "\n",
    "    for (event, bin_time), group in gdf.groupby([\"event\", \"bin\"]):\n",
    "        # Count each GPE in this bin\n",
    "        counts = group[\"GPE\"].value_counts().reset_index()\n",
    "        counts.columns = [\"GPE\", \"count\"]\n",
    "\n",
    "        # Store with readable bin end time\n",
    "        bin_start = bin_time\n",
    "        bin_end = bin_time + timedelta(hours=bin_hours)\n",
    "\n",
    "        key = (event, bin_start, bin_end)\n",
    "        bin_dict[key] = counts\n",
    "\n",
    "        print(f\"  ✓ {event} | {bin_start} → {bin_end}: {len(counts)} unique GPEs, {counts['count'].sum()} total mentions\")\n",
    "\n",
    "    return bin_dict\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Export counts to CSV (for QGIS joining)\n",
    "# ============================================================================\n",
    "\n",
    "def export_counts_csv(bin_dict, output_dir, gdf):\n",
    "    \"\"\"\n",
    "    Export one CSV per bin, ready to join to shapefiles in QGIS.\n",
    "\n",
    "    CSV format:\n",
    "      GPE, count, bin_start, bin_end, event\n",
    "    \"\"\"\n",
    "    print(f\"\\n[*] Exporting count CSVs...\")\n",
    "\n",
    "    bin_metadata = []\n",
    "\n",
    "    for (event, bin_start, bin_end), counts_df in sorted(bin_dict.items()):\n",
    "        # Add metadata columns\n",
    "        counts_df[\"bin_start\"] = bin_start\n",
    "        counts_df[\"bin_end\"] = bin_end\n",
    "        counts_df[\"event\"] = event\n",
    "\n",
    "        # Create filename\n",
    "        bin_str = bin_start.strftime(\"%Y%m%d_%H%M\")\n",
    "        filename = f\"counts_{event}_{bin_str}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Save\n",
    "        counts_df.to_csv(filepath, index=False)\n",
    "\n",
    "        print(f\"  ✓ {filename} ({len(counts_df)} GPEs)\")\n",
    "\n",
    "        # Add to metadata\n",
    "        bin_metadata.append({\n",
    "            \"event\": event,\n",
    "            \"bin_start\": bin_start,\n",
    "            \"bin_end\": bin_end,\n",
    "            \"csv_file\": filename,\n",
    "            \"unique_gpes\": len(counts_df),\n",
    "            \"total_mentions\": counts_df[\"count\"].sum()\n",
    "        })\n",
    "\n",
    "    # Export metadata summary\n",
    "    metadata_df = pd.DataFrame(bin_metadata)\n",
    "    metadata_path = os.path.join(output_dir, \"bins_summary.csv\")\n",
    "    metadata_df.to_csv(metadata_path, index=False)\n",
    "    print(f\"\\n  ✓ Summary: {metadata_path}\")\n",
    "\n",
    "    return metadata_df\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Create shapefiles per bin (optional, for QA)\n",
    "# ============================================================================\n",
    "\n",
    "def export_geometries_shp(gdf, bin_dict, output_dir):\n",
    "    \"\"\"\n",
    "    Optional: Export points per bin as shapefiles for visual QA in QGIS.\n",
    "\n",
    "    This lets you verify your binning and GPE extraction worked.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[*] Exporting geometries per bin (optional QA)...\")\n",
    "\n",
    "    for (event, bin_start, bin_end), counts_df in sorted(bin_dict.items()):\n",
    "        # Filter GDF to just this bin\n",
    "        bin_gdf = gdf[\n",
    "            (gdf[\"event\"] == event) &\n",
    "            (gdf[\"bin\"] == bin_start)\n",
    "        ].copy()\n",
    "\n",
    "        if len(bin_gdf) == 0:\n",
    "            continue\n",
    "\n",
    "        # Keep only needed columns\n",
    "        bin_gdf = bin_gdf[[\"GPE\", \"latitude\", \"longitude\", \"geometry\"]]\n",
    "\n",
    "        # Save shapefile\n",
    "        bin_str = bin_start.strftime(\"%Y%m%d_%H%M\")\n",
    "        shp_path = os.path.join(output_dir, f\"geometries_{event}_{bin_str}.shp\")\n",
    "        bin_gdf.to_file(shp_path, driver=\"ESRI Shapefile\")\n",
    "\n",
    "        print(f\"  ✓ geometries_{event}_{bin_str}.shp ({len(bin_gdf)} points)\")\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Print summary statistics\n",
    "# ============================================================================\n",
    "\n",
    "def print_summary(gdf, bin_dict, metadata_df):\n",
    "    \"\"\"Print a nice summary of what was processed.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\nTotal features loaded: {len(gdf)}\")\n",
    "    print(f\"Total bins created: {len(bin_dict)}\")\n",
    "    print(f\"Total unique GPEs: {gdf['GPE'].nunique()}\")\n",
    "    print(f\"\\nTime range: {gdf['time'].min()} → {gdf['time'].max()}\")\n",
    "    print(f\"Duration: {(gdf['time'].max() - gdf['time'].min()).days} days, {(gdf['time'].max() - gdf['time'].min()).seconds // 3600} hours\")\n",
    "\n",
    "    print(f\"\\nBreakdown by event:\")\n",
    "    for event in gdf[\"event\"].unique():\n",
    "        n = len(gdf[gdf[\"event\"] == event])\n",
    "        print(f\"  {event}: {n} features\")\n",
    "\n",
    "    print(f\"\\nTop 10 most-mentioned GPEs:\")\n",
    "    top_gpes = gdf[\"GPE\"].value_counts().head(10)\n",
    "    for gpe, count in top_gpes.items():\n",
    "        print(f\"  {gpe}: {count}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"✅ DONE! Output files in: {OUTPUT_DIR}\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Load your shapefiles (states, counties, cities) in QGIS\")\n",
    "    print(\"2. For each bin, join the corresponding counts_*.csv to the shapefile:\")\n",
    "    print(\"   - Right-click layer → Properties → Joins tab → Add Join\")\n",
    "    print(\"   - Join field: GPE (from CSV)\")\n",
    "    print(\"   - Target field: NAME (from shapefile)\")\n",
    "    print(\"3. Rasterize each joined layer using Processing → Raster → Rasterize\")\n",
    "    print(\"4. Use Raster Calculator to combine: (state*0.1) + (county*0.5) + (city*1.0)\")\n",
    "    print(\"5. Stack rasters into GeoPackage and enable Temporal Controller\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 1. Load all GeoJSONs\n",
    "    all_gdfs = []\n",
    "    for geojson_path in INPUT_GEOJSONS:\n",
    "        if Path(geojson_path).exists():\n",
    "            gdf = parse_geojson(geojson_path)\n",
    "            all_gdfs.append(gdf)\n",
    "        else:\n",
    "            print(f\"⚠️  Warning: {geojson_path} not found, skipping\")\n",
    "\n",
    "    if not all_gdfs:\n",
    "        print(\"❌ No GeoJSON files found!\")\n",
    "        exit(1)\n",
    "\n",
    "    # Combine all events into one GDF\n",
    "    gdf = pd.concat(all_gdfs, ignore_index=True)\n",
    "    print(f\"\\nTotal combined features: {len(gdf)}\")\n",
    "\n",
    "    # 2. Bin and count\n",
    "    bin_dict = bin_and_count(gdf, bin_hours=BIN_HOURS)\n",
    "\n",
    "    # 3. Export CSVs\n",
    "    metadata_df = export_counts_csv(bin_dict, OUTPUT_DIR, gdf)\n",
    "\n",
    "    # 4. Optional: Export geometries for QA\n",
    "    export_geometries_shp(gdf, bin_dict, OUTPUT_DIR)\n",
    "\n",
    "    # 5. Print summary\n",
    "    print_summary(gdf, bin_dict, metadata_df)"
   ],
   "id": "55e42e73887bcfbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: ./counts_output\n",
      "⚠️  Warning: helene.geojson not found, skipping\n",
      "❌ No GeoJSON files found!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 301\u001B[39m\n\u001B[32m    298\u001B[39m     exit(\u001B[32m1\u001B[39m)\n\u001B[32m    300\u001B[39m \u001B[38;5;66;03m# Combine all events into one GDF\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m301\u001B[39m gdf = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_gdfs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    302\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mTotal combined features: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(gdf)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    304\u001B[39m \u001B[38;5;66;03m# 2. Bin and count\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001B[39m, in \u001B[36mconcat\u001B[39m\u001B[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[39m\n\u001B[32m    379\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m copy \u001B[38;5;129;01mand\u001B[39;00m using_copy_on_write():\n\u001B[32m    380\u001B[39m     copy = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m382\u001B[39m op = \u001B[43m_Concatenator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    383\u001B[39m \u001B[43m    \u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    384\u001B[39m \u001B[43m    \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    385\u001B[39m \u001B[43m    \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    386\u001B[39m \u001B[43m    \u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m=\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    387\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    388\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlevels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlevels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    389\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    390\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverify_integrity\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverify_integrity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    391\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    392\u001B[39m \u001B[43m    \u001B[49m\u001B[43msort\u001B[49m\u001B[43m=\u001B[49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    393\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    395\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m op.get_result()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001B[39m, in \u001B[36m_Concatenator.__init__\u001B[39m\u001B[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001B[39m\n\u001B[32m    442\u001B[39m \u001B[38;5;28mself\u001B[39m.verify_integrity = verify_integrity\n\u001B[32m    443\u001B[39m \u001B[38;5;28mself\u001B[39m.copy = copy\n\u001B[32m--> \u001B[39m\u001B[32m445\u001B[39m objs, keys = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_clean_keys_and_objs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    447\u001B[39m \u001B[38;5;66;03m# figure out what our result ndim is going to be\u001B[39;00m\n\u001B[32m    448\u001B[39m ndims = \u001B[38;5;28mself\u001B[39m._get_ndims(objs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001B[39m, in \u001B[36m_Concatenator._clean_keys_and_objs\u001B[39m\u001B[34m(self, objs, keys)\u001B[39m\n\u001B[32m    504\u001B[39m     objs_list = \u001B[38;5;28mlist\u001B[39m(objs)\n\u001B[32m    506\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(objs_list) == \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m507\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mNo objects to concatenate\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    509\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m keys \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    510\u001B[39m     objs_list = \u001B[38;5;28mlist\u001B[39m(com.not_none(*objs_list))\n",
      "\u001B[31mValueError\u001B[39m: No objects to concatenate"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9e674f7344901dca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
