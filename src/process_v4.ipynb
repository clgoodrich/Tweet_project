{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-29T23:34:37.858283Z",
     "start_time": "2025-09-29T23:34:35.382605Z"
    }
   },
   "source": [
    "# 1.1 Initial Data Loading and Validation\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup paths\n",
    "print(os.getcwd())\n",
    "local_path = os.path.dirname(os.getcwd())\n",
    "francine_path = os.path.join(local_path, r'data\\geojson\\francine.geojson')\n",
    "helene_path = os.path.join(local_path, r'data\\geojson\\helene.geojson')\n",
    "states_path = os.path.join(local_path, r'data\\shape_files\\cb_2023_us_state_20m.shp')\n",
    "counties_path = os.path.join(local_path, r'data\\shape_files\\cb_2023_us_county_20m.shp')\n",
    "cities_path = os.path.join(local_path, r'data\\shape_files\\US_Cities.shp')\n",
    "output_dir = os.path.join(local_path, r'rasters_output')\n",
    "\n",
    "francine_gdf = gpd.read_file(francine_path)\n",
    "helene_gdf = gpd.read_file(helene_path)\n",
    "\n",
    "# Standardize timestamps to UTC\n",
    "francine_gdf['timestamp'] = pd.to_datetime(francine_gdf['time'], utc=True)\n",
    "# print(francine_gdf['time'])\n",
    "helene_gdf['timestamp'] = pd.to_datetime(helene_gdf['time'], utc=True)\n",
    "\n",
    "\n",
    "# Floor to 4-hour bins\n",
    "francine_gdf['time_bin'] = francine_gdf['timestamp'].dt.floor('4h')\n",
    "helene_gdf['time_bin'] = helene_gdf['timestamp'].dt.floor('4h')\n",
    "all_data = francine_gdf['time_bin'].unique()\n",
    "francine_gdf['unix_timestamp'] = francine_gdf['time_bin'].astype('int64') // 1000\n",
    "helene_gdf['unix_timestamp'] = helene_gdf['time_bin'].astype('int64') // 1000\n",
    "# Create readable bin labels for file naming\n",
    "francine_gdf['bin_label'] = francine_gdf['time_bin'].dt.strftime('%Y%m%d_%H%M')\n",
    "helene_gdf['bin_label'] = helene_gdf['time_bin'].dt.strftime('%Y%m%d_%H%M')\n",
    "\n",
    "\n",
    "# Load reference data\n",
    "print(\"Loading reference data...\")\n",
    "states_gdf = gpd.read_file(states_path)\n",
    "counties_gdf = gpd.read_file(counties_path)\n",
    "cities_gdf = gpd.read_file(cities_path)\n",
    "\n",
    "# Ensure consistent CRS (EPSG:4326)\n",
    "francine_gdf = francine_gdf.to_crs('EPSG:4326')\n",
    "helene_gdf = helene_gdf.to_crs('EPSG:4326')\n",
    "states_gdf = states_gdf.to_crs('EPSG:4326')\n",
    "counties_gdf = counties_gdf.to_crs('EPSG:4326')\n",
    "cities_gdf = cities_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "# Summary statistics\n",
    "\n",
    "print(f\"\\nReference data loaded:\")\n",
    "print(f\"  States: {len(states_gdf)}\")\n",
    "print(f\"  Counties: {len(counties_gdf)}\")\n",
    "print(f\"  Cities: {len(cities_gdf)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\src\n",
      "Loading reference data...\n",
      "\n",
      "Reference data loaded:\n",
      "  States: 52\n",
      "  Counties: 3222\n",
      "  Cities: 31615\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T23:35:36.595466Z",
     "start_time": "2025-09-29T23:34:37.864662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "states_gdf = gpd.read_file(states_path)\n",
    "counties_gdf = gpd.read_file(counties_path)\n",
    "cities_gdf = gpd.read_file(cities_path)\n",
    "\n",
    "# PLACE THIS CODE AFTER LOADING SHAPEFILES BUT BEFORE CREATING SIMPLE LOOKUPS\n",
    "# =============================================================================\n",
    "# MULTI-LEVEL GEOGRAPHIC MATCHING SYSTEM (ALL LEVELS)\n",
    "# =============================================================================\n",
    "\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "\n",
    "def preprocess_place_name(name):\n",
    "    \"\"\"Standardize place names for better matching\"\"\"\n",
    "    if pd.isna(name) or name == 'NAN':\n",
    "        return None\n",
    "\n",
    "    name = str(name).upper().strip()\n",
    "\n",
    "    # Common abbreviation standardizations\n",
    "    name = re.sub(r'\\bST\\.?\\b', 'SAINT', name)  # St. -> Saint\n",
    "    name = re.sub(r'\\bMT\\.?\\b', 'MOUNT', name)  # Mt. -> Mount\n",
    "    name = re.sub(r'\\bFT\\.?\\b', 'FORT', name)   # Ft. -> Fort\n",
    "    name = re.sub(r'\\bN\\.?\\b', 'NORTH', name)   # N. -> North\n",
    "    name = re.sub(r'\\bS\\.?\\b', 'SOUTH', name)   # S. -> South\n",
    "    name = re.sub(r'\\bE\\.?\\b', 'EAST', name)    # E. -> East\n",
    "    name = re.sub(r'\\bW\\.?\\b', 'WEST', name)    # W. -> West\n",
    "\n",
    "    # Remove extra spaces and punctuation\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)  # Remove punctuation\n",
    "    name = re.sub(r'\\s+', ' ', name)     # Normalize spaces\n",
    "\n",
    "    return name.strip()\n",
    "\n",
    "def parse_gpe_entities(gpe_string):\n",
    "    \"\"\"Parse GPE string into multiple potential geographic entities\"\"\"\n",
    "    if not gpe_string or pd.isna(gpe_string) or str(gpe_string).strip() == '':\n",
    "        return []\n",
    "\n",
    "    gpe_string = str(gpe_string).strip()\n",
    "\n",
    "    # Split by common separators\n",
    "    entities = []\n",
    "\n",
    "    # Primary split by comma\n",
    "    parts = [part.strip() for part in gpe_string.split(',')]\n",
    "\n",
    "    for part in parts:\n",
    "        if part:\n",
    "            # Further split by other separators\n",
    "            sub_parts = re.split(r'[;&|]', part)\n",
    "            for sub_part in sub_parts:\n",
    "                sub_part = sub_part.strip()\n",
    "                if sub_part and len(sub_part) > 1:  # Ignore single characters\n",
    "                    entities.append(preprocess_place_name(sub_part))\n",
    "\n",
    "    # Remove None values and duplicates while preserving order\n",
    "    clean_entities = []\n",
    "    seen = set()\n",
    "    for entity in entities:\n",
    "        if entity and entity not in seen:\n",
    "            clean_entities.append(entity)\n",
    "            seen.add(entity)\n",
    "\n",
    "    return clean_entities\n",
    "\n",
    "def create_hierarchical_lookups(states_gdf, counties_gdf, cities_gdf):\n",
    "    \"\"\"Create hierarchical lookup dictionaries for fuzzy matching\"\"\"\n",
    "    print(\"\\nCreating hierarchical lookup dictionaries...\")\n",
    "\n",
    "    # 1. States - simple lookup with preprocessed names + abbreviations\n",
    "    state_lookup = {}\n",
    "    state_abbrev_to_name = {}  # Abbreviation to full name\n",
    "    state_name_to_abbrev = {}  # Full name to abbreviation\n",
    "\n",
    "    for idx, row in states_gdf.iterrows():\n",
    "        state_name = preprocess_place_name(row['NAME'])\n",
    "        if state_name:\n",
    "            state_lookup[state_name] = row.geometry\n",
    "            # Handle abbreviations if available\n",
    "            if 'STUSPS' in row:\n",
    "                abbrev = row['STUSPS'].upper()\n",
    "                state_abbrev_to_name[abbrev] = state_name\n",
    "                state_name_to_abbrev[state_name] = abbrev\n",
    "                # Also add abbreviation as a lookup option\n",
    "                state_lookup[abbrev] = row.geometry\n",
    "\n",
    "    # 2. Counties - organized by state\n",
    "    county_by_state = {}\n",
    "    county_lookup = {}\n",
    "\n",
    "    for idx, row in counties_gdf.iterrows():\n",
    "        county_name = preprocess_place_name(row['NAME'])\n",
    "        state_fips = row.get('STATEFP', '')\n",
    "\n",
    "        if county_name:\n",
    "            county_lookup[county_name] = row.geometry\n",
    "\n",
    "            # Try to get state name from STATEFP or other fields\n",
    "            state_name = None\n",
    "            if 'STATE_NAME' in row:\n",
    "                state_name = preprocess_place_name(row['STATE_NAME'])\n",
    "            else:\n",
    "                # Try to find state by FIPS code\n",
    "                for s_idx, s_row in states_gdf.iterrows():\n",
    "                    if s_row.get('STATEFP', '') == state_fips:\n",
    "                        state_name = preprocess_place_name(s_row['NAME'])\n",
    "                        break\n",
    "\n",
    "            if state_name:\n",
    "                if state_name not in county_by_state:\n",
    "                    county_by_state[state_name] = {}\n",
    "                county_by_state[state_name][county_name] = row.geometry\n",
    "\n",
    "    # 3. Cities - organized by state\n",
    "    city_by_state = {}\n",
    "    city_lookup = {}\n",
    "\n",
    "    for idx, row in cities_gdf.iterrows():\n",
    "        city_name = preprocess_place_name(row['NAME'])\n",
    "        state_abbrev = row.get('ST', '').upper()\n",
    "\n",
    "        if city_name:\n",
    "            city_lookup[city_name] = row.geometry\n",
    "\n",
    "            # Convert state abbreviation to full name\n",
    "            if state_abbrev in state_abbrev_to_name:\n",
    "                state_full = state_abbrev_to_name[state_abbrev]\n",
    "                if state_full not in city_by_state:\n",
    "                    city_by_state[state_full] = {}\n",
    "                city_by_state[state_full][city_name] = row.geometry\n",
    "    #\n",
    "    # print(f\"  States: {len(state_lookup)} (including abbreviations)\")\n",
    "    # print(f\"  Counties: {len(county_lookup)} (organized by {len(county_by_state)} states)\")\n",
    "    # print(f\"  Cities: {len(city_lookup)} (organized by {len(city_by_state)} states)\")\n",
    "\n",
    "    return {\n",
    "        'state_lookup': state_lookup,\n",
    "        'county_lookup': county_lookup,\n",
    "        'city_lookup': city_lookup,\n",
    "        'county_by_state': county_by_state,\n",
    "        'city_by_state': city_by_state,\n",
    "        'state_abbrev_to_name': state_abbrev_to_name,\n",
    "        'state_name_to_abbrev': state_name_to_abbrev\n",
    "    }\n",
    "\n",
    "def fuzzy_match_entity(entity, candidates, threshold=75):\n",
    "    \"\"\"Fuzzy match an entity against candidates\"\"\"\n",
    "    if not entity or not candidates:\n",
    "        return None, 0\n",
    "\n",
    "    # Try exact match first\n",
    "    if entity in candidates:\n",
    "        return entity, 100\n",
    "\n",
    "    # Use fuzzy matching\n",
    "    match = process.extractOne(entity, candidates.keys(), scorer=fuzz.ratio)\n",
    "\n",
    "    if match and match[1] >= threshold:\n",
    "        return match[0], match[1]\n",
    "\n",
    "    return None, 0\n",
    "\n",
    "def find_all_geographic_matches(entities, lookups):\n",
    "    \"\"\"Find ALL geographic matches (state, county, city) for the entities\"\"\"\n",
    "    if not entities:\n",
    "        return []\n",
    "\n",
    "    state_lookup = lookups['state_lookup']\n",
    "    county_lookup = lookups['county_lookup']\n",
    "    city_lookup = lookups['city_lookup']\n",
    "    county_by_state = lookups['county_by_state']\n",
    "    city_by_state = lookups['city_by_state']\n",
    "\n",
    "    # Store all successful matches\n",
    "    all_matches = []\n",
    "\n",
    "    # Context tracking for better matching\n",
    "    found_states = set()\n",
    "\n",
    "    # STEP 1: Find all state matches first\n",
    "    for entity in entities:\n",
    "        state_match, state_score = fuzzy_match_entity(entity, state_lookup, threshold=75)\n",
    "        if state_match:\n",
    "            all_matches.append(('STATE', state_match, state_lookup[state_match], state_score))\n",
    "            found_states.add(state_match)\n",
    "\n",
    "    # STEP 2: Find county matches (global first, then state-specific)\n",
    "    for entity in entities:\n",
    "        # Global county search\n",
    "        county_match, county_score = fuzzy_match_entity(entity, county_lookup, threshold=75)\n",
    "        if county_match:\n",
    "            all_matches.append(('COUNTY', county_match, county_lookup[county_match], county_score))\n",
    "\n",
    "        # State-specific county search (higher accuracy)\n",
    "        for state_name in found_states:\n",
    "            if state_name in county_by_state:\n",
    "                state_counties = county_by_state[state_name]\n",
    "                state_county_match, state_county_score = fuzzy_match_entity(entity, state_counties, threshold=70)\n",
    "                if state_county_match and state_county_score > county_score:\n",
    "                    # Replace with better state-specific match\n",
    "                    # Remove the global match if it exists\n",
    "                    all_matches = [m for m in all_matches if not (m[0] == 'COUNTY' and m[1] == county_match)]\n",
    "                    all_matches.append(('COUNTY', state_county_match, state_counties[state_county_match], state_county_score))\n",
    "\n",
    "    # STEP 3: Find city matches (global first, then state-specific)\n",
    "    for entity in entities:\n",
    "        # Global city search\n",
    "        city_match, city_score = fuzzy_match_entity(entity, city_lookup, threshold=75)\n",
    "        if city_match:\n",
    "            all_matches.append(('CITY', city_match, city_lookup[city_match], city_score))\n",
    "\n",
    "        # State-specific city search (higher accuracy)\n",
    "        for state_name in found_states:\n",
    "            if state_name in city_by_state:\n",
    "                state_cities = city_by_state[state_name]\n",
    "                state_city_match, state_city_score = fuzzy_match_entity(entity, state_cities, threshold=70)\n",
    "                if state_city_match and state_city_score > city_score:\n",
    "                    # Replace with better state-specific match\n",
    "                    # Remove the global match if it exists\n",
    "                    all_matches = [m for m in all_matches if not (m[0] == 'CITY' and m[1] == city_match)]\n",
    "                    all_matches.append(('CITY', state_city_match, state_cities[state_city_match], state_city_score))\n",
    "\n",
    "    # Remove duplicates (same scale + name)\n",
    "    unique_matches = []\n",
    "    seen_combinations = set()\n",
    "    for match in all_matches:\n",
    "        combo = (match[0], match[1])  # (scale, name)\n",
    "        if combo not in seen_combinations:\n",
    "            unique_matches.append(match)\n",
    "            seen_combinations.add(combo)\n",
    "\n",
    "    return unique_matches\n",
    "\n",
    "def multi_level_assign_scale_levels(row, lookups):\n",
    "    \"\"\"\n",
    "    Return ALL geographic scale levels that match this tweet\n",
    "    Returns a list of matches: [(scale, name, geom, score), ...]\n",
    "    \"\"\"\n",
    "    gpe = str(row.get('GPE', '')).strip()\n",
    "    fac = str(row.get('FAC', '')).strip()\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    # Parse GPE into multiple entities\n",
    "    entities = parse_gpe_entities(gpe)\n",
    "\n",
    "    if entities:\n",
    "        # Find all geographic matches\n",
    "        geo_matches = find_all_geographic_matches(entities, lookups)\n",
    "        matches.extend(geo_matches)\n",
    "\n",
    "    # Add facility as separate match if available\n",
    "    if fac and fac not in ['nan', 'NAN', '']:\n",
    "        matches.append(('FACILITY', fac, row.geometry, 100))\n",
    "\n",
    "    # If no matches found, return unmatched\n",
    "    if not matches:\n",
    "        matches.append(('UNMATCHED', None, row.geometry, 0))\n",
    "\n",
    "    return matches\n",
    "\n",
    "def expand_tweets_by_matches(gdf, lookups, dataset_name):\n",
    "    \"\"\"\n",
    "    Expand the GeoDataFrame so each tweet creates multiple rows (one per geographic match)\n",
    "    \"\"\"\n",
    "    # print(f\"\\nExpanding {dataset_name} tweets by geographic matches...\")\n",
    "\n",
    "    expanded_rows = []\n",
    "\n",
    "    for idx, row in gdf.iterrows():\n",
    "        matches = multi_level_assign_scale_levels(row, lookups)\n",
    "\n",
    "        # Create one row per match\n",
    "        for scale, name, geom, score in matches:\n",
    "            new_row = row.copy()\n",
    "            new_row['scale_level'] = scale\n",
    "            new_row['matched_name'] = name\n",
    "            new_row['matched_geom'] = geom\n",
    "            new_row['match_score'] = score\n",
    "            new_row['original_index'] = idx  # Track original tweet\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    # Create new GeoDataFrame and preserve the original CRS\n",
    "    expanded_gdf = gpd.GeoDataFrame(expanded_rows, crs=gdf.crs)\n",
    "\n",
    "    # Print statistics\n",
    "    original_count = len(gdf)\n",
    "    expanded_count = len(expanded_gdf)\n",
    "    expansion_ratio = expanded_count / original_count\n",
    "\n",
    "    # print(f\"  Original tweets: {original_count}\")\n",
    "    # print(f\"  Expanded rows: {expanded_count}\")\n",
    "    # print(f\"  Expansion ratio: {expansion_ratio:.2f}x\")\n",
    "\n",
    "    # Print scale distribution\n",
    "    # scale_counts = expanded_gdf['scale_level'].value_counts()\n",
    "    # print(f\"  {dataset_name} scale distribution:\")\n",
    "    # for scale, count in scale_counts.items():\n",
    "    #     print(f\"    {scale}: {count}\")\n",
    "\n",
    "    # Print average match scores by scale level\n",
    "    # print(f\"  Average match scores:\")\n",
    "    # for scale in ['STATE', 'COUNTY', 'CITY', 'FACILITY']:\n",
    "    #     if scale in expanded_gdf['scale_level'].values:\n",
    "    #         avg_score = expanded_gdf[expanded_gdf['scale_level'] == scale]['match_score'].mean()\n",
    "            # print(f\"    {scale}: {avg_score:.1f}%\")\n",
    "\n",
    "    # Show some examples of multi-level matches\n",
    "    print(f\"  Sample multi-level matches:\")\n",
    "    # Group by original tweet and show ones with multiple matches\n",
    "    multi_matches = expanded_gdf.groupby('original_index').size()\n",
    "    multi_match_indices = multi_matches[multi_matches > 1].head(5).index\n",
    "\n",
    "    for orig_idx in multi_match_indices:\n",
    "        tweet_matches = expanded_gdf[expanded_gdf['original_index'] == orig_idx]\n",
    "        original_gpe = tweet_matches.iloc[0]['GPE']\n",
    "        match_summary = ', '.join([f\"{row['scale_level']}:{row['matched_name']}\" for _, row in tweet_matches.iterrows()])\n",
    "        # print(f\"    '{original_gpe}' → {match_summary}\")\n",
    "\n",
    "    return expanded_gdf\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTE MULTI-LEVEL FUZZY MATCHING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-LEVEL GEOGRAPHIC MATCHING (ALL LEVELS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create hierarchical lookups\n",
    "lookups = create_hierarchical_lookups(states_gdf, counties_gdf, cities_gdf)\n",
    "\n",
    "# Apply to both datasets (this will expand the datasets)\n",
    "# francine_gdf = expand_tweets_by_matches(francine_gdf, lookups, \"FRANCINE\")\n",
    "helene_gdf = expand_tweets_by_matches(helene_gdf, lookups, \"HELENE\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-LEVEL FUZZY MATCHING COMPLETE ✓\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNote: Datasets are now expanded - each original tweet may have multiple rows\")\n",
    "print(\"representing different geographic scales (STATE, COUNTY, CITY, etc.)\")"
   ],
   "id": "86cc322264f58876",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MULTI-LEVEL GEOGRAPHIC MATCHING (ALL LEVELS)\n",
      "============================================================\n",
      "\n",
      "Creating hierarchical lookup dictionaries...\n",
      "  Sample multi-level matches:\n",
      "\n",
      "============================================================\n",
      "MULTI-LEVEL FUZZY MATCHING COMPLETE ✓\n",
      "============================================================\n",
      "\n",
      "Note: Datasets are now expanded - each original tweet may have multiple rows\n",
      "representing different geographic scales (STATE, COUNTY, CITY, etc.)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T23:35:39.179608Z",
     "start_time": "2025-09-29T23:35:36.663985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# STEP 2: SPATIAL PROCESSING FRAMEWORK\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.features import rasterize\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# =============================================================================\n",
    "# 2.1 GRID DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "def create_master_grid(francine_gdf, helene_gdf, buffer_percent=10):\n",
    "    \"\"\"Create consistent raster grid parameters for both hurricanes\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CREATING MASTER GRID\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Configuration\n",
    "    TARGET_CRS = 'EPSG:3857'  # Web Mercator\n",
    "    CELL_SIZE_M = 5000  # 5km cells\n",
    "\n",
    "    # Project both datasets\n",
    "    print(f\"\\nProjecting to {TARGET_CRS}...\")\n",
    "    francine_proj = francine_gdf.to_crs(TARGET_CRS)\n",
    "    helene_proj = helene_gdf.to_crs(TARGET_CRS)\n",
    "\n",
    "    # Calculate combined extent\n",
    "    f_bounds = francine_proj.total_bounds\n",
    "    h_bounds = helene_proj.total_bounds\n",
    "\n",
    "    minx = min(f_bounds[0], h_bounds[0])\n",
    "    miny = min(f_bounds[1], h_bounds[1])\n",
    "    maxx = max(f_bounds[2], h_bounds[2])\n",
    "    maxy = max(f_bounds[3], h_bounds[3])\n",
    "\n",
    "    # Add buffer\n",
    "    width_buffer = (maxx - minx) * buffer_percent / 100\n",
    "    height_buffer = (maxy - miny) * buffer_percent / 100\n",
    "\n",
    "    minx -= width_buffer\n",
    "    maxx += width_buffer\n",
    "    miny -= height_buffer\n",
    "    maxy += height_buffer\n",
    "\n",
    "    # Calculate grid dimensions\n",
    "    width = int(np.ceil((maxx - minx) / CELL_SIZE_M))\n",
    "    height = int(np.ceil((maxy - miny) / CELL_SIZE_M))\n",
    "\n",
    "    # Create transform\n",
    "    transform = from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "    grid_params = {\n",
    "        'crs': TARGET_CRS,\n",
    "        'cell_size': CELL_SIZE_M,\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'bounds': (minx, miny, maxx, maxy),\n",
    "        'transform': transform,\n",
    "        'shape': (height, width)\n",
    "    }\n",
    "\n",
    "    print(f\"\\nGrid Configuration:\")\n",
    "    print(f\"  Cell size: {CELL_SIZE_M:,} meters\")\n",
    "    print(f\"  Grid dimensions: {width} x {height} cells\")\n",
    "    print(f\"  Total cells: {width * height:,}\")\n",
    "    print(f\"  Coverage area: {(width * height * CELL_SIZE_M * CELL_SIZE_M) / 1e6:,.0f} km²\")\n",
    "\n",
    "    return grid_params, francine_proj, helene_proj\n",
    "\n",
    "# =============================================================================\n",
    "# 2.2 HIERARCHICAL RASTERIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def rasterize_by_scale(gdf_time_bin, grid_params, reference_gdfs):\n",
    "    \"\"\"Rasterize all geographic scales for a single time bin\"\"\"\n",
    "\n",
    "    # Initialize grids for each scale\n",
    "    state_grid = np.zeros(grid_params['shape'], dtype=np.float32)\n",
    "    county_grid = np.zeros(grid_params['shape'], dtype=np.float32)\n",
    "    city_grid = np.zeros(grid_params['shape'], dtype=np.float32)\n",
    "    facility_grid = np.zeros(grid_params['shape'], dtype=np.float32)\n",
    "\n",
    "    if len(gdf_time_bin) == 0:\n",
    "        return state_grid, county_grid, city_grid, facility_grid\n",
    "\n",
    "    # Project reference data to match grid CRS\n",
    "    states_proj = reference_gdfs['states'].to_crs(grid_params['crs'])\n",
    "    counties_proj = reference_gdfs['counties'].to_crs(grid_params['crs'])\n",
    "    cities_proj = reference_gdfs['cities'].to_crs(grid_params['crs'])\n",
    "\n",
    "    # STATES\n",
    "    state_data = gdf_time_bin[gdf_time_bin['scale_level'] == 'STATE']\n",
    "    if len(state_data) > 0:\n",
    "        state_counts = state_data.groupby('matched_name')['weight'].sum()\n",
    "        for state_name, count in state_counts.items():\n",
    "            state_geom = states_proj[states_proj['NAME'].str.upper() == state_name]\n",
    "            if not state_geom.empty:\n",
    "                shapes = [(geom, count) for geom in state_geom.geometry]\n",
    "                temp = rasterize(shapes, out_shape=grid_params['shape'],\n",
    "                               transform=grid_params['transform'], fill=0, all_touched=True)\n",
    "                state_grid += temp\n",
    "\n",
    "    # COUNTIES\n",
    "    county_data = gdf_time_bin[gdf_time_bin['scale_level'] == 'COUNTY']\n",
    "    if len(county_data) > 0:\n",
    "        county_counts = county_data.groupby('matched_name')['weight'].sum()\n",
    "        for county_name, count in county_counts.items():\n",
    "            county_geom = counties_proj[counties_proj['NAME'].str.upper() == county_name]\n",
    "            if not county_geom.empty:\n",
    "                shapes = [(geom, count) for geom in county_geom.geometry]\n",
    "                temp = rasterize(shapes, out_shape=grid_params['shape'],\n",
    "                               transform=grid_params['transform'], fill=0, all_touched=True)\n",
    "                county_grid += temp\n",
    "\n",
    "    # CITIES (with 5km buffers)\n",
    "    city_data = gdf_time_bin[gdf_time_bin['scale_level'] == 'CITY']\n",
    "    if len(city_data) > 0:\n",
    "        city_counts = city_data.groupby('matched_name')['weight'].sum()\n",
    "        for city_name, count in city_counts.items():\n",
    "            city_geom = cities_proj[cities_proj['NAME'].str.upper() == city_name]\n",
    "            if not city_geom.empty:\n",
    "                # Create 5km buffer around city points\n",
    "                buffered = city_geom.geometry.buffer(grid_params['cell_size'])\n",
    "                shapes = [(geom, count) for geom in buffered]\n",
    "                temp = rasterize(shapes, out_shape=grid_params['shape'],\n",
    "                               transform=grid_params['transform'], fill=0, all_touched=True)\n",
    "                city_grid += temp\n",
    "\n",
    "    # FACILITIES (kernel density)\n",
    "    facility_data = gdf_time_bin[gdf_time_bin['scale_level'] == 'FACILITY']\n",
    "    if len(facility_data) > 0:\n",
    "        sigma_pixels = 2  # 10km kernel (2 * 5km cells)\n",
    "        for _, row in facility_data.iterrows():\n",
    "            # Project facility point\n",
    "            point_proj = row['matched_geom']\n",
    "            # Convert to pixel coordinates\n",
    "            px = int((point_proj.x - grid_params['bounds'][0]) / grid_params['cell_size'])\n",
    "            py = int((grid_params['bounds'][3] - point_proj.y) / grid_params['cell_size'])\n",
    "\n",
    "            if 0 <= px < grid_params['width'] and 0 <= py < grid_params['height']:\n",
    "                # Create point source\n",
    "                point_grid = np.zeros(grid_params['shape'], dtype=np.float32)\n",
    "                point_grid[py, px] = row['weight']\n",
    "                # Apply Gaussian kernel\n",
    "                kernel = gaussian_filter(point_grid, sigma=sigma_pixels, mode='constant')\n",
    "                facility_grid += kernel\n",
    "\n",
    "    return state_grid, county_grid, city_grid, facility_grid\n",
    "\n",
    "# =============================================================================\n",
    "# 2.3 GRID INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "def integrate_grids(state_grid, county_grid, city_grid, facility_grid, smooth=True):\n",
    "    \"\"\"Combine all scale grids into unified heat map\"\"\"\n",
    "\n",
    "    # Simple additive model\n",
    "    unified_grid = state_grid + county_grid + city_grid + facility_grid\n",
    "\n",
    "    # Optional smoothing\n",
    "    if smooth:\n",
    "        unified_grid = gaussian_filter(unified_grid, sigma=1, mode='constant')\n",
    "\n",
    "    return unified_grid\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTE GRID CREATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create master grid\n",
    "grid_params, francine_proj, helene_proj = create_master_grid(francine_gdf, helene_gdf)\n",
    "\n",
    "# Store reference GeoDataFrames for rasterization\n",
    "reference_gdfs = {\n",
    "    'states': states_gdf,\n",
    "    'counties': counties_gdf,\n",
    "    'cities': cities_gdf\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SPATIAL PROCESSING FRAMEWORK READY ✓\")\n",
    "print(\"=\"*60)"
   ],
   "id": "add67223b2710d05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING MASTER GRID\n",
      "============================================================\n",
      "\n",
      "Projecting to EPSG:3857...\n",
      "\n",
      "Grid Configuration:\n",
      "  Cell size: 5,000 meters\n",
      "  Grid dimensions: 808 x 527 cells\n",
      "  Total cells: 425,816\n",
      "  Coverage area: 10,645,400 km²\n",
      "\n",
      "============================================================\n",
      "SPATIAL PROCESSING FRAMEWORK READY ✓\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T23:35:39.865452Z",
     "start_time": "2025-09-29T23:35:39.208230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# STEP 3: TEMPORAL PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# 3.1 TIME SERIES GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def process_hurricane_temporal(hurricane_name, gdf_proj, grid_params, reference_gdfs, output_dir):\n",
    "    \"\"\"Process a hurricane through all time bins to create temporal series\"\"\"\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"PROCESSING {hurricane_name.upper()} TEMPORAL SERIES\")\n",
    "    print(f\"=\"*60)\n",
    "\n",
    "    # Create output directory\n",
    "    hurricane_dir = os.path.join(output_dir, hurricane_name.lower())\n",
    "    os.makedirs(hurricane_dir, exist_ok=True)\n",
    "\n",
    "    # Get unique time bins (sorted)\n",
    "    time_bins = sorted(gdf_proj['unix_timestamp'].unique())\n",
    "    print(f\"\\nTime bins to process: {len(time_bins)}\")\n",
    "\n",
    "    # Initialize cumulative grid\n",
    "    cumulative_grid = np.zeros(grid_params['shape'], dtype=np.float32)\n",
    "\n",
    "    # Store metadata for each time step\n",
    "    metadata = []\n",
    "\n",
    "    # Process each time bin\n",
    "    for idx, time_bin in enumerate(time_bins):\n",
    "        print(f\"\\nProcessing time bin {idx+1}/{len(time_bins)}: {datetime.fromtimestamp(time_bin/1000)}\")\n",
    "\n",
    "        # Filter data for current time bin\n",
    "        gdf_time = gdf_proj[gdf_proj['unix_timestamp'] == time_bin]\n",
    "        tweet_count = len(gdf_time[gdf_time['scale_level'] != 'UNMATCHED'])\n",
    "\n",
    "        # Rasterize all scales\n",
    "        state_grid, county_grid, city_grid, facility_grid = rasterize_by_scale(\n",
    "            gdf_time, grid_params, reference_gdfs\n",
    "        )\n",
    "\n",
    "        # Create incremental grid (current time bin only)\n",
    "        incremental_grid = integrate_grids(state_grid, county_grid, city_grid, facility_grid)\n",
    "\n",
    "        # Update cumulative grid\n",
    "        cumulative_grid += incremental_grid\n",
    "\n",
    "        # Save rasters\n",
    "        save_raster(incremental_grid, hurricane_dir,\n",
    "                   f\"{hurricane_name}_increment_{time_bin}.tif\",\n",
    "                   grid_params)\n",
    "        save_raster(cumulative_grid, hurricane_dir,\n",
    "                   f\"{hurricane_name}_cumulative_{time_bin}.tif\",\n",
    "                   grid_params)\n",
    "\n",
    "        # Collect metadata\n",
    "        metadata.append({\n",
    "            'time_bin': time_bin,\n",
    "            'datetime': datetime.fromtimestamp(time_bin/1000),\n",
    "            'tweet_count': tweet_count,\n",
    "            'incremental_max': np.max(incremental_grid),\n",
    "            'incremental_sum': np.sum(incremental_grid),\n",
    "            'cumulative_max': np.max(cumulative_grid),\n",
    "            'cumulative_sum': np.sum(cumulative_grid),\n",
    "            'active_pixels': np.count_nonzero(incremental_grid)\n",
    "        })\n",
    "\n",
    "        print(f\"  Tweets: {tweet_count} | Max value: {np.max(incremental_grid):.1f} | Active pixels: {np.count_nonzero(incremental_grid)}\")\n",
    "\n",
    "    # Save metadata\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    metadata_df.to_csv(os.path.join(hurricane_dir, f\"{hurricane_name}_metadata.csv\"), index=False)\n",
    "\n",
    "    print(f\"\\n{hurricane_name.upper()} processing complete!\")\n",
    "    print(f\"Output saved to: {hurricane_dir}\")\n",
    "\n",
    "    return metadata_df\n",
    "\n",
    "def save_raster(grid, output_dir, filename, grid_params):\n",
    "    \"\"\"Save a grid as GeoTIFF\"\"\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    with rasterio.open(\n",
    "        filepath, 'w',\n",
    "        driver='GTiff',\n",
    "        height=grid_params['height'],\n",
    "        width=grid_params['width'],\n",
    "        count=1,\n",
    "        dtype=grid.dtype,\n",
    "        crs=grid_params['crs'],\n",
    "        transform=grid_params['transform'],\n",
    "        compress='lzw'\n",
    "    ) as dst:\n",
    "        dst.write(grid, 1)\n",
    "\n",
    "# =============================================================================\n",
    "# 3.2 TEMPORAL INTERPOLATION (OPTIONAL)\n",
    "# =============================================================================\n",
    "\n",
    "def create_interpolated_frames(hurricane_dir, metadata_df, interpolation_steps=3):\n",
    "    \"\"\"Create interpolated frames between time bins for smoother animation\"\"\"\n",
    "\n",
    "    print(\"\\nCreating interpolated frames...\")\n",
    "\n",
    "    time_bins = sorted(metadata_df['time_bin'].values)\n",
    "\n",
    "    for i in range(len(time_bins) - 1):\n",
    "        current_bin = time_bins[i]\n",
    "        next_bin = time_bins[i + 1]\n",
    "\n",
    "        # Load current and next incremental grids\n",
    "        current_file = os.path.join(hurricane_dir, f\"*_increment_{current_bin}.tif\")\n",
    "        next_file = os.path.join(hurricane_dir, f\"*_increment_{next_bin}.tif\")\n",
    "\n",
    "        # Skip actual interpolation for now (would need to load rasters)\n",
    "        # This is a placeholder for the interpolation logic\n",
    "\n",
    "    print(\"  Interpolation complete (optional step)\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTE TEMPORAL PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process Francine\n",
    "francine_metadata = process_hurricane_temporal(\n",
    "    'francine', francine_proj, grid_params, reference_gdfs, output_dir\n",
    ")\n",
    "\n",
    "# Process Helene\n",
    "helene_metadata = process_hurricane_temporal(\n",
    "    'helene', helene_proj, grid_params, reference_gdfs, output_dir\n",
    ")\n",
    "\n",
    "# Optional: Create interpolated frames (commented out for performance)\n",
    "# create_interpolated_frames(os.path.join(output_dir, 'francine'), francine_metadata)\n",
    "# create_interpolated_frames(os.path.join(output_dir, 'helene'), helene_metadata)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEMPORAL PROCESSING COMPLETE ✓\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOutputs saved to: {output_dir}\")\n",
    "print(\"  - Incremental rasters (per time bin)\")\n",
    "print(\"  - Cumulative rasters (running total)\")\n",
    "print(\"  - Metadata CSV files\")"
   ],
   "id": "7f07a65a3096ea77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING FRANCINE TEMPORAL SERIES\n",
      "============================================================\n",
      "\n",
      "Time bins to process: 42\n",
      "\n",
      "Processing time bin 1/42: 1970-01-20 17:24:28.800000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'scale_level'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3811\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3812\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/index.pyx:167\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/index.pyx:196\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mKeyError\u001B[39m: 'scale_level'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 132\u001B[39m\n\u001B[32m    129\u001B[39m os.makedirs(output_dir, exist_ok=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    131\u001B[39m \u001B[38;5;66;03m# Process Francine\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m132\u001B[39m francine_metadata = \u001B[43mprocess_hurricane_temporal\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mfrancine\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrancine_proj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreference_gdfs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dir\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[38;5;66;03m# Process Helene\u001B[39;00m\n\u001B[32m    137\u001B[39m helene_metadata = process_hurricane_temporal(\n\u001B[32m    138\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mhelene\u001B[39m\u001B[33m'\u001B[39m, helene_proj, grid_params, reference_gdfs, output_dir\n\u001B[32m    139\u001B[39m )\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 39\u001B[39m, in \u001B[36mprocess_hurricane_temporal\u001B[39m\u001B[34m(hurricane_name, gdf_proj, grid_params, reference_gdfs, output_dir)\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[38;5;66;03m# Filter data for current time bin\u001B[39;00m\n\u001B[32m     38\u001B[39m gdf_time = gdf_proj[gdf_proj[\u001B[33m'\u001B[39m\u001B[33munix_timestamp\u001B[39m\u001B[33m'\u001B[39m] == time_bin]\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m tweet_count = \u001B[38;5;28mlen\u001B[39m(gdf_time[\u001B[43mgdf_time\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mscale_level\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m != \u001B[33m'\u001B[39m\u001B[33mUNMATCHED\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m# Rasterize all scales\u001B[39;00m\n\u001B[32m     42\u001B[39m state_grid, county_grid, city_grid, facility_grid = rasterize_by_scale(\n\u001B[32m     43\u001B[39m     gdf_time, grid_params, reference_gdfs\n\u001B[32m     44\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\geopandas\\geodataframe.py:1896\u001B[39m, in \u001B[36mGeoDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[32m   1891\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1892\u001B[39m \u001B[33;03m    If the result is a column containing only 'geometry', return a\u001B[39;00m\n\u001B[32m   1893\u001B[39m \u001B[33;03m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001B[39;00m\n\u001B[32m   1894\u001B[39m \u001B[33;03m    return a GeoDataFrame.\u001B[39;00m\n\u001B[32m   1895\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1896\u001B[39m     result = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1897\u001B[39m     \u001B[38;5;66;03m# Custom logic to avoid waiting for pandas GH51895\u001B[39;00m\n\u001B[32m   1898\u001B[39m     \u001B[38;5;66;03m# result is not geometry dtype for multi-indexes\u001B[39;00m\n\u001B[32m   1899\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1900\u001B[39m         pd.api.types.is_scalar(key)\n\u001B[32m   1901\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m key == \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1904\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_geometry_type(result)\n\u001B[32m   1905\u001B[39m     ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4105\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns.nlevels > \u001B[32m1\u001B[39m:\n\u001B[32m   4106\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._getitem_multilevel(key)\n\u001B[32m-> \u001B[39m\u001B[32m4107\u001B[39m indexer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4108\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[32m   4109\u001B[39m     indexer = [indexer]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3814\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   3815\u001B[39m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc.Iterable)\n\u001B[32m   3816\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[32m   3817\u001B[39m     ):\n\u001B[32m   3818\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[32m-> \u001B[39m\u001B[32m3819\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m   3820\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m   3821\u001B[39m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[32m   3822\u001B[39m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[32m   3823\u001B[39m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[32m   3824\u001B[39m     \u001B[38;5;28mself\u001B[39m._check_indexing_error(key)\n",
      "\u001B[31mKeyError\u001B[39m: 'scale_level'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f9baf5c9907335b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
