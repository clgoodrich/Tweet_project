{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Tweet Data Generator\n",
    "\n",
    "Generates synthetic tweet data with **natural, realistic patterns** for the Southeast US region.\n",
    "\n",
    "**Natural patterns:**\n",
    "- Normal daily activity cycles (people tweet more during waking hours)\n",
    "- Geographic distribution following population density\n",
    "- Entity mentions reflect real place names in the region\n",
    "- No artificial hurricane-specific behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from shapely.geometry import Point\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURATION - Just change these!\n",
    "# ==============================================================================\n",
    "\n",
    "NUM_TWEETS = 50000  # How many tweets to generate\n",
    "\n",
    "DATASET_NAME = 'natural_test'  # Output filename\n",
    "\n",
    "# Time range (natural daily pattern)\n",
    "START_DATE = datetime(2024, 9, 26, 0, 0, 0)\n",
    "NUM_DAYS = 3  # Duration of dataset\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Generating {NUM_TWEETS:,} tweets\")\n",
    "print(f\"  Time range: {START_DATE} + {NUM_DAYS} days\")\n",
    "print(f\"  Region: Southeast US (FL, GA, NC, SC, AL, TN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real data just to extract entity names\n",
    "local_path = os.path.dirname(os.getcwd())\n",
    "helene_path = os.path.join(local_path, 'data', 'geojson', 'helene.geojson')\n",
    "\n",
    "real_gdf = gpd.read_file(helene_path)\n",
    "\n",
    "# Extract all unique entities from real data\n",
    "all_entities = set()\n",
    "for gpe in real_gdf['GPE'].dropna():\n",
    "    if str(gpe).strip():\n",
    "        parts = str(gpe).replace(',', '|').replace(';', '|').split('|')\n",
    "        all_entities.update([e.strip() for e in parts if e.strip()])\n",
    "\n",
    "all_facilities = list(real_gdf[real_gdf['FAC'] != '']['FAC'].dropna().unique())\n",
    "\n",
    "entity_list = sorted(list(all_entities))\n",
    "\n",
    "print(f\"Extracted {len(entity_list)} unique place names from real data\")\n",
    "print(f\"Extracted {len(all_facilities)} facility names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Pattern Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major cities and their populations (for weighted sampling)\n",
    "CITIES_WITH_POPULATION = [\n",
    "    ('Tampa', 27.95, -82.46, 400000),\n",
    "    ('Atlanta', 33.75, -84.39, 500000),\n",
    "    ('Charlotte', 35.23, -80.84, 900000),\n",
    "    ('Jacksonville', 30.33, -81.66, 950000),\n",
    "    ('Tallahassee', 30.44, -84.28, 200000),\n",
    "    ('Asheville', 35.60, -82.55, 95000),\n",
    "    ('Gainesville', 29.65, -82.32, 140000),\n",
    "    ('Savannah', 32.08, -81.09, 150000),\n",
    "    ('Birmingham', 33.52, -86.80, 200000),\n",
    "    ('Nashville', 36.16, -86.78, 700000),\n",
    "]\n",
    "\n",
    "# Calculate weights based on population\n",
    "city_weights = [pop for _, _, _, pop in CITIES_WITH_POPULATION]\n",
    "city_weights = np.array(city_weights) / sum(city_weights)\n",
    "\n",
    "def natural_timestamp(start_date, num_days):\n",
    "    \"\"\"\n",
    "    Generate timestamp with natural daily patterns:\n",
    "    - People sleep at night (less activity 12 AM - 6 AM)\n",
    "    - Peak activity during day/evening (9 AM - 10 PM)\n",
    "    \"\"\"\n",
    "    # Random day and hour\n",
    "    day_offset = np.random.randint(0, num_days)\n",
    "    \n",
    "    # Hour probability (people are awake during the day)\n",
    "    hour_weights = np.array([\n",
    "        0.2, 0.1, 0.05, 0.05, 0.1, 0.3,  # 12AM-6AM (sleeping)\n",
    "        0.7, 1.0, 1.2, 1.3, 1.2, 1.1,    # 6AM-12PM (morning/lunch)\n",
    "        1.0, 1.0, 1.0, 1.1, 1.2, 1.4,    # 12PM-6PM (afternoon)\n",
    "        1.5, 1.4, 1.2, 1.0, 0.7, 0.4     # 6PM-12AM (evening)\n",
    "    ])\n",
    "    hour_weights = hour_weights / hour_weights.sum()\n",
    "    \n",
    "    hour = np.random.choice(24, p=hour_weights)\n",
    "    minute = np.random.randint(0, 60)\n",
    "    second = np.random.randint(0, 60)\n",
    "    \n",
    "    return start_date + timedelta(days=day_offset, hours=hour, minutes=minute, seconds=second)\n",
    "\n",
    "\n",
    "def natural_location():\n",
    "    \"\"\"\n",
    "    Generate location following population distribution.\n",
    "    More tweets from larger cities.\n",
    "    \"\"\"\n",
    "    # Select city weighted by population\n",
    "    city_idx = np.random.choice(len(CITIES_WITH_POPULATION), p=city_weights)\n",
    "    city_name, lat, lon, pop = CITIES_WITH_POPULATION[city_idx]\n",
    "    \n",
    "    # Add scatter around city center (gaussian)\n",
    "    # Larger cities have more spread\n",
    "    spread = 0.2 + (pop / 1000000) * 0.3  # 0.2 to 0.5 degrees\n",
    "    lat = lat + np.random.normal(0, spread)\n",
    "    lon = lon + np.random.normal(0, spread)\n",
    "    \n",
    "    return lat, lon\n",
    "\n",
    "\n",
    "def natural_gpe(entity_list):\n",
    "    \"\"\"\n",
    "    Generate GPE naturally:\n",
    "    - Most tweets mention 1 place\n",
    "    - Some mention 2 places\n",
    "    - Rarely 3+\n",
    "    \"\"\"\n",
    "    if np.random.random() < 0.15:\n",
    "        # 15% no location mention\n",
    "        return ''\n",
    "    \n",
    "    # How many entities?\n",
    "    num_entities = np.random.choice([1, 2, 3], p=[0.70, 0.25, 0.05])\n",
    "    \n",
    "    entities = np.random.choice(entity_list, size=min(num_entities, len(entity_list)), replace=False)\n",
    "    return ', '.join(entities)\n",
    "\n",
    "\n",
    "def natural_fac(facilities):\n",
    "    \"\"\"\n",
    "    Facilities mentioned occasionally (5% of tweets)\n",
    "    \"\"\"\n",
    "    if facilities and np.random.random() < 0.05:\n",
    "        return np.random.choice(facilities)\n",
    "    return ''\n",
    "\n",
    "\n",
    "print(\"✓ Natural pattern functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Natural Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating natural tweet data...\")\n",
    "print(\"Patterns: Normal daily cycles + population-based geography\")\n",
    "print()\n",
    "\n",
    "synthetic_data = []\n",
    "\n",
    "for i in range(NUM_TWEETS):\n",
    "    lat, lon = natural_location()\n",
    "    \n",
    "    record = {\n",
    "        'FAC': natural_fac(all_facilities),\n",
    "        'LOC': '',\n",
    "        'GPE': natural_gpe(entity_list),\n",
    "        'time': natural_timestamp(START_DATE, NUM_DAYS),\n",
    "        'Latitude': lat,\n",
    "        'Longitude': lon,\n",
    "        'make_polygon': 1,\n",
    "        'geometry': Point(lon, lat)\n",
    "    }\n",
    "    synthetic_data.append(record)\n",
    "    \n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"  Generated {i + 1:,} / {NUM_TWEETS:,} tweets\")\n",
    "\n",
    "# Create GeoDataFrame\n",
    "synthetic_gdf = gpd.GeoDataFrame(synthetic_data, crs='EPSG:4326')\n",
    "\n",
    "print(f\"\\n✓ Generated {len(synthetic_gdf):,} tweets with natural patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerated Dataset Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal tweets: {len(synthetic_gdf):,}\")\n",
    "\n",
    "print(f\"\\nTime range:\")\n",
    "print(f\"  Start: {synthetic_gdf['time'].min()}\")\n",
    "print(f\"  End: {synthetic_gdf['time'].max()}\")\n",
    "print(f\"  Duration: {(synthetic_gdf['time'].max() - synthetic_gdf['time'].min()).total_seconds() / 3600:.1f} hours\")\n",
    "\n",
    "print(f\"\\nGeographic extent:\")\n",
    "print(f\"  Latitude: {synthetic_gdf['Latitude'].min():.2f} to {synthetic_gdf['Latitude'].max():.2f}\")\n",
    "print(f\"  Longitude: {synthetic_gdf['Longitude'].min():.2f} to {synthetic_gdf['Longitude'].max():.2f}\")\n",
    "\n",
    "print(f\"\\nEntity mentions:\")\n",
    "gpe_count = (synthetic_gdf['GPE'] != '').sum()\n",
    "fac_count = (synthetic_gdf['FAC'] != '').sum()\n",
    "print(f\"  GPE (places): {gpe_count:,} ({gpe_count/len(synthetic_gdf)*100:.1f}%)\")\n",
    "print(f\"  FAC (facilities): {fac_count:,} ({fac_count/len(synthetic_gdf)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nSample tweets:\")\n",
    "print(synthetic_gdf[['GPE', 'FAC', 'time', 'Latitude', 'Longitude']].head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to GeoJSON\n",
    "output_path = os.path.join(local_path, 'data', 'geojson', f'{DATASET_NAME}.geojson')\n",
    "synthetic_gdf.to_file(output_path, driver='GeoJSON')\n",
    "\n",
    "file_size_mb = os.path.getsize(output_path) / 1024 / 1024\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SAVED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOutput: {output_path}\")\n",
    "print(f\"Size: {file_size_mb:.2f} MB\")\n",
    "print(f\"\\n✓ Ready to use in processing pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Temporal pattern (hourly)\n",
    "synthetic_gdf['hour'] = synthetic_gdf['time'].dt.hour\n",
    "hourly_counts = synthetic_gdf['hour'].value_counts().sort_index()\n",
    "\n",
    "axes[0].bar(hourly_counts.index, hourly_counts.values, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Hour of Day')\n",
    "axes[0].set_ylabel('Tweet Count')\n",
    "axes[0].set_title('Natural Daily Activity Pattern')\n",
    "axes[0].set_xticks(range(0, 24, 2))\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].axvspan(0, 6, alpha=0.2, color='gray', label='Sleep hours')\n",
    "axes[0].legend()\n",
    "\n",
    "# Geographic distribution\n",
    "sample_size = min(5000, len(synthetic_gdf))\n",
    "sample = synthetic_gdf.sample(sample_size)\n",
    "\n",
    "axes[1].scatter(sample['Longitude'], sample['Latitude'], s=1, alpha=0.4, color='darkgreen')\n",
    "axes[1].set_xlabel('Longitude')\n",
    "axes[1].set_ylabel('Latitude')\n",
    "axes[1].set_title(f'Geographic Distribution (n={sample_size:,})')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Mark major cities\n",
    "for city_name, lat, lon, _ in CITIES_WITH_POPULATION:\n",
    "    axes[1].plot(lon, lat, 'r*', markersize=8, alpha=0.7)\n",
    "    axes[1].text(lon, lat, f'  {city_name}', fontsize=7, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(local_path, 'natural_data_patterns.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Pattern visualization saved: natural_data_patterns.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
