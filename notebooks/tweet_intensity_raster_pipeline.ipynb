{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurricane Tweet Intensity â†’ Time-Aware Rasters\n",
    "\n",
    "**Purpose**: Convert geolocated tweets for Francine & Helene into ArcGIS-native time-enabled rasters (CRF) and Space-Time Cubes (.nc) for animation and Space-Time Pattern Mining.\n",
    "\n",
    "**Strategy**: Fuse tweet point events with geographic context layers (states, counties, cities) using spatial proximity and density-based weighting to create continuous intensity surfaces. Each time bin produces a raster slice; iterative mode shows per-bin activity, cumulative mode shows accumulated activity from start to current bin.\n",
    "\n",
    "**Environment**: ArcGIS Pro Python (arcpy only, no external dependencies)\n",
    "\n",
    "**Outputs per hurricane**:\n",
    "- Iterative CRF + Space-Time Cube\n",
    "- Cumulative CRF + Space-Time Cube\n",
    "- GeoTIFF stack + manifest table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration (Centralized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Cell size: 10 km\n",
      "  Time bin: 6 hours\n",
      "  CRS: EPSG:5070\n",
      "  Output GDB: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\outputs\\rasters.gdb\n",
      "\n",
      "âš¡ SAMPLE MODE ENABLED âš¡\n",
      "  Running francine only with 4 slices per mode\n",
      "  Set SAMPLE_MODE = False for full pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# ========== PROJECT PATHS ==========\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\")\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\"\n",
    "\n",
    "INPUTS = {\n",
    "    \"cities\": DATA_ROOT / \"tables\" / \"cities1000.csv\",\n",
    "    \"counties\": DATA_ROOT / \"shape_files\" / \"cb_2023_us_county_20m.shp\",\n",
    "    \"states\": DATA_ROOT / \"shape_files\" / \"cb_2023_us_state_20m.shp\",\n",
    "    \"francine\": DATA_ROOT / \"geojson\" / \"francine.geojson\",\n",
    "    \"helene\": DATA_ROOT / \"geojson\" / \"helene.geojson\"\n",
    "}\n",
    "\n",
    "# ========== RASTER PARAMETERS ==========\n",
    "CELL_SIZE_KM = 10  # Spatial resolution (10 km)\n",
    "TIME_BIN_HOURS = 6  # Temporal resolution (6-hour bins)\n",
    "CRS_EPSG = 5070  # NAD83 Conus Albers (meters)\n",
    "NODATA_VALUE = -9999\n",
    "\n",
    "# ========== OUTPUT PARAMETERS ==========\n",
    "OUTPUT_ROOT = PROJECT_ROOT / \"outputs\"\n",
    "OUTPUT_GDB = OUTPUT_ROOT / \"rasters.gdb\"\n",
    "EVENTS = [\"francine\", \"helene\"]\n",
    "MODES = [\"iter\", \"cum\"]  # iterative, cumulative\n",
    "\n",
    "# ========== SAMPLE/DEBUG MODE ==========\n",
    "# Set SAMPLE_MODE = True to run a quick test with limited data\n",
    "SAMPLE_MODE = True  # Change to False for full pipeline\n",
    "SAMPLE_NUM_SLICES = 4  # Number of time slices to generate in sample mode\n",
    "SAMPLE_EVENT = \"francine\"  # Which event to use for sample run\n",
    "\n",
    "# ========== PROCESSING PARAMETERS ==========\n",
    "# Fusion weights: balance tweet density with geographic context\n",
    "WEIGHTS = {\n",
    "    \"tweet_density\": 0.6,  # Primary signal from tweet points\n",
    "    \"city_proximity\": 0.2,  # Secondary: distance to populated places\n",
    "    \"admin_context\": 0.2   # Tertiary: state/county boundaries\n",
    "}\n",
    "\n",
    "# Search radius for density calculation (meters in EPSG:5070)\n",
    "SEARCH_RADIUS_M = CELL_SIZE_KM * 1000 * 2  # 2x cell size\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Cell size: {CELL_SIZE_KM} km\")\n",
    "print(f\"  Time bin: {TIME_BIN_HOURS} hours\")\n",
    "print(f\"  CRS: EPSG:{CRS_EPSG}\")\n",
    "print(f\"  Output GDB: {OUTPUT_GDB}\")\n",
    "if SAMPLE_MODE:\n",
    "    print(f\"\\nâš¡ SAMPLE MODE ENABLED âš¡\")\n",
    "    print(f\"  Running {SAMPLE_EVENT} only with {SAMPLE_NUM_SLICES} slices per mode\")\n",
    "    print(f\"  Set SAMPLE_MODE = False for full pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing geodatabase: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\outputs\\rasters.gdb\n",
      "ArcPy environment configured\n",
      "  Spatial Analyst: Available\n",
      "  Overwrite: True\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "from arcpy import env\n",
    "from arcpy.sa import *\n",
    "import arcpy.stpm as stpm\n",
    "\n",
    "# Enable spatial analyst\n",
    "arcpy.CheckOutExtension(\"Spatial\")\n",
    "\n",
    "# Configure environment\n",
    "env.overwriteOutput = True\n",
    "env.outputCoordinateSystem = arcpy.SpatialReference(CRS_EPSG)\n",
    "env.cellSize = CELL_SIZE_KM * 1000  # Convert km to meters\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "if not arcpy.Exists(str(OUTPUT_GDB)):\n",
    "    arcpy.management.CreateFileGDB(str(OUTPUT_ROOT), OUTPUT_GDB.name)\n",
    "    print(f\"Created geodatabase: {OUTPUT_GDB}\")\n",
    "else:\n",
    "    print(f\"Using existing geodatabase: {OUTPUT_GDB}\")\n",
    "\n",
    "# Set workspace\n",
    "env.workspace = str(OUTPUT_GDB)\n",
    "\n",
    "print(\"ArcPy environment configured\")\n",
    "print(f\"  Spatial Analyst: {arcpy.CheckExtension('Spatial')}\")\n",
    "print(f\"  Overwrite: {env.overwriteOutput}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. I/O Validation & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating inputs...\n",
      "  âœ“ cities: cities1000.csv\n",
      "  âœ“ counties: cb_2023_us_county_20m.shp\n",
      "  âœ“ states: cb_2023_us_state_20m.shp\n",
      "  âœ“ francine: francine.geojson\n",
      "  âœ“ helene: helene.geojson\n",
      "All inputs valid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def validate_inputs():\n",
    "    \"\"\"Assert all input files exist and are readable.\"\"\"\n",
    "    print(\"Validating inputs...\")\n",
    "    for name, path in INPUTS.items():\n",
    "        assert path.exists(), f\"NEED_INFO: Missing input file: {path}\"\n",
    "        if path.suffix == \".shp\":\n",
    "            desc = arcpy.Describe(str(path))\n",
    "            assert desc.shapeType == \"Polygon\", f\"Expected Polygon, got {desc.shapeType} for {name}\"\n",
    "        print(f\"  âœ“ {name}: {path.name}\")\n",
    "    print(\"All inputs valid\\n\")\n",
    "\n",
    "def parse_iso_time(time_str):\n",
    "    \"\"\"Parse ISO8601 timestamp with timezone.\"\"\"\n",
    "    # Handle formats: \"2024-09-10 23:58:43+00:00\" or \"2024-09-10T23:58:43+00:00\"\n",
    "    time_str = time_str.replace(\" \", \"T\")\n",
    "    if \"+\" in time_str or time_str.endswith(\"Z\"):\n",
    "        # Python 3.7+ fromisoformat handles this\n",
    "        try:\n",
    "            return datetime.fromisoformat(time_str.replace(\"Z\", \"+00:00\"))\n",
    "        except:\n",
    "            # Fallback for edge cases\n",
    "            from dateutil import parser\n",
    "            return parser.isoparse(time_str)\n",
    "    else:\n",
    "        return datetime.fromisoformat(time_str).replace(tzinfo=timezone.utc)\n",
    "\n",
    "def make_safe_name(name):\n",
    "    \"\"\"Create filesystem/geodatabase safe name.\"\"\"\n",
    "    return name.replace(\"-\", \"_\").replace(\" \", \"_\").replace(\":\", \"\")\n",
    "\n",
    "validate_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GeoJSON Parsing & Feature Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading francine.geojson...\n",
      "  Loaded 2303 features\n",
      "  Time range: 2024-09-09 11:00:36+00:00 â†’ 2024-09-16 15:24:14+00:00\n",
      "Sample feature: {'lon': -92.007126, 'lat': 30.8703881, 'timestamp': datetime.datetime(2024, 9, 10, 23, 58, 43, tzinfo=datetime.timezone.utc), 'FAC': '', 'LOC': '', 'GPE': 'Louisiana', 'make_polygon': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_geojson_features(geojson_path):\n",
    "    \"\"\"Load and validate GeoJSON features. Returns list of (lon, lat, timestamp, properties).\"\"\"\n",
    "    print(f\"Loading {geojson_path.name}...\")\n",
    "    \n",
    "    with open(geojson_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    features = data.get('features', [])\n",
    "    assert len(features) > 0, f\"NEED_INFO: No features found in {geojson_path}\"\n",
    "    \n",
    "    parsed = []\n",
    "    for i, feat in enumerate(features):\n",
    "        geom = feat.get('geometry', {})\n",
    "        props = feat.get('properties', {})\n",
    "        \n",
    "        # Assert Point geometry\n",
    "        assert geom.get('type') == 'Point', f\"Feature {i}: Expected Point geometry, got {geom.get('type')}\"\n",
    "        \n",
    "        # Extract coordinates (authoritative)\n",
    "        coords = geom.get('coordinates', [])\n",
    "        assert len(coords) >= 2, f\"Feature {i}: Invalid coordinates {coords}\"\n",
    "        lon, lat = coords[0], coords[1]\n",
    "        \n",
    "        # Validate required properties\n",
    "        required_keys = ['FAC', 'LOC', 'GPE', 'time', 'Latitude', 'Longitude', 'make_polygon']\n",
    "        for key in required_keys:\n",
    "            assert key in props, f\"Feature {i}: Missing required property '{key}'\"\n",
    "        \n",
    "        # Parse timestamp\n",
    "        timestamp = parse_iso_time(props['time'])\n",
    "        \n",
    "        # QA: compare geometry vs. properties (tolerance 0.001Â°)\n",
    "        lat_prop = float(props['Latitude'])\n",
    "        lon_prop = float(props['Longitude'])\n",
    "        if abs(lat - lat_prop) > 0.001 or abs(lon - lon_prop) > 0.001:\n",
    "            print(f\"  âš  Feature {i}: Coordinate mismatch (geom vs props) - using geometry\")\n",
    "        \n",
    "        parsed.append({\n",
    "            'lon': lon,\n",
    "            'lat': lat,\n",
    "            'timestamp': timestamp,\n",
    "            'FAC': props['FAC'],\n",
    "            'LOC': props['LOC'],\n",
    "            'GPE': props['GPE'],\n",
    "            'make_polygon': int(props['make_polygon'])\n",
    "        })\n",
    "    \n",
    "    print(f\"  Loaded {len(parsed)} features\")\n",
    "    print(f\"  Time range: {min(f['timestamp'] for f in parsed)} â†’ {max(f['timestamp'] for f in parsed)}\")\n",
    "    return parsed\n",
    "\n",
    "# Test load one event\n",
    "test_features = load_geojson_features(INPUTS['francine'])\n",
    "print(f\"Sample feature: {test_features[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 29 time bins (6h each)\n",
      "  Range: 2024-09-09 11:00:00+00:00 â†’ 2024-09-16 17:00:00+00:00\n",
      "  Empty bins: 1\n",
      "Sample bins: [{'start': datetime.datetime(2024, 9, 9, 11, 0, tzinfo=datetime.timezone.utc), 'end': datetime.datetime(2024, 9, 9, 17, 0, tzinfo=datetime.timezone.utc), 'indices': [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98], 'count': 26}, {'start': datetime.datetime(2024, 9, 9, 17, 0, tzinfo=datetime.timezone.utc), 'end': datetime.datetime(2024, 9, 9, 23, 0, tzinfo=datetime.timezone.utc), 'indices': [62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123], 'count': 49}, {'start': datetime.datetime(2024, 9, 9, 23, 0, tzinfo=datetime.timezone.utc), 'end': datetime.datetime(2024, 9, 10, 5, 0, tzinfo=datetime.timezone.utc), 'indices': [124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379], 'count': 55}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_time_bins(features, bin_hours):\n",
    "    \"\"\"Create contiguous time bins, fill empty bins. Returns list of (bin_start, bin_end, feature_indices).\"\"\"\n",
    "    if not features:\n",
    "        return []\n",
    "    \n",
    "    timestamps = [f['timestamp'] for f in features]\n",
    "    min_time = min(timestamps)\n",
    "    max_time = max(timestamps)\n",
    "    \n",
    "    # Round down to hour boundary\n",
    "    start_time = min_time.replace(minute=0, second=0, microsecond=0)\n",
    "    \n",
    "    bins = []\n",
    "    current = start_time\n",
    "    bin_delta = timedelta(hours=bin_hours)\n",
    "    \n",
    "    while current <= max_time:\n",
    "        bin_end = current + bin_delta\n",
    "        \n",
    "        # Find features in this bin\n",
    "        indices = [i for i, f in enumerate(features) \n",
    "                  if current <= f['timestamp'] < bin_end]\n",
    "        \n",
    "        bins.append({\n",
    "            'start': current,\n",
    "            'end': bin_end,\n",
    "            'indices': indices,\n",
    "            'count': len(indices)\n",
    "        })\n",
    "        \n",
    "        current = bin_end\n",
    "    \n",
    "    print(f\"Created {len(bins)} time bins ({bin_hours}h each)\")\n",
    "    print(f\"  Range: {bins[0]['start']} â†’ {bins[-1]['end']}\")\n",
    "    print(f\"  Empty bins: {sum(1 for b in bins if b['count'] == 0)}\")\n",
    "    return bins\n",
    "\n",
    "# Test binning\n",
    "test_bins = create_time_bins(test_features, TIME_BIN_HOURS)\n",
    "print(f\"Sample bins: {test_bins[:3]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spatial Context Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading context layers...\n",
      "  âœ“ States: 52 features\n",
      "  âœ“ Counties: 3222 features\n",
      "Context layers ready\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_context_layers():\n",
    "    \"\"\"Load and prepare reference layers (states, counties) for fusion.\"\"\"\n",
    "    print(\"Loading context layers...\")\n",
    "    \n",
    "    context = {}\n",
    "    \n",
    "    # States (already polygon)\n",
    "    context['states'] = str(INPUTS['states'])\n",
    "    print(f\"  âœ“ States: {arcpy.management.GetCount(context['states'])[0]} features\")\n",
    "    \n",
    "    # Counties (already polygon)\n",
    "    context['counties'] = str(INPUTS['counties'])\n",
    "    print(f\"  âœ“ Counties: {arcpy.management.GetCount(context['counties'])[0]} features\")\n",
    "    \n",
    "    # Note: Cities not loaded - tweet points provide direct location signal\n",
    "    print(\"Context layers ready\\n\")\n",
    "    return context\n",
    "\n",
    "CONTEXT = load_context_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Raster Fusion & Slice Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion functions defined\n"
     ]
    }
   ],
   "source": [
    "def create_tweet_feature_class(features, indices, output_fc):\n",
    "    \"\"\"Convert tweet features to point feature class.\"\"\"\n",
    "    # Create feature class\n",
    "    sr = arcpy.SpatialReference(4326)  # Input is WGS84\n",
    "    arcpy.management.CreateFeatureclass(\n",
    "        os.path.dirname(output_fc),\n",
    "        os.path.basename(output_fc),\n",
    "        \"POINT\",\n",
    "        spatial_reference=sr\n",
    "    )\n",
    "    \n",
    "    # Add fields\n",
    "    arcpy.management.AddField(output_fc, \"tweet_id\", \"LONG\")\n",
    "    arcpy.management.AddField(output_fc, \"FAC\", \"TEXT\", field_length=100)\n",
    "    arcpy.management.AddField(output_fc, \"LOC\", \"TEXT\", field_length=100)\n",
    "    arcpy.management.AddField(output_fc, \"GPE\", \"TEXT\", field_length=100)\n",
    "    \n",
    "    # Insert features\n",
    "    with arcpy.da.InsertCursor(output_fc, [\"SHAPE@XY\", \"tweet_id\", \"FAC\", \"LOC\", \"GPE\"]) as cursor:\n",
    "        for idx in indices:\n",
    "            f = features[idx]\n",
    "            cursor.insertRow([(f['lon'], f['lat']), idx, f['FAC'], f['LOC'], f['GPE']])\n",
    "    \n",
    "    # Project to working CRS\n",
    "    output_proj = output_fc + \"_proj\"\n",
    "    arcpy.management.Project(output_fc, output_proj, arcpy.SpatialReference(CRS_EPSG))\n",
    "    return output_proj\n",
    "\n",
    "def create_intensity_raster(features, indices, output_raster, extent_fc=None):\n",
    "    \"\"\"\n",
    "    Create intensity raster from tweet points using kernel density.\n",
    "    \n",
    "    Fusion strategy: Compute point density of tweets weighted by spatial distribution.\n",
    "    The resulting surface represents tweet activity intensity based on the geographic\n",
    "    concentration of events, producing a continuous field suitable for time-series analysis.\n",
    "    \"\"\"\n",
    "    if len(indices) == 0:\n",
    "        # Empty bin: create zero-filled raster with consistent extent\n",
    "        if extent_fc and arcpy.Exists(extent_fc):\n",
    "            # Use previous extent\n",
    "            desc = arcpy.Describe(extent_fc)\n",
    "            extent = desc.extent\n",
    "            \n",
    "            # Create constant raster\n",
    "            cell_size = CELL_SIZE_KM * 1000\n",
    "            width = int((extent.XMax - extent.XMin) / cell_size)\n",
    "            height = int((extent.YMax - extent.YMin) / cell_size)\n",
    "            \n",
    "            # Use CreateConstantRaster\n",
    "            const_raster = CreateConstantRaster(0, \"FLOAT\", cell_size, \n",
    "                                               arcpy.Extent(extent.XMin, extent.YMin, extent.XMax, extent.YMax))\n",
    "            const_raster.save(output_raster)\n",
    "        else:\n",
    "            # No extent reference: skip\n",
    "            return None\n",
    "    else:\n",
    "        # Create point feature class\n",
    "        temp_fc = os.path.join(str(OUTPUT_GDB), f\"temp_tweets_{make_safe_name(os.path.basename(output_raster))}\")\n",
    "        tweet_fc = create_tweet_feature_class(features, indices, temp_fc)\n",
    "        \n",
    "        # Kernel density estimation\n",
    "        density_raster = KernelDensity(\n",
    "            tweet_fc,\n",
    "            population_field=\"NONE\",\n",
    "            cell_size=CELL_SIZE_KM * 1000,\n",
    "            search_radius=SEARCH_RADIUS_M,\n",
    "            area_unit_scale_factor=\"SQUARE_KILOMETERS\"\n",
    "        )\n",
    "        \n",
    "        # Save\n",
    "        density_raster.save(output_raster)\n",
    "        \n",
    "        # Cleanup temp\n",
    "        arcpy.management.Delete(temp_fc)\n",
    "        if arcpy.Exists(tweet_fc):\n",
    "            arcpy.management.Delete(tweet_fc)\n",
    "    \n",
    "    return output_raster\n",
    "\n",
    "print(\"Fusion functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Raster Slices (Iterative & Cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice generation function ready\n"
     ]
    }
   ],
   "source": [
    "def generate_raster_slices(event_name, features, bins, mode):\n",
    "    \"\"\"\n",
    "    Generate raster slices for all time bins.\n",
    "    \n",
    "    mode='iter': Each slice shows only that bin's tweets\n",
    "    mode='cum': Each slice shows all tweets from start to current bin\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating {mode} slices for {event_name}...\")\n",
    "    \n",
    "    # Apply sample mode limit\n",
    "    bins_to_process = bins[:SAMPLE_NUM_SLICES] if SAMPLE_MODE else bins\n",
    "    if SAMPLE_MODE:\n",
    "        print(f\"  âš¡ Sample mode: processing {len(bins_to_process)}/{len(bins)} bins\")\n",
    "    \n",
    "    slices = []\n",
    "    cumulative_indices = []\n",
    "    extent_ref = None\n",
    "    \n",
    "    for i, bin_info in enumerate(bins_to_process):\n",
    "        bin_start = bin_info['start'].strftime(\"%Y%m%d_%H%M\")\n",
    "        \n",
    "        # Determine which indices to include\n",
    "        if mode == 'iter':\n",
    "            indices = bin_info['indices']\n",
    "        elif mode == 'cum':\n",
    "            cumulative_indices.extend(bin_info['indices'])\n",
    "            indices = cumulative_indices\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "        \n",
    "        # Create output path\n",
    "        slice_name = f\"{event_name}_{mode}_slice_{i:03d}_{bin_start}\"\n",
    "        slice_path = os.path.join(str(OUTPUT_GDB), slice_name)\n",
    "        \n",
    "        # Generate raster\n",
    "        try:\n",
    "            result = create_intensity_raster(features, indices, slice_path, extent_ref)\n",
    "            if result and arcpy.Exists(result):\n",
    "                extent_ref = result  # Use for subsequent empty bins\n",
    "                \n",
    "                slices.append({\n",
    "                    'path': slice_path,\n",
    "                    'name': slice_name,\n",
    "                    'bin_index': i,\n",
    "                    'start_time': bin_info['start'],\n",
    "                    'end_time': bin_info['end'],\n",
    "                    'tweet_count': len(indices)\n",
    "                })\n",
    "                \n",
    "                print(f\"  Created slice {i+1}/{len(bins_to_process)}: {slice_name} ({len(indices)} tweets)\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš  Error creating slice {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"  âœ“ Created {len(slices)} slices\")\n",
    "    return slices\n",
    "\n",
    "print(\"Slice generation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Mosaic Dataset & Multidimensional Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosaic functions ready\n"
     ]
    }
   ],
   "source": [
    "def create_mosaic_dataset(event_name, mode, slices):\n",
    "    \"\"\"Create mosaic dataset and populate with raster slices.\"\"\"\n",
    "    print(f\"\\nCreating mosaic dataset: {event_name}_{mode}...\")\n",
    "    \n",
    "    mosaic_name = f\"{event_name}_{mode}_mosaic\"\n",
    "    mosaic_path = os.path.join(str(OUTPUT_GDB), mosaic_name)\n",
    "    \n",
    "    # Delete if exists\n",
    "    if arcpy.Exists(mosaic_path):\n",
    "        arcpy.management.Delete(mosaic_path)\n",
    "    \n",
    "    # Create mosaic dataset\n",
    "    arcpy.management.CreateMosaicDataset(\n",
    "        str(OUTPUT_GDB),\n",
    "        mosaic_name,\n",
    "        arcpy.SpatialReference(CRS_EPSG),\n",
    "        num_bands=1,\n",
    "        pixel_type=\"32_BIT_FLOAT\"\n",
    "    )\n",
    "    \n",
    "    # Create table for AddRasters (Table raster type)\n",
    "    table_name = f\"{event_name}_{mode}_manifest\"\n",
    "    table_path = os.path.join(str(OUTPUT_GDB), table_name)\n",
    "    \n",
    "    if arcpy.Exists(table_path):\n",
    "        arcpy.management.Delete(table_path)\n",
    "    \n",
    "    arcpy.management.CreateTable(str(OUTPUT_GDB), table_name)\n",
    "    # Field MUST be named \"Raster\" for Table raster type\n",
    "    arcpy.management.AddField(table_path, \"Raster\", \"TEXT\", field_length=512)\n",
    "    arcpy.management.AddField(table_path, \"Date\", \"DATE\")\n",
    "    arcpy.management.AddField(table_path, \"Variable\", \"TEXT\", field_length=50)\n",
    "    \n",
    "    # Populate table\n",
    "    with arcpy.da.InsertCursor(table_path, [\"Raster\", \"Date\", \"Variable\"]) as cursor:\n",
    "        for s in slices:\n",
    "            cursor.insertRow([s['path'], s['start_time'], \"intensity\"])\n",
    "    \n",
    "    print(f\"  âœ“ Manifest table: {table_name} ({len(slices)} records)\")\n",
    "    \n",
    "    # Add rasters to mosaic using Table raster type\n",
    "    arcpy.management.AddRastersToMosaicDataset(\n",
    "        mosaic_path,\n",
    "        \"Table\",\n",
    "        table_path,\n",
    "        update_cellsize_ranges=\"UPDATE_CELL_SIZES\",\n",
    "        update_boundary=\"UPDATE_BOUNDARY\"\n",
    "    )\n",
    "    \n",
    "    print(f\"  âœ“ Added {len(slices)} rasters to mosaic\")\n",
    "    \n",
    "    # Build multidimensional info\n",
    "    arcpy.management.BuildMultidimensionalInfo(\n",
    "        mosaic_path,\n",
    "        variable_field=\"Variable\",\n",
    "        dimension_fields=\"Date\"\n",
    "    )\n",
    "    \n",
    "    print(f\"  âœ“ Built multidimensional info (Date Ã— Variable)\")\n",
    "    \n",
    "    # Optional: Build pyramids and statistics\n",
    "    try:\n",
    "        arcpy.management.BuildPyramidsandStatistics(\n",
    "            mosaic_path,\n",
    "            build_pyramids=True,\n",
    "            calculate_statistics=True\n",
    "        )\n",
    "        print(f\"  âœ“ Built pyramids and statistics\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âš  Pyramid/stats build warning: {e}\")\n",
    "    \n",
    "    return mosaic_path, table_path\n",
    "\n",
    "print(\"Mosaic functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export CRF & Space-Time Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export functions ready\n"
     ]
    }
   ],
   "source": [
    "def export_crf(mosaic_path, output_crf):\n",
    "    \"\"\"Export mosaic dataset to Cloud Raster Format (CRF) with multidimensional support.\"\"\"\n",
    "    print(f\"  Exporting to CRF: {output_crf}...\")\n",
    "    \n",
    "    arcpy.management.CopyRaster(\n",
    "        mosaic_path,\n",
    "        output_crf,\n",
    "        pixel_type=\"32_BIT_FLOAT\",\n",
    "        format=\"CRF\",\n",
    "        process_as_multidimensional=\"PROCESS_AS_MULTIDIMENSIONAL\",\n",
    "        build_multidimensional_transpose=\"TRANSPOSE\"\n",
    "    )\n",
    "    \n",
    "    print(f\"  âœ“ CRF created: {output_crf}\")\n",
    "    return output_crf\n",
    "\n",
    "def export_space_time_cube(crf_path, output_nc):\n",
    "    \"\"\"Create Space-Time Cube (.nc) from multidimensional raster.\"\"\"\n",
    "    print(f\"  Creating Space-Time Cube: {output_nc}...\")\n",
    "    \n",
    "    try:\n",
    "        stpm.CreateSpaceTimeCubeFromMultidimensionalRasterLayer(\n",
    "            crf_path,\n",
    "            output_nc,\n",
    "            \"intensity\"\n",
    "        )\n",
    "        print(f\"  âœ“ Space-Time Cube created: {output_nc}\")\n",
    "        return output_nc\n",
    "    except Exception as e:\n",
    "        print(f\"  âš  Space-Time Cube creation failed: {e}\")\n",
    "        print(f\"     This may be due to data extent or toolbox version issues.\")\n",
    "        return None\n",
    "\n",
    "print(\"Export functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. QA Checks & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA functions ready\n"
     ]
    }
   ],
   "source": [
    "def qa_check_slices(slices):\n",
    "    \"\"\"Validate that all slices have consistent CRS, extent, and pixel size.\"\"\"\n",
    "    if not slices:\n",
    "        return {\"status\": \"FAIL\", \"reason\": \"No slices to check\"}\n",
    "    \n",
    "    print(\"  Running QA checks...\")\n",
    "    \n",
    "    # Get reference properties from first slice\n",
    "    ref = arcpy.Describe(slices[0]['path'])\n",
    "    ref_sr = ref.spatialReference.factoryCode\n",
    "    ref_extent = ref.extent\n",
    "    ref_cell = (ref.meanCellWidth, ref.meanCellHeight)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    for i, s in enumerate(slices[1:], start=1):\n",
    "        desc = arcpy.Describe(s['path'])\n",
    "        \n",
    "        # Check CRS\n",
    "        if desc.spatialReference.factoryCode != ref_sr:\n",
    "            issues.append(f\"Slice {i}: CRS mismatch ({desc.spatialReference.factoryCode} vs {ref_sr})\")\n",
    "        \n",
    "        # Check extent (tolerance 1m)\n",
    "        ext = desc.extent\n",
    "        if (abs(ext.XMin - ref_extent.XMin) > 1 or \n",
    "            abs(ext.YMin - ref_extent.YMin) > 1 or\n",
    "            abs(ext.XMax - ref_extent.XMax) > 1 or\n",
    "            abs(ext.YMax - ref_extent.YMax) > 1):\n",
    "            issues.append(f\"Slice {i}: Extent mismatch\")\n",
    "        \n",
    "        # Check pixel size (tolerance 0.1m)\n",
    "        if (abs(desc.meanCellWidth - ref_cell[0]) > 0.1 or\n",
    "            abs(desc.meanCellHeight - ref_cell[1]) > 0.1):\n",
    "            issues.append(f\"Slice {i}: Pixel size mismatch\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"    âš  Found {len(issues)} issues:\")\n",
    "        for issue in issues[:5]:  # Show first 5\n",
    "            print(f\"      - {issue}\")\n",
    "        return {\"status\": \"WARN\", \"issues\": issues}\n",
    "    else:\n",
    "        print(f\"    âœ“ All {len(slices)} slices consistent\")\n",
    "        return {\n",
    "            \"status\": \"PASS\",\n",
    "            \"slice_count\": len(slices),\n",
    "            \"crs_epsg\": ref_sr,\n",
    "            \"extent\": [ref_extent.XMin, ref_extent.YMin, ref_extent.XMax, ref_extent.YMax],\n",
    "            \"cell_size_m\": ref_cell\n",
    "        }\n",
    "\n",
    "def create_qa_report(results):\n",
    "    \"\"\"Generate JSON QA report.\"\"\"\n",
    "    report_path = OUTPUT_ROOT / \"qa_report.json\"\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š QA Report saved: {report_path}\")\n",
    "    return report_path\n",
    "\n",
    "print(\"QA functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Main Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE MODE: QUICK TEST PIPELINE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PROCESSING EVENT: FRANCINE\n",
      "================================================================================\n",
      "Loading francine.geojson...\n",
      "  Loaded 2303 features\n",
      "  Time range: 2024-09-09 11:00:36+00:00 â†’ 2024-09-16 15:24:14+00:00\n",
      "Created 29 time bins (6h each)\n",
      "  Range: 2024-09-09 11:00:00+00:00 â†’ 2024-09-16 17:00:00+00:00\n",
      "  Empty bins: 1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MODE: ITER\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Generating iter slices for francine...\n",
      "  âš¡ Sample mode: processing 4/29 bins\n",
      "  Created slice 1/4: francine_iter_slice_000_20240909_1100 (26 tweets)\n",
      "  Created slice 2/4: francine_iter_slice_001_20240909_1700 (49 tweets)\n",
      "  Created slice 3/4: francine_iter_slice_002_20240909_2300 (55 tweets)\n",
      "  Created slice 4/4: francine_iter_slice_003_20240910_0500 (25 tweets)\n",
      "  âœ“ Created 4 slices\n",
      "  Running QA checks...\n",
      "    âš  Found 3 issues:\n",
      "      - Slice 1: Extent mismatch\n",
      "      - Slice 2: Extent mismatch\n",
      "      - Slice 3: Extent mismatch\n",
      "\n",
      "Creating mosaic dataset: francine_iter...\n",
      "  âœ“ Manifest table: francine_iter_manifest (4 records)\n",
      "  âœ“ Added 4 rasters to mosaic\n",
      "  âœ“ Built multidimensional info (Date Ã— Variable)\n",
      "  âœ“ Built pyramids and statistics\n",
      "  Exporting to CRF: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\outputs\\francine_iter.crf...\n"
     ]
    },
    {
     "ename": "<class 'arcgisscripting.ExecuteError'>",
     "evalue": "ERROR 000622: Failed to execute (Copy Raster). Parameters are not valid.\nERROR 000800: The value is not a member of ALL_SLICES | CURRENT_SLICE.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExecuteError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Execute pipeline\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m pipeline_results \u001b[38;5;241m=\u001b[39m run_pipeline()\n",
      "Cell \u001b[1;32mIn[36], line 49\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m crf_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.crf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m crf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(OUTPUT_ROOT \u001b[38;5;241m/\u001b[39m crf_name)\n\u001b[1;32m---> 49\u001b[0m export_crf(mosaic_path, crf_path)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Export Space-Time Cube\u001b[39;00m\n\u001b[0;32m     52\u001b[0m nc_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m, in \u001b[0;36mexport_crf\u001b[1;34m(mosaic_path, output_crf)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Export mosaic dataset to Cloud Raster Format (CRF) with multidimensional support.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Exporting to CRF: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_crf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m arcpy\u001b[38;5;241m.\u001b[39mmanagement\u001b[38;5;241m.\u001b[39mCopyRaster(\n\u001b[0;32m      6\u001b[0m     mosaic_path,\n\u001b[0;32m      7\u001b[0m     output_crf,\n\u001b[0;32m      8\u001b[0m     pixel_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m32_BIT_FLOAT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRF\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     process_as_multidimensional\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROCESS_AS_MULTIDIMENSIONAL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     build_multidimensional_transpose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRANSPOSE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  âœ“ CRF created: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_crf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_crf\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\management.py:27490\u001b[0m, in \u001b[0;36mCopyRaster\u001b[1;34m(in_raster, out_rasterdataset, config_keyword, background_value, nodata_value, onebit_to_eightbit, colormap_to_RGB, pixel_type, scale_pixel_value, RGB_to_Colormap, format, transform, process_as_multidimensional, build_multidimensional_transpose)\u001b[0m\n\u001b[0;32m  27488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[0;32m  27489\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m> 27490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\management.py:27466\u001b[0m, in \u001b[0;36mCopyRaster\u001b[1;34m(in_raster, out_rasterdataset, config_keyword, background_value, nodata_value, onebit_to_eightbit, colormap_to_RGB, pixel_type, scale_pixel_value, RGB_to_Colormap, format, transform, process_as_multidimensional, build_multidimensional_transpose)\u001b[0m\n\u001b[0;32m  27462\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01marcpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marcobjects\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marcobjectconversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convertArcObjectToPythonObject\n\u001b[0;32m  27464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m  27465\u001b[0m     retval \u001b[38;5;241m=\u001b[39m convertArcObjectToPythonObject(\n\u001b[1;32m> 27466\u001b[0m         gp\u001b[38;5;241m.\u001b[39mCopyRaster_management(\n\u001b[0;32m  27467\u001b[0m             \u001b[38;5;241m*\u001b[39mgp_fixargs(\n\u001b[0;32m  27468\u001b[0m                 (\n\u001b[0;32m  27469\u001b[0m                     in_raster,\n\u001b[0;32m  27470\u001b[0m                     out_rasterdataset,\n\u001b[0;32m  27471\u001b[0m                     config_keyword,\n\u001b[0;32m  27472\u001b[0m                     background_value,\n\u001b[0;32m  27473\u001b[0m                     nodata_value,\n\u001b[0;32m  27474\u001b[0m                     onebit_to_eightbit,\n\u001b[0;32m  27475\u001b[0m                     colormap_to_RGB,\n\u001b[0;32m  27476\u001b[0m                     pixel_type,\n\u001b[0;32m  27477\u001b[0m                     scale_pixel_value,\n\u001b[0;32m  27478\u001b[0m                     RGB_to_Colormap,\n\u001b[0;32m  27479\u001b[0m                     \u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m  27480\u001b[0m                     transform,\n\u001b[0;32m  27481\u001b[0m                     process_as_multidimensional,\n\u001b[0;32m  27482\u001b[0m                     build_multidimensional_transpose,\n\u001b[0;32m  27483\u001b[0m                 ),\n\u001b[0;32m  27484\u001b[0m                 \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m  27485\u001b[0m             )\n\u001b[0;32m  27486\u001b[0m         )\n\u001b[0;32m  27487\u001b[0m     )\n\u001b[0;32m  27488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[0;32m  27489\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\_base.py:533\u001b[0m, in \u001b[0;36mGeoprocessor.__getattr__.<locals>.<lambda>\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    531\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gp, attr)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(val):\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs: val(\u001b[38;5;241m*\u001b[39mgp_fixargs(args, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convertArcObjectToPythonObject(val)\n",
      "\u001b[1;31mExecuteError\u001b[0m: ERROR 000622: Failed to execute (Copy Raster). Parameters are not valid.\nERROR 000800: The value is not a member of ALL_SLICES | CURRENT_SLICE.\n"
     ]
    }
   ],
   "source": [
    "def run_pipeline():\n",
    "    \"\"\"Execute full pipeline for all events and modes.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    if SAMPLE_MODE:\n",
    "        print(\"SAMPLE MODE: QUICK TEST PIPELINE\")\n",
    "    else:\n",
    "        print(\"STARTING FULL RASTER PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Use sample event list or full event list\n",
    "    events_to_process = [SAMPLE_EVENT] if SAMPLE_MODE else EVENTS\n",
    "    \n",
    "    for event in events_to_process:\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"PROCESSING EVENT: {event.upper()}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        # Load features\n",
    "        features = load_geojson_features(INPUTS[event])\n",
    "        \n",
    "        # Create time bins\n",
    "        bins = create_time_bins(features, TIME_BIN_HOURS)\n",
    "        \n",
    "        results[event] = {}\n",
    "        \n",
    "        for mode in MODES:\n",
    "            print(f\"\\n{'-' * 80}\")\n",
    "            print(f\"MODE: {mode.upper()}\")\n",
    "            print(f\"{'-' * 80}\")\n",
    "            \n",
    "            # Generate raster slices\n",
    "            slices = generate_raster_slices(event, features, bins, mode)\n",
    "            \n",
    "            if not slices:\n",
    "                print(f\"  âš  No slices created for {event}_{mode}\")\n",
    "                continue\n",
    "            \n",
    "            # QA check slices\n",
    "            qa_result = qa_check_slices(slices)\n",
    "            \n",
    "            # Create mosaic dataset\n",
    "            mosaic_path, table_path = create_mosaic_dataset(event, mode, slices)\n",
    "            \n",
    "            # Export CRF\n",
    "            crf_name = f\"{event}_{mode}.crf\"\n",
    "            crf_path = str(OUTPUT_ROOT / crf_name)\n",
    "            export_crf(mosaic_path, crf_path)\n",
    "            \n",
    "            # Export Space-Time Cube\n",
    "            nc_name = f\"{event}_{mode}.nc\"\n",
    "            nc_path = str(OUTPUT_ROOT / nc_name)\n",
    "            export_space_time_cube(crf_path, nc_path)\n",
    "            \n",
    "            # Store results\n",
    "            results[event][mode] = {\n",
    "                \"slice_count\": len(slices),\n",
    "                \"time_range\": [slices[0]['start_time'], slices[-1]['end_time']],\n",
    "                \"mosaic_dataset\": mosaic_path,\n",
    "                \"manifest_table\": table_path,\n",
    "                \"crf_output\": crf_path,\n",
    "                \"nc_output\": nc_path,\n",
    "                \"qa_status\": qa_result\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n  âœ… {event.upper()} {mode.upper()} complete\")\n",
    "    \n",
    "    # Generate QA report\n",
    "    report = create_qa_report(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nOutputs saved to: {OUTPUT_ROOT}\")\n",
    "    print(f\"\\nProducts per event:\")\n",
    "    for event in events_to_process:\n",
    "        print(f\"  {event.upper()}:\")\n",
    "        for mode in MODES:\n",
    "            if mode in results.get(event, {}):\n",
    "                r = results[event][mode]\n",
    "                print(f\"    {mode}: {r['slice_count']} slices â†’ CRF + NC\")\n",
    "    \n",
    "    if SAMPLE_MODE:\n",
    "        print(f\"\\nâš¡ Sample mode was enabled. To run full pipeline:\")\n",
    "        print(f\"   Set SAMPLE_MODE = False in the configuration cell\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute pipeline\n",
    "pipeline_results = run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Verification & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“‹ VERIFICATION CHECKLIST:\")\n",
    "print(\"\\n1. Add CRF files to ArcGIS Pro map:\")\n",
    "for event in EVENTS:\n",
    "    for mode in MODES:\n",
    "        crf_path = OUTPUT_ROOT / f\"{event}_{mode}.crf\"\n",
    "        print(f\"   - {crf_path}\")\n",
    "\n",
    "print(\"\\n2. Enable Time on the map and verify:\")\n",
    "print(\"   - Time slider appears\")\n",
    "print(\"   - Raster updates as you scrub through time\")\n",
    "print(\"   - Check Layer Properties â†’ Time for multidimensional info\")\n",
    "\n",
    "print(\"\\n3. Test Space-Time Cubes with STPM tools:\")\n",
    "print(\"   - Emerging Hot Spot Analysis\")\n",
    "print(\"   - Local Outlier Analysis\")\n",
    "print(\"   - 3D visualization in Scene\")\n",
    "\n",
    "print(\"\\n4. Verify consistency:\")\n",
    "print(\"   - All slices same extent/CRS/pixel size (see QA report)\")\n",
    "print(\"   - Mosaic footprints show Standard Time dimension\")\n",
    "print(f\"   - Check: {OUTPUT_ROOT / 'qa_report.json'}\")\n",
    "\n",
    "print(\"\\n5. Compare iterative vs cumulative:\")\n",
    "print(\"   - Iterative: shows per-bin activity (changes over time)\")\n",
    "print(\"   - Cumulative: shows accumulated activity (grows over time)\")\n",
    "\n",
    "print(\"\\nâœ… Pipeline complete. Review outputs and QA report.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
