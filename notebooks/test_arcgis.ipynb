{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de08420d",
   "metadata": {},
   "source": [
    "# ArcGIS Pro Tweet Mention Processing\n",
    "\n",
    "This notebook recreates the tweet mention aggregation workflow with `arcpy` so that it can run directly inside ArcGIS Pro 3.5 without relying on GeoPandas. It loads the same data inputs as the original workflow, counts geopolitical entity mentions, bins them into four-hour intervals, and exports incremental and cumulative feature classes and CSV summaries that are ready to use in ArcGIS Pro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8641dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import difflib\n",
    "import datetime as dt\n",
    "import uuid\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c93ae237",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_OUTPUT_FOLDER = 'arcgis_outputs'\n",
    "FOUR_HOUR_INTERVAL = 4\n",
    "\n",
    "\n",
    "def get_project_root():\n",
    "    return os.getcwd()\n",
    "\n",
    "\n",
    "def get_data_file_path(*segments):\n",
    "    return os.path.join(get_project_root(), *segments)\n",
    "\n",
    "\n",
    "def ensure_directory(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "\n",
    "def find_field_case_insensitive(dataset, field_name):\n",
    "    target = field_name.lower()\n",
    "    for field in arcpy.ListFields(dataset):\n",
    "        if field.name.lower() == target:\n",
    "            return field.name\n",
    "    raise ValueError(f\"Field '{field_name}' not found in {dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e244a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweet_feature_class():\n",
    "    geojson_path = get_data_file_path('data', 'geojson', 'helene.geojson')\n",
    "    scratch_gdb = arcpy.env.scratchGDB\n",
    "    ensure_directory(os.path.dirname(scratch_gdb))\n",
    "\n",
    "    intermediate_name = os.path.join(scratch_gdb, 'tweets_mentions_raw')\n",
    "    projected_name = os.path.join(scratch_gdb, 'tweets_mentions_4326')\n",
    "    wgs84 = arcpy.SpatialReference(4326)\n",
    "\n",
    "    if arcpy.Exists(intermediate_name):\n",
    "        arcpy.management.Delete(intermediate_name)\n",
    "    if arcpy.Exists(projected_name):\n",
    "        arcpy.management.Delete(projected_name)\n",
    "\n",
    "    result = arcpy.conversion.JSONToFeatures(geojson_path, intermediate_name)\n",
    "    tweet_fc = result.getOutput(0)\n",
    "\n",
    "    desc = arcpy.Describe(tweet_fc)\n",
    "    sr = getattr(desc, 'spatialReference', None)\n",
    "    if not sr or not getattr(sr, 'name', None):\n",
    "        arcpy.management.DefineProjection(tweet_fc, wgs84)\n",
    "        sr = wgs84\n",
    "\n",
    "    if sr.factoryCode == wgs84.factoryCode:\n",
    "        arcpy.management.CopyFeatures(tweet_fc, projected_name)\n",
    "    else:\n",
    "        arcpy.management.Project(tweet_fc, projected_name, wgs84)\n",
    "\n",
    "    return projected_name\n",
    "\n",
    "\n",
    "def load_states_feature_class():\n",
    "    source = get_data_file_path('data', 'shape_files', 'cb_2023_us_state_20m.shp')\n",
    "    scratch_gdb = arcpy.env.scratchGDB\n",
    "    output_fc = os.path.join(scratch_gdb, 'us_states_4326')\n",
    "    if arcpy.Exists(output_fc):\n",
    "        arcpy.management.Delete(output_fc)\n",
    "    arcpy.management.Project(source, output_fc, arcpy.SpatialReference(4326))\n",
    "    return output_fc\n",
    "\n",
    "\n",
    "def load_counties_feature_class():\n",
    "    source = get_data_file_path('data', 'shape_files', 'cb_2023_us_county_20m.shp')\n",
    "    scratch_gdb = arcpy.env.scratchGDB\n",
    "    output_fc = os.path.join(scratch_gdb, 'us_counties_4326')\n",
    "    if arcpy.Exists(output_fc):\n",
    "        arcpy.management.Delete(output_fc)\n",
    "    arcpy.management.Project(source, output_fc, arcpy.SpatialReference(4326))\n",
    "    return output_fc\n",
    "\n",
    "\n",
    "def load_cities_feature_class():\n",
    "    csv_path = get_data_file_path('data', 'tables', 'cities1000.csv')\n",
    "    scratch_gdb = arcpy.env.scratchGDB\n",
    "    view_name = f\"cities_view_{uuid.uuid4().hex[:6]}\"\n",
    "    table_view = arcpy.management.MakeTableView(csv_path, view_name)\n",
    "\n",
    "    expression = \"country_code = 'US' AND feature_class = 'P' AND population IS NOT NULL AND latitude IS NOT NULL AND longitude IS NOT NULL\"\n",
    "    arcpy.management.SelectLayerByAttribute(table_view, 'NEW_SELECTION', expression)\n",
    "\n",
    "    temp_table = os.path.join('in_memory', f\"cities_{uuid.uuid4().hex[:6]}\")\n",
    "    arcpy.management.CopyRows(table_view, temp_table)\n",
    "\n",
    "    output_fc = os.path.join(scratch_gdb, 'us_cities_4326')\n",
    "    if arcpy.Exists(output_fc):\n",
    "        arcpy.management.Delete(output_fc)\n",
    "\n",
    "    arcpy.management.XYTableToPoint(temp_table, output_fc, 'longitude', 'latitude', coordinate_system=arcpy.SpatialReference(4326))\n",
    "\n",
    "    arcpy.management.Delete(table_view)\n",
    "    arcpy.management.Delete(temp_table)\n",
    "    return output_fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00281cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_place_name(value):\n",
    "    if value is None:\n",
    "        return None\n",
    "    name = str(value).upper().strip()\n",
    "    if not name or name in {'', 'NAN'}:\n",
    "        return None\n",
    "    name = re.sub(r\"\\bST\\.?\\b\", 'SAINT', name)\n",
    "    name = re.sub(r\"\\bMT\\.?\\b\", 'MOUNT', name)\n",
    "    name = re.sub(r\"\\bFT\\.?\\b\", 'FORT', name)\n",
    "    name = re.sub(r\"[^A-Z0-9\\s]\", '', name)\n",
    "    name = re.sub(r\"\\s+\", ' ', name)\n",
    "    return name.strip()\n",
    "\n",
    "\n",
    "def parse_gpe_entities(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    value = str(text).strip()\n",
    "    if not value:\n",
    "        return []\n",
    "    entities = []\n",
    "    for part in [segment.strip() for segment in value.split(',') if segment.strip()]:\n",
    "        for sub_part in re.split(r'[;&|]', part):\n",
    "            normalized = preprocess_place_name(sub_part)\n",
    "            if normalized and len(normalized) > 1:\n",
    "                entities.append(normalized)\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for ent in entities:\n",
    "        if ent not in seen:\n",
    "            unique.append(ent)\n",
    "            seen.add(ent)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def fuzzy_match_entity(entity, lookup_dict, threshold=0.85):\n",
    "    if entity in lookup_dict:\n",
    "        return lookup_dict[entity], 1.0\n",
    "    if not lookup_dict:\n",
    "        return None, 0.0\n",
    "    choices = list(lookup_dict.keys())\n",
    "    matches = difflib.get_close_matches(entity, choices, n=1, cutoff=threshold)\n",
    "    if matches:\n",
    "        key = matches[0]\n",
    "        score = difflib.SequenceMatcher(None, entity, key).ratio()\n",
    "        if score >= threshold:\n",
    "            return lookup_dict[key], score\n",
    "    return None, 0.0\n",
    "\n",
    "\n",
    "def parse_datetime(value):\n",
    "    if isinstance(value, dt.datetime):\n",
    "        return value.replace(tzinfo=None)\n",
    "    if value in (None, '', 'nan', 'NaN'):\n",
    "        return None\n",
    "    text = str(value).strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    text = text.replace('Z', '+00:00')\n",
    "    formats = [\n",
    "        '%Y-%m-%d %H:%M:%S',\n",
    "        '%Y-%m-%d %H:%M',\n",
    "        '%Y-%m-%dT%H:%M:%S',\n",
    "        '%Y-%m-%dT%H:%M:%S.%f',\n",
    "        '%Y-%m-%dT%H:%M:%S%z',\n",
    "        '%Y-%m-%dT%H:%M:%S.%f%z'\n",
    "    ]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            parsed = dt.datetime.strptime(text, fmt)\n",
    "            if parsed.tzinfo:\n",
    "                parsed = parsed.astimezone(dt.timezone.utc).replace(tzinfo=None)\n",
    "            return parsed\n",
    "        except ValueError:\n",
    "            continue\n",
    "    try:\n",
    "        parsed = dt.datetime.fromisoformat(text)\n",
    "        if parsed.tzinfo:\n",
    "            parsed = parsed.astimezone(dt.timezone.utc).replace(tzinfo=None)\n",
    "        return parsed\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def floor_datetime(value, hours=4):\n",
    "    if value is None:\n",
    "        return None\n",
    "    hour = (value.hour // hours) * hours\n",
    "    return value.replace(hour=hour, minute=0, second=0, microsecond=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f91da5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_dictionaries(states_fc, counties_fc, cities_fc):\n",
    "    state_lookup = {}\n",
    "    state_field = find_field_case_insensitive(states_fc, 'STUSPS')\n",
    "    state_name_field = find_field_case_insensitive(states_fc, 'NAME')\n",
    "\n",
    "    with arcpy.da.SearchCursor(states_fc, [state_field, state_name_field]) as cursor:\n",
    "        for stusps, name in cursor:\n",
    "            normalized = preprocess_place_name(name)\n",
    "            if not normalized:\n",
    "                continue\n",
    "            entry = {\n",
    "                'STUSPS': stusps,\n",
    "                'NAME': name,\n",
    "                'lookup_key': normalized\n",
    "            }\n",
    "            state_lookup[normalized] = entry\n",
    "            if stusps:\n",
    "                state_lookup[str(stusps).upper()] = entry\n",
    "\n",
    "    county_lookup = {}\n",
    "    county_id_field = find_field_case_insensitive(counties_fc, 'GEOID')\n",
    "    county_name_field = find_field_case_insensitive(counties_fc, 'NAME')\n",
    "\n",
    "    with arcpy.da.SearchCursor(counties_fc, [county_id_field, county_name_field]) as cursor:\n",
    "        for geoid, name in cursor:\n",
    "            normalized = preprocess_place_name(name)\n",
    "            if not normalized:\n",
    "                continue\n",
    "            county_lookup[normalized] = {\n",
    "                'GEOID': str(geoid),\n",
    "                'NAME': name\n",
    "            }\n",
    "\n",
    "    city_lookup = {}\n",
    "    city_id_field = find_field_case_insensitive(cities_fc, 'geonameid')\n",
    "    city_name_field = find_field_case_insensitive(cities_fc, 'name')\n",
    "    lat_field = find_field_case_insensitive(cities_fc, 'latitude')\n",
    "    lon_field = find_field_case_insensitive(cities_fc, 'longitude')\n",
    "    pop_field = find_field_case_insensitive(cities_fc, 'population')\n",
    "\n",
    "    with arcpy.da.SearchCursor(cities_fc, [city_id_field, city_name_field, lat_field, lon_field, pop_field]) as cursor:\n",
    "        for geonameid, name, lat, lon, pop in cursor:\n",
    "            normalized = preprocess_place_name(name)\n",
    "            if not normalized:\n",
    "                continue\n",
    "            entry = {\n",
    "                'geonameid': str(geonameid),\n",
    "                'name': name,\n",
    "                'latitude': float(lat) if lat not in (None, '') else None,\n",
    "                'longitude': float(lon) if lon not in (None, '') else None,\n",
    "                'population': int(pop) if pop not in (None, '') else None\n",
    "            }\n",
    "            city_lookup[normalized] = entry\n",
    "    return state_lookup, county_lookup, city_lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c7bc77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweet_records(tweets_fc):\n",
    "    gpe_field = find_field_case_insensitive(tweets_fc, 'GPE')\n",
    "    time_field_name = None\n",
    "    for candidate in ['time', 'timestamp', 'created_at', 'createdat']:\n",
    "        try:\n",
    "            time_field_name = find_field_case_insensitive(tweets_fc, candidate)\n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "    if time_field_name is None:\n",
    "        raise ValueError('No time-like field found in the tweet feature class.')\n",
    "\n",
    "    records = []\n",
    "    with arcpy.da.SearchCursor(tweets_fc, ['OID@', gpe_field, time_field_name]) as cursor:\n",
    "        for oid, gpe_text, time_value in cursor:\n",
    "            gpe_text = gpe_text if gpe_text not in (None, '') else ''\n",
    "            parsed_time = parse_datetime(time_value)\n",
    "            record = {\n",
    "                'oid': oid,\n",
    "                'original_gpe': gpe_text,\n",
    "                'entities': parse_gpe_entities(gpe_text),\n",
    "                'time': parsed_time,\n",
    "                'time_string': parsed_time.isoformat() if parsed_time else ''\n",
    "            }\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "\n",
    "def assign_time_bins(records, bin_hours=FOUR_HOUR_INTERVAL):\n",
    "    bins = set()\n",
    "    for record in records:\n",
    "        record['time_bin'] = floor_datetime(record['time'], hours=bin_hours)\n",
    "        if record['time_bin'] is not None:\n",
    "            bins.add(record['time_bin'])\n",
    "    return sorted(bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b28805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mentions_in_tweets(records, state_lookup, county_lookup, city_lookup):\n",
    "    state_mentions = defaultdict(int)\n",
    "    county_mentions = defaultdict(int)\n",
    "    city_mentions = defaultdict(int)\n",
    "\n",
    "    state_details = defaultdict(list)\n",
    "    county_details = defaultdict(list)\n",
    "    city_details = defaultdict(list)\n",
    "\n",
    "    for record in records:\n",
    "        for entity in record['entities']:\n",
    "            state_match, state_score = fuzzy_match_entity(entity, state_lookup, threshold=0.9)\n",
    "            if state_match:\n",
    "                state_code = state_match['STUSPS']\n",
    "                state_mentions[state_code] += 1\n",
    "                state_details[state_code].append({\n",
    "                    'matched_entity': entity,\n",
    "                    'original_gpe': record['original_gpe'],\n",
    "                    'time': record['time_string']\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            county_match, county_score = fuzzy_match_entity(entity, county_lookup, threshold=0.85)\n",
    "            if county_match:\n",
    "                county_id = county_match['GEOID']\n",
    "                county_mentions[county_id] += 1\n",
    "                county_details[county_id].append({\n",
    "                    'matched_entity': entity,\n",
    "                    'original_gpe': record['original_gpe'],\n",
    "                    'time': record['time_string']\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            city_match, city_score = fuzzy_match_entity(entity, city_lookup, threshold=0.85)\n",
    "            if city_match:\n",
    "                city_id = city_match['geonameid']\n",
    "                city_mentions[city_id] += 1\n",
    "                city_details[city_id].append({\n",
    "                    'matched_entity': entity,\n",
    "                    'original_gpe': record['original_gpe'],\n",
    "                    'time': record['time_string']\n",
    "                })\n",
    "\n",
    "    return (\n",
    "        state_mentions,\n",
    "        county_mentions,\n",
    "        city_mentions,\n",
    "        state_details,\n",
    "        county_details,\n",
    "        city_details\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2382febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overall_summaries(state_mentions, county_mentions, city_mentions,\n",
    "                             state_details, county_details, city_details,\n",
    "                             city_lookup):\n",
    "    states_summary = []\n",
    "    for code, count in state_mentions.items():\n",
    "        samples = state_details.get(code, [])\n",
    "        states_summary.append({\n",
    "            'state_code': code,\n",
    "            'tweet_count': count,\n",
    "            'sample_mentions': '; '.join(detail['matched_entity'] for detail in samples[:5]),\n",
    "            'sample_gpe_text': '; '.join(detail['original_gpe'][:100] for detail in samples[:3])\n",
    "        })\n",
    "\n",
    "    counties_summary = []\n",
    "    for fips, count in county_mentions.items():\n",
    "        samples = county_details.get(fips, [])\n",
    "        counties_summary.append({\n",
    "            'county_fips': fips,\n",
    "            'tweet_count': count,\n",
    "            'sample_mentions': '; '.join(detail['matched_entity'] for detail in samples[:5]),\n",
    "            'sample_gpe_text': '; '.join(detail['original_gpe'][:100] for detail in samples[:3])\n",
    "        })\n",
    "\n",
    "    cities_summary = []\n",
    "    for city_id, count in city_mentions.items():\n",
    "        samples = city_details.get(city_id, [])\n",
    "        matched = '; '.join(detail['matched_entity'] for detail in samples)\n",
    "        original = ' | '.join(detail['original_gpe'] for detail in samples)\n",
    "        lookup_info = None\n",
    "        for entry in city_lookup.values():\n",
    "            if entry['geonameid'] == city_id:\n",
    "                lookup_info = entry\n",
    "                break\n",
    "        cities_summary.append({\n",
    "            'city_id': city_id,\n",
    "            'tweet_count': count,\n",
    "            'matched_entities': matched,\n",
    "            'original_gpe_text': original,\n",
    "            'name': lookup_info['name'] if lookup_info else None,\n",
    "            'population': lookup_info['population'] if lookup_info else None,\n",
    "            'latitude': lookup_info['latitude'] if lookup_info else None,\n",
    "            'longitude': lookup_info['longitude'] if lookup_info else None\n",
    "        })\n",
    "\n",
    "    return states_summary, counties_summary, cities_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8eee2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mentions_in_tweets_temporal(records, time_bins, state_lookup, county_lookup, city_lookup):\n",
    "    temporal_state_mentions = {bin_time: defaultdict(int) for bin_time in time_bins}\n",
    "    temporal_county_mentions = {bin_time: defaultdict(int) for bin_time in time_bins}\n",
    "    temporal_city_mentions = {bin_time: defaultdict(int) for bin_time in time_bins}\n",
    "\n",
    "    temporal_state_details = {bin_time: defaultdict(list) for bin_time in time_bins}\n",
    "    temporal_county_details = {bin_time: defaultdict(list) for bin_time in time_bins}\n",
    "    temporal_city_details = {bin_time: defaultdict(list) for bin_time in time_bins}\n",
    "\n",
    "    for record in records:\n",
    "        bin_time = record.get('time_bin')\n",
    "        if bin_time is None:\n",
    "            continue\n",
    "        for entity in record['entities']:\n",
    "            state_match, _ = fuzzy_match_entity(entity, state_lookup, threshold=0.9)\n",
    "            if state_match:\n",
    "                state_code = state_match['STUSPS']\n",
    "                temporal_state_mentions[bin_time][state_code] += 1\n",
    "                temporal_state_details[bin_time][state_code].append({\n",
    "                    'matched_entity': entity,\n",
    "                    'original_gpe': record['original_gpe'],\n",
    "                    'time': record['time_string']\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            county_match, _ = fuzzy_match_entity(entity, county_lookup, threshold=0.85)\n",
    "            if county_match:\n",
    "                county_id = county_match['GEOID']\n",
    "                temporal_county_mentions[bin_time][county_id] += 1\n",
    "                temporal_county_details[bin_time][county_id].append({\n",
    "                    'matched_entity': entity,\n",
    "                    'original_gpe': record['original_gpe'],\n",
    "                    'time': record['time_string']\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            city_match, _ = fuzzy_match_entity(entity, city_lookup, threshold=0.85)\n",
    "            if city_match:\n",
    "                city_id = city_match['geonameid']\n",
    "                temporal_city_mentions[bin_time][city_id] += 1\n",
    "                temporal_city_details[bin_time][city_id].append({\n",
    "                    'matched_entity': entity,\n",
    "                    'original_gpe': record['original_gpe'],\n",
    "                    'time': record['time_string']\n",
    "                })\n",
    "\n",
    "    return (\n",
    "        temporal_state_mentions,\n",
    "        temporal_county_mentions,\n",
    "        temporal_city_mentions,\n",
    "        temporal_state_details,\n",
    "        temporal_county_details,\n",
    "        temporal_city_details\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c15d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_aggregations(time_bins, temporal_state_mentions, temporal_county_mentions,\n",
    "                                 temporal_city_mentions, temporal_state_details, temporal_county_details,\n",
    "                                 temporal_city_details):\n",
    "    temporal_data = OrderedDict()\n",
    "    for bin_time in time_bins:\n",
    "        states = []\n",
    "        for state_code, count in temporal_state_mentions[bin_time].items():\n",
    "            details = temporal_state_details[bin_time][state_code]\n",
    "            states.append({\n",
    "                'state_code': state_code,\n",
    "                'tweet_count': count,\n",
    "                'sample_gpe_text': ' | '.join(detail['original_gpe'][:100] for detail in details[:3])\n",
    "            })\n",
    "\n",
    "        counties = []\n",
    "        for county_id, count in temporal_county_mentions[bin_time].items():\n",
    "            details = temporal_county_details[bin_time][county_id]\n",
    "            counties.append({\n",
    "                'county_fips': county_id,\n",
    "                'tweet_count': count,\n",
    "                'sample_gpe_text': ' | '.join(detail['original_gpe'][:100] for detail in details[:3])\n",
    "            })\n",
    "\n",
    "        cities = []\n",
    "        for city_id, count in temporal_city_mentions[bin_time].items():\n",
    "            details = temporal_city_details[bin_time][city_id]\n",
    "            cities.append({\n",
    "                'city_id': city_id,\n",
    "                'tweet_count': count,\n",
    "                'original_gpe_text': ' | '.join(detail['original_gpe'] for detail in details),\n",
    "                'matched_entities': '; '.join(detail['matched_entity'] for detail in details)\n",
    "            })\n",
    "\n",
    "        temporal_data[bin_time] = {\n",
    "            'states': states,\n",
    "            'counties': counties,\n",
    "            'cities': cities\n",
    "        }\n",
    "    return temporal_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96310368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_where_clause(dataset, field_name, values):\n",
    "    if not values:\n",
    "        return None\n",
    "    field = arcpy.AddFieldDelimiters(dataset, field_name)\n",
    "    cleaned = []\n",
    "    is_numeric = all(isinstance(v, (int, float)) for v in values)\n",
    "    for value in values:\n",
    "        if value in (None, ''):\n",
    "            continue\n",
    "        if is_numeric:\n",
    "            cleaned.append(str(value))\n",
    "        else:\n",
    "            cleaned.append(\"'{}'\".format(str(value).replace(\"'\", \"''\")))\n",
    "\n",
    "    if not cleaned:\n",
    "        return None\n",
    "    return f\"{field} IN ({', '.join(cleaned)})\"\n",
    "\n",
    "\n",
    "def copy_and_enrich_features(base_fc, id_field, items, output_path, field_definitions, constant_values=None):\n",
    "    if not items:\n",
    "        return None\n",
    "    constant_values = constant_values or {}\n",
    "    id_values = [item['id'] for item in items if item.get('id') not in (None, '')]\n",
    "    where_clause = build_where_clause(base_fc, id_field, id_values)\n",
    "    if not where_clause:\n",
    "        return None\n",
    "\n",
    "    layer_name = f\"lyr_{uuid.uuid4().hex[:8]}\"\n",
    "    arcpy.management.MakeFeatureLayer(base_fc, layer_name, where_clause)\n",
    "    arcpy.management.CopyFeatures(layer_name, output_path)\n",
    "    arcpy.management.Delete(layer_name)\n",
    "\n",
    "    existing_fields = {field.name for field in arcpy.ListFields(output_path)}\n",
    "    for field_name, field_type, params in field_definitions:\n",
    "        if field_name not in existing_fields:\n",
    "            if params:\n",
    "                arcpy.management.AddField(output_path, field_name, field_type, **params)\n",
    "            else:\n",
    "                arcpy.management.AddField(output_path, field_name, field_type)\n",
    "\n",
    "    value_lookup = {item['id']: item for item in items}\n",
    "    update_fields = [id_field] + [field_name for field_name, _, _ in field_definitions]\n",
    "    with arcpy.da.UpdateCursor(output_path, update_fields) as cursor:\n",
    "        for row in cursor:\n",
    "            key = row[0]\n",
    "            data = value_lookup.get(key)\n",
    "            if not data:\n",
    "                continue\n",
    "            for idx, field_name in enumerate(update_fields[1:], start=1):\n",
    "                if field_name in constant_values:\n",
    "                    row[idx] = constant_values[field_name]\n",
    "                    continue\n",
    "                if field_name in data:\n",
    "                    value = data[field_name]\n",
    "                    if isinstance(value, str):\n",
    "                        params = next((p for f, _, p in field_definitions if f == field_name), {})\n",
    "                        max_length = params.get('field_length')\n",
    "                        if max_length:\n",
    "                            row[idx] = value[:max_length]\n",
    "                        else:\n",
    "                            row[idx] = value\n",
    "                    else:\n",
    "                        row[idx] = value\n",
    "            cursor.updateRow(row)\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def merge_feature_classes(inputs, output_path):\n",
    "    if not inputs:\n",
    "        return None\n",
    "    if len(inputs) == 1:\n",
    "        arcpy.management.CopyFeatures(inputs[0], output_path)\n",
    "    else:\n",
    "        arcpy.management.Merge(inputs, output_path)\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def write_city_csv(path, rows, fieldnames):\n",
    "    if not rows:\n",
    "        return None\n",
    "    with open(path, 'w', newline='', encoding='utf-8') as handle:\n",
    "        writer = csv.DictWriter(handle, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b875adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_temporal_to_arcgis(temporal_data, time_bins, states_fc, counties_fc, cities_fc,\n",
    "                               city_lookup, output_dir=PROJECT_OUTPUT_FOLDER):\n",
    "    temporal_dir = ensure_directory(get_data_file_path(output_dir, 'temporal_4hour_bins'))\n",
    "    incremental_dir = ensure_directory(os.path.join(temporal_dir, 'incremental'))\n",
    "    cumulative_dir = ensure_directory(os.path.join(temporal_dir, 'cumulative'))\n",
    "\n",
    "    state_id_field = find_field_case_insensitive(states_fc, 'STUSPS')\n",
    "    county_id_field = find_field_case_insensitive(counties_fc, 'GEOID')\n",
    "    city_id_field = find_field_case_insensitive(cities_fc, 'geonameid')\n",
    "\n",
    "    states_incremental_fields = [\n",
    "        ('tweet_cnt', 'LONG', {}),\n",
    "        ('smpl_gpe', 'TEXT', {'field_length': 254}),\n",
    "        ('time_bin', 'TEXT', {'field_length': 32}),\n",
    "        ('bin_start', 'DATE', {}),\n",
    "        ('cnt_type', 'TEXT', {'field_length': 12})\n",
    "    ]\n",
    "\n",
    "    states_cumulative_fields = [\n",
    "        ('cumul_cnt', 'LONG', {}),\n",
    "        ('time_bin', 'TEXT', {'field_length': 32}),\n",
    "        ('bin_start', 'DATE', {}),\n",
    "        ('cnt_type', 'TEXT', {'field_length': 12})\n",
    "    ]\n",
    "\n",
    "    counties_incremental_fields = [\n",
    "        ('tweet_cnt', 'LONG', {}),\n",
    "        ('smpl_gpe', 'TEXT', {'field_length': 254}),\n",
    "        ('time_bin', 'TEXT', {'field_length': 32}),\n",
    "        ('bin_start', 'DATE', {}),\n",
    "        ('cnt_type', 'TEXT', {'field_length': 12})\n",
    "    ]\n",
    "\n",
    "    counties_cumulative_fields = [\n",
    "        ('cumul_cnt', 'LONG', {}),\n",
    "        ('time_bin', 'TEXT', {'field_length': 32}),\n",
    "        ('bin_start', 'DATE', {}),\n",
    "        ('cnt_type', 'TEXT', {'field_length': 12})\n",
    "    ]\n",
    "\n",
    "    cities_incremental_fields = [\n",
    "        ('tweet_cnt', 'LONG', {}),\n",
    "        ('mtchd_ent', 'TEXT', {'field_length': 254}),\n",
    "        ('orig_gpe', 'TEXT', {'field_length': 254}),\n",
    "        ('time_bin', 'TEXT', {'field_length': 32}),\n",
    "        ('bin_start', 'DATE', {}),\n",
    "        ('cnt_type', 'TEXT', {'field_length': 12})\n",
    "    ]\n",
    "\n",
    "    cities_cumulative_fields = [\n",
    "        ('cumul_cnt', 'LONG', {}),\n",
    "        ('time_bin', 'TEXT', {'field_length': 32}),\n",
    "        ('bin_start', 'DATE', {}),\n",
    "        ('cnt_type', 'TEXT', {'field_length': 12})\n",
    "    ]\n",
    "\n",
    "    all_states_incremental = []\n",
    "    all_states_cumulative = []\n",
    "    all_counties_incremental = []\n",
    "    all_counties_cumulative = []\n",
    "    all_cities_incremental = []\n",
    "    all_cities_cumulative = []\n",
    "\n",
    "    city_incremental_rows = []\n",
    "    city_cumulative_rows = []\n",
    "\n",
    "    cumulative_state_counts = OrderedDict()\n",
    "    cumulative_county_counts = OrderedDict()\n",
    "    cumulative_city_counts = OrderedDict()\n",
    "\n",
    "    all_mentioned_states = set()\n",
    "    all_mentioned_counties = set()\n",
    "    all_mentioned_cities = set()\n",
    "\n",
    "    for counts in temporal_data.values():\n",
    "        all_mentioned_states.update(entry['state_code'] for entry in counts['states'])\n",
    "        all_mentioned_counties.update(entry['county_fips'] for entry in counts['counties'])\n",
    "        all_mentioned_cities.update(entry['city_id'] for entry in counts['cities'])\n",
    "\n",
    "    for index, bin_time in enumerate(time_bins, start=1):\n",
    "        bin_label = bin_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        bin_str = bin_time.strftime('%Y%m%d_%H%M')\n",
    "        counts = temporal_data[bin_time]\n",
    "\n",
    "        state_items = []\n",
    "        for entry in counts['states']:\n",
    "            cumulative_state_counts[entry['state_code']] = cumulative_state_counts.get(entry['state_code'], 0) + entry['tweet_count']\n",
    "            state_items.append({\n",
    "                'id': entry['state_code'],\n",
    "                'tweet_cnt': entry['tweet_count'],\n",
    "                'smpl_gpe': entry['sample_gpe_text']\n",
    "            })\n",
    "        if state_items:\n",
    "            output_path = os.path.join(incremental_dir, f'states_inc_{bin_str}.shp')\n",
    "            copy_and_enrich_features(\n",
    "                states_fc,\n",
    "                state_id_field,\n",
    "                state_items,\n",
    "                output_path,\n",
    "                states_incremental_fields,\n",
    "                {'time_bin': bin_label, 'bin_start': bin_time, 'cnt_type': 'incremental'}\n",
    "            )\n",
    "            all_states_incremental.append(output_path)\n",
    "\n",
    "        cumulative_state_items = []\n",
    "        for state_code, total in cumulative_state_counts.items():\n",
    "            if state_code in all_mentioned_states:\n",
    "                cumulative_state_items.append({'id': state_code, 'cumul_cnt': total})\n",
    "        if cumulative_state_items:\n",
    "            output_path = os.path.join(cumulative_dir, f'states_cum_{bin_str}.shp')\n",
    "            copy_and_enrich_features(\n",
    "                states_fc,\n",
    "                state_id_field,\n",
    "                cumulative_state_items,\n",
    "                output_path,\n",
    "                states_cumulative_fields,\n",
    "                {'time_bin': bin_label, 'bin_start': bin_time, 'cnt_type': 'cumulative'}\n",
    "            )\n",
    "            all_states_cumulative.append(output_path)\n",
    "\n",
    "        county_items = []\n",
    "        for entry in counts['counties']:\n",
    "            cumulative_county_counts[entry['county_fips']] = cumulative_county_counts.get(entry['county_fips'], 0) + entry['tweet_count']\n",
    "            county_items.append({\n",
    "                'id': entry['county_fips'],\n",
    "                'tweet_cnt': entry['tweet_count'],\n",
    "                'smpl_gpe': entry['sample_gpe_text']\n",
    "            })\n",
    "        if county_items:\n",
    "            output_path = os.path.join(incremental_dir, f'counties_inc_{bin_str}.shp')\n",
    "            copy_and_enrich_features(\n",
    "                counties_fc,\n",
    "                county_id_field,\n",
    "                county_items,\n",
    "                output_path,\n",
    "                counties_incremental_fields,\n",
    "                {'time_bin': bin_label, 'bin_start': bin_time, 'cnt_type': 'incremental'}\n",
    "            )\n",
    "            all_counties_incremental.append(output_path)\n",
    "\n",
    "        cumulative_county_items = []\n",
    "        for county_id, total in cumulative_county_counts.items():\n",
    "            if county_id in all_mentioned_counties:\n",
    "                cumulative_county_items.append({'id': county_id, 'cumul_cnt': total})\n",
    "        if cumulative_county_items:\n",
    "            output_path = os.path.join(cumulative_dir, f'counties_cum_{bin_str}.shp')\n",
    "            copy_and_enrich_features(\n",
    "                counties_fc,\n",
    "                county_id_field,\n",
    "                cumulative_county_items,\n",
    "                output_path,\n",
    "                counties_cumulative_fields,\n",
    "                {'time_bin': bin_label, 'bin_start': bin_time, 'cnt_type': 'cumulative'}\n",
    "            )\n",
    "            all_counties_cumulative.append(output_path)\n",
    "\n",
    "        city_items = []\n",
    "        for entry in counts['cities']:\n",
    "            cumulative_city_counts[entry['city_id']] = cumulative_city_counts.get(entry['city_id'], 0) + entry['tweet_count']\n",
    "            city_items.append({\n",
    "                'id': entry['city_id'],\n",
    "                'tweet_cnt': entry['tweet_count'],\n",
    "                'mtchd_ent': entry['matched_entities'],\n",
    "                'orig_gpe': entry['original_gpe_text']\n",
    "            })\n",
    "        if city_items:\n",
    "            output_path = os.path.join(incremental_dir, f'cities_inc_{bin_str}.shp')\n",
    "            copy_and_enrich_features(\n",
    "                cities_fc,\n",
    "                city_id_field,\n",
    "                city_items,\n",
    "                output_path,\n",
    "                cities_incremental_fields,\n",
    "                {'time_bin': bin_label, 'bin_start': bin_time, 'cnt_type': 'incremental'}\n",
    "            )\n",
    "            all_cities_incremental.append(output_path)\n",
    "\n",
    "            for entry in counts['cities']:\n",
    "                city_info = None\n",
    "                for lookup_entry in city_lookup.values():\n",
    "                    if lookup_entry['geonameid'] == entry['city_id']:\n",
    "                        city_info = lookup_entry\n",
    "                        break\n",
    "                if city_info:\n",
    "                    city_incremental_rows.append({\n",
    "                        'city_name': city_info['name'],\n",
    "                        'city_id': entry['city_id'],\n",
    "                        'population': city_info['population'],\n",
    "                        'latitude': city_info['latitude'],\n",
    "                        'longitude': city_info['longitude'],\n",
    "                        'tweet_count': entry['tweet_count'],\n",
    "                        'matched_entities': entry['matched_entities'],\n",
    "                        'original_gpe_text': entry['original_gpe_text'],\n",
    "                        'time_bin': bin_label,\n",
    "                        'bin_start': bin_time.isoformat()\n",
    "                    })\n",
    "\n",
    "        cumulative_city_items = []\n",
    "        for city_id, total in cumulative_city_counts.items():\n",
    "            if city_id in all_mentioned_cities:\n",
    "                cumulative_city_items.append({'id': city_id, 'cumul_cnt': total})\n",
    "        if cumulative_city_items:\n",
    "            output_path = os.path.join(cumulative_dir, f'cities_cum_{bin_str}.shp')\n",
    "            copy_and_enrich_features(\n",
    "                cities_fc,\n",
    "                city_id_field,\n",
    "                cumulative_city_items,\n",
    "                output_path,\n",
    "                cities_cumulative_fields,\n",
    "                {'time_bin': bin_label, 'bin_start': bin_time, 'cnt_type': 'cumulative'}\n",
    "            )\n",
    "            all_cities_cumulative.append(output_path)\n",
    "\n",
    "    states_inc_master = merge_feature_classes(\n",
    "        all_states_incremental,\n",
    "        os.path.join(incremental_dir, 'states_INCREMENTAL_ALL.shp')\n",
    "    )\n",
    "    counties_inc_master = merge_feature_classes(\n",
    "        all_counties_incremental,\n",
    "        os.path.join(incremental_dir, 'counties_INCREMENTAL_ALL.shp')\n",
    "    )\n",
    "    cities_inc_master = merge_feature_classes(\n",
    "        all_cities_incremental,\n",
    "        os.path.join(incremental_dir, 'cities_INCREMENTAL_ALL.shp')\n",
    "    )\n",
    "\n",
    "    states_cum_master = merge_feature_classes(\n",
    "        all_states_cumulative,\n",
    "        os.path.join(cumulative_dir, 'states_CUMULATIVE_ALL.shp')\n",
    "    )\n",
    "    counties_cum_master = merge_feature_classes(\n",
    "        all_counties_cumulative,\n",
    "        os.path.join(cumulative_dir, 'counties_CUMULATIVE_ALL.shp')\n",
    "    )\n",
    "    cities_cum_master = merge_feature_classes(\n",
    "        all_cities_cumulative,\n",
    "        os.path.join(cumulative_dir, 'cities_CUMULATIVE_ALL.shp')\n",
    "    )\n",
    "\n",
    "    if cities_inc_master:\n",
    "        write_city_csv(\n",
    "            os.path.join(incremental_dir, 'cities_INCREMENTAL_ALL.csv'),\n",
    "            city_incremental_rows,\n",
    "            [\n",
    "                'city_name', 'city_id', 'population', 'latitude', 'longitude',\n",
    "                'tweet_count', 'matched_entities', 'original_gpe_text', 'time_bin', 'bin_start'\n",
    "            ]\n",
    "        )\n",
    "    if cities_cum_master:\n",
    "        for city_id, total in cumulative_city_counts.items():\n",
    "            if city_id in all_mentioned_cities:\n",
    "                city_info = None\n",
    "                for lookup_entry in city_lookup.values():\n",
    "                    if lookup_entry['geonameid'] == city_id:\n",
    "                        city_info = lookup_entry\n",
    "                        break\n",
    "                if city_info:\n",
    "                    city_cumulative_rows.append({\n",
    "                        'city_name': city_info['name'],\n",
    "                        'city_id': city_id,\n",
    "                        'population': city_info['population'],\n",
    "                        'latitude': city_info['latitude'],\n",
    "                        'longitude': city_info['longitude'],\n",
    "                        'cumulative_count': total\n",
    "                    })\n",
    "        write_city_csv(\n",
    "            os.path.join(cumulative_dir, 'cities_CUMULATIVE_ALL.csv'),\n",
    "            city_cumulative_rows,\n",
    "            ['city_name', 'city_id', 'population', 'latitude', 'longitude', 'cumulative_count']\n",
    "        )\n",
    "\n",
    "    if states_inc_master:\n",
    "        arcpy.conversion.FeaturesToJSON(states_inc_master, os.path.join(incremental_dir, 'states_INCREMENTAL_ALL.geojson'), geoJSON='GEOJSON')\n",
    "    if counties_inc_master:\n",
    "        arcpy.conversion.FeaturesToJSON(counties_inc_master, os.path.join(incremental_dir, 'counties_INCREMENTAL_ALL.geojson'), geoJSON='GEOJSON')\n",
    "    if cities_inc_master:\n",
    "        arcpy.conversion.FeaturesToJSON(cities_inc_master, os.path.join(incremental_dir, 'cities_INCREMENTAL_ALL.geojson'), geoJSON='GEOJSON')\n",
    "\n",
    "    if states_cum_master:\n",
    "        arcpy.conversion.FeaturesToJSON(states_cum_master, os.path.join(cumulative_dir, 'states_CUMULATIVE_ALL.geojson'), geoJSON='GEOJSON')\n",
    "    if counties_cum_master:\n",
    "        arcpy.conversion.FeaturesToJSON(counties_cum_master, os.path.join(cumulative_dir, 'counties_CUMULATIVE_ALL.geojson'), geoJSON='GEOJSON')\n",
    "    if cities_cum_master:\n",
    "        arcpy.conversion.FeaturesToJSON(cities_cum_master, os.path.join(cumulative_dir, 'cities_CUMULATIVE_ALL.geojson'), geoJSON='GEOJSON')\n",
    "\n",
    "    return {\n",
    "        'incremental': {\n",
    "            'states': all_states_incremental,\n",
    "            'counties': all_counties_incremental,\n",
    "            'cities': all_cities_incremental\n",
    "        },\n",
    "        'cumulative': {\n",
    "            'states': all_states_cumulative,\n",
    "            'counties': all_counties_cumulative,\n",
    "            'cities': all_cities_cumulative\n",
    "        },\n",
    "        'master': {\n",
    "            'states_incremental': states_inc_master,\n",
    "            'counties_incremental': counties_inc_master,\n",
    "            'cities_incremental': cities_inc_master,\n",
    "            'states_cumulative': states_cum_master,\n",
    "            'counties_cumulative': counties_cum_master,\n",
    "            'cities_cumulative': cities_cum_master\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb0b90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_entities(summary, label, count_field='tweet_count', name_field=None, top_n=10):\n",
    "    print(f\"Top {label} by mentions\")\n",
    "    print('-' * 40)\n",
    "    sorted_rows = sorted(summary, key=lambda row: row.get(count_field, 0), reverse=True)\n",
    "    for row in sorted_rows[:top_n]:\n",
    "        name = row.get(name_field) if name_field else row.get(label.lower() + '_code', '')\n",
    "        print(f\"{name}: {row.get(count_field, 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "533373f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "ename": "<class 'arcgisscripting.ExecuteError'>",
     "evalue": "Failed to execute. Parameters are not valid.\nERROR 000517: The coordinate system is not defined for the input dataset.\nFailed to execute (Project).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExecuteError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading datasets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m tweets_fc \u001b[38;5;241m=\u001b[39m load_tweet_feature_class()\n\u001b[0;32m      3\u001b[0m states_fc \u001b[38;5;241m=\u001b[39m load_states_feature_class()\n\u001b[0;32m      4\u001b[0m counties_fc \u001b[38;5;241m=\u001b[39m load_counties_feature_class()\n",
      "Cell \u001b[1;32mIn[41], line 16\u001b[0m, in \u001b[0;36mload_tweet_feature_class\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m result \u001b[38;5;241m=\u001b[39m arcpy\u001b[38;5;241m.\u001b[39mconversion\u001b[38;5;241m.\u001b[39mJSONToFeatures(geojson_path, intermediate_name)\n\u001b[0;32m     15\u001b[0m tweet_fc \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mgetOutput(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m arcpy\u001b[38;5;241m.\u001b[39mmanagement\u001b[38;5;241m.\u001b[39mProject(tweet_fc, projected_name, arcpy\u001b[38;5;241m.\u001b[39mSpatialReference(\u001b[38;5;241m4326\u001b[39m))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m projected_name\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\management.py:20777\u001b[0m, in \u001b[0;36mProject\u001b[1;34m(in_dataset, out_dataset, out_coor_system, transform_method, in_coor_system, preserve_shape, max_deviation, vertical)\u001b[0m\n\u001b[0;32m  20775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[0;32m  20776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m> 20777\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\management.py:20759\u001b[0m, in \u001b[0;36mProject\u001b[1;34m(in_dataset, out_dataset, out_coor_system, transform_method, in_coor_system, preserve_shape, max_deviation, vertical)\u001b[0m\n\u001b[0;32m  20755\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01marcpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marcobjects\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marcobjectconversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convertArcObjectToPythonObject\n\u001b[0;32m  20757\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m  20758\u001b[0m     retval \u001b[38;5;241m=\u001b[39m convertArcObjectToPythonObject(\n\u001b[1;32m> 20759\u001b[0m         gp\u001b[38;5;241m.\u001b[39mProject_management(\n\u001b[0;32m  20760\u001b[0m             \u001b[38;5;241m*\u001b[39mgp_fixargs(\n\u001b[0;32m  20761\u001b[0m                 (\n\u001b[0;32m  20762\u001b[0m                     in_dataset,\n\u001b[0;32m  20763\u001b[0m                     out_dataset,\n\u001b[0;32m  20764\u001b[0m                     out_coor_system,\n\u001b[0;32m  20765\u001b[0m                     transform_method,\n\u001b[0;32m  20766\u001b[0m                     in_coor_system,\n\u001b[0;32m  20767\u001b[0m                     preserve_shape,\n\u001b[0;32m  20768\u001b[0m                     max_deviation,\n\u001b[0;32m  20769\u001b[0m                     vertical,\n\u001b[0;32m  20770\u001b[0m                 ),\n\u001b[0;32m  20771\u001b[0m                 \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m  20772\u001b[0m             )\n\u001b[0;32m  20773\u001b[0m         )\n\u001b[0;32m  20774\u001b[0m     )\n\u001b[0;32m  20775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n\u001b[0;32m  20776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\_base.py:533\u001b[0m, in \u001b[0;36mGeoprocessor.__getattr__.<locals>.<lambda>\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    531\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gp, attr)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(val):\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs: val(\u001b[38;5;241m*\u001b[39mgp_fixargs(args, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convertArcObjectToPythonObject(val)\n",
      "\u001b[1;31mExecuteError\u001b[0m: Failed to execute. Parameters are not valid.\nERROR 000517: The coordinate system is not defined for the input dataset.\nFailed to execute (Project).\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "tweets_fc = load_tweet_feature_class()\n",
    "states_fc = load_states_feature_class()\n",
    "counties_fc = load_counties_feature_class()\n",
    "cities_fc = load_cities_feature_class()\n",
    "\n",
    "print(\"Creating lookup dictionaries...\")\n",
    "state_lookup, county_lookup, city_lookup = create_lookup_dictionaries(states_fc, counties_fc, cities_fc)\n",
    "\n",
    "print(\"Reading tweet records...\")\n",
    "tweet_records = load_tweet_records(tweets_fc)\n",
    "print(f\"Tweets loaded: {len(tweet_records)}\")\n",
    "\n",
    "print(\"Assigning time bins...\")\n",
    "time_bins = assign_time_bins(tweet_records)\n",
    "print(f\"Time bins: {len(time_bins)}\")\n",
    "\n",
    "print(\"Counting overall mentions...\")\n",
    "(state_mentions, county_mentions, city_mentions,\n",
    " state_details, county_details, city_details) = count_mentions_in_tweets(\n",
    "    tweet_records, state_lookup, county_lookup, city_lookup\n",
    ")\n",
    "\n",
    "(states_summary, counties_summary, cities_summary) = create_overall_summaries(\n",
    "    state_mentions, county_mentions, city_mentions,\n",
    "    state_details, county_details, city_details,\n",
    "    city_lookup\n",
    ")\n",
    "\n",
    "print_top_entities(states_summary, 'States', count_field='tweet_count', name_field='state_code')\n",
    "print_top_entities(counties_summary, 'Counties', count_field='tweet_count', name_field='county_fips')\n",
    "print_top_entities(cities_summary, 'Cities', count_field='tweet_count', name_field='city_id')\n",
    "\n",
    "print(\"Counting temporal mentions...\")\n",
    "(\n",
    "    temporal_state_mentions,\n",
    "    temporal_county_mentions,\n",
    "    temporal_city_mentions,\n",
    "    temporal_state_details,\n",
    "    temporal_county_details,\n",
    "    temporal_city_details\n",
    ") = count_mentions_in_tweets_temporal(\n",
    "    tweet_records,\n",
    "    time_bins,\n",
    "    state_lookup,\n",
    "    county_lookup,\n",
    "    city_lookup\n",
    ")\n",
    "\n",
    "print(\"Creating temporal aggregations...\")\n",
    "temporal_data = create_temporal_aggregations(\n",
    "    time_bins,\n",
    "    temporal_state_mentions,\n",
    "    temporal_county_mentions,\n",
    "    temporal_city_mentions,\n",
    "    temporal_state_details,\n",
    "    temporal_county_details,\n",
    "    temporal_city_details\n",
    ")\n",
    "\n",
    "print(\"Exporting temporal outputs for ArcGIS Pro...\")\n",
    "export_results = export_temporal_to_arcgis(\n",
    "    temporal_data,\n",
    "    time_bins,\n",
    "    states_fc,\n",
    "    counties_fc,\n",
    "    cities_fc,\n",
    "    city_lookup,\n",
    "    output_dir=PROJECT_OUTPUT_FOLDER\n",
    ")\n",
    "\n",
    "print(\"Export complete. Output paths summary:\")\n",
    "print(json.dumps(export_results, indent=2, default=str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f768a-bdbb-49a7-9f56-e1a6778d6530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}