{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Synthetic GeoJSON Fusion Dataset Generator\n",
        "\n",
        "This notebook explores the provided GeoJSON assets and offers tooling to synthesize large tweet-like records that blend hurricane situational data with U.S. city metadata. Adjust the configuration cells to produce datasets as large as you need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow\n",
        "\n",
        "1. Inspect the hurricane and city GeoJSON inputs to understand their schema.\n",
        "2. Define a reusable generator that fuses hurricane points with city metadata and text templates.\n",
        "3. Tune the dataset size and export options to create massive synthetic samples on demand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from collections import Counter\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import math\n",
        "from typing import Dict, Any, Iterator, Optional\n",
        "\n",
        "BASE_PATH = Path('..') / 'data' / 'geojson'\n",
        "if not BASE_PATH.exists():\n",
        "    raise FileNotFoundError(f'Expected GeoJSON directory at {BASE_PATH!s}')\n",
        "BASE_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect hurricane GeoJSON files\n",
        "\n",
        "We gather headline statistics about the storm feature collections to understand how many facilities, place mentions, and timestamps they provide for sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "hurricane_files = ['francine.geojson', 'helene.geojson']\n",
        "for name in hurricane_files:\n",
        "    data = json.loads((BASE_PATH / name).read_text())\n",
        "    features = data.get('features', [])\n",
        "    facilities = Counter()\n",
        "    places = Counter()\n",
        "    times = []\n",
        "    latitudes = []\n",
        "    longitudes = []\n",
        "\n",
        "    for feature in features:\n",
        "        props = feature.get('properties', {})\n",
        "        fac = props.get('FAC')\n",
        "        place = props.get('GPE')\n",
        "        timestamp = props.get('time')\n",
        "        lat = props.get('Latitude')\n",
        "        lon = props.get('Longitude')\n",
        "\n",
        "        if fac:\n",
        "            facilities[fac] += 1\n",
        "        if place:\n",
        "            places[place] += 1\n",
        "        if timestamp:\n",
        "            try:\n",
        "                times.append(datetime.fromisoformat(timestamp.replace('Z', '+00:00')))\n",
        "            except ValueError:\n",
        "                pass\n",
        "        if lat not in (None, ''):\n",
        "            latitudes.append(float(lat))\n",
        "        if lon not in (None, ''):\n",
        "            longitudes.append(float(lon))\n",
        "\n",
        "    print(f\"{name}: {len(features)} features\")\n",
        "    print(f\"  unique facilities: {len(facilities)}\")\n",
        "    print(f\"  unique place mentions: {len(places)}\")\n",
        "    if times:\n",
        "        print(f\"  time span: {min(times).isoformat()} -> {max(times).isoformat()}\")\n",
        "    if latitudes and longitudes:\n",
        "        print(f\"  latitude range: {min(latitudes):.3f} -> {max(latitudes):.3f}\")\n",
        "        print(f\"  longitude range: {min(longitudes):.3f} -> {max(longitudes):.3f}\")\n",
        "    print(f\"  top facilities: {facilities.most_common(5)}\n",
        "\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect U.S. city GeoJSON file\n",
        "\n",
        "Population distributions and timezone coverage help determine how rich the downstream synthetic dataset can be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "city_data = json.loads((BASE_PATH / 'us_cities.geojson').read_text())\n",
        "city_features = city_data.get('features', [])\n",
        "print(f\"us_cities.geojson: {len(city_features)} records\")\n",
        "populations = []\n",
        "timezones = Counter()\n",
        "for feature in city_features:\n",
        "    props = feature.get('properties', {})\n",
        "    population = props.get('population')\n",
        "    timezone = props.get('timezone')\n",
        "    if population not in (None, ''):\n",
        "        populations.append(int(population))\n",
        "    if timezone:\n",
        "        timezones[timezone] += 1\n",
        "\n",
        "print(f\"  population min/max: {min(populations)} -> {max(populations)}\")\n",
        "print(f\"  population mean: {sum(populations)/len(populations):,.1f}\")\n",
        "sorted_pops = sorted(populations)\n",
        "print(f\"  population median: {sorted_pops[len(sorted_pops)//2]:,}\")\n",
        "print(f\"  sample timezones: {timezones.most_common(5)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Synthetic dataset generator\n",
        "\n",
        "The generator blends hurricane observations with randomly paired U.S. cities, perturbs geographic coordinates, and composes narrative strings. It exposes helpers for bulk iteration and CSV export so you can scale to millions of rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def _haversine(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n",
        "    radius_km = 6371.0\n",
        "    phi1 = math.radians(lat1)\n",
        "    phi2 = math.radians(lat2)\n",
        "    delta_phi = math.radians(lat2 - lat1)\n",
        "    delta_lambda = math.radians(lon2 - lon1)\n",
        "    a = (math.sin(delta_phi / 2) ** 2\n",
        "         + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2) ** 2)\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "    return radius_km * c\n",
        "\n",
        "\n",
        "class SyntheticTweetDatasetGenerator:\n",
        "    '''Fuse hurricane GeoJSON features with city metadata to create synthetic samples.'''\n",
        "\n",
        "    def __init__(self, base_path: Path):\n",
        "        self.base_path = Path(base_path)\n",
        "        self.events = self._load_events()\n",
        "        self.cities = self._load_cities()\n",
        "        self.event_pool = [\n",
        "            (event_name, feature)\n",
        "            for event_name, features in self.events.items()\n",
        "            for feature in features\n",
        "        ]\n",
        "        if not self.event_pool:\n",
        "            raise ValueError('No event data loaded from hurricane GeoJSON files.')\n",
        "        if not self.cities:\n",
        "            raise ValueError('No city data loaded from the us_cities GeoJSON file.')\n",
        "\n",
        "    def _load_events(self) -> Dict[str, list]:\n",
        "        events: Dict[str, list] = {}\n",
        "        for filename in ['francine.geojson', 'helene.geojson']:\n",
        "            data = json.loads((self.base_path / filename).read_text())\n",
        "            features = []\n",
        "            for feature in data.get('features', []):\n",
        "                props = feature.get('properties', {})\n",
        "                geometry = feature.get('geometry', {})\n",
        "                timestamp = props.get('time')\n",
        "                parsed_time = None\n",
        "                if timestamp:\n",
        "                    cleaned = timestamp.replace('Z', '+00:00')\n",
        "                    try:\n",
        "                        parsed_time = datetime.fromisoformat(cleaned)\n",
        "                    except ValueError:\n",
        "                        parsed_time = None\n",
        "                latitude = props.get('Latitude')\n",
        "                longitude = props.get('Longitude')\n",
        "                features.append({\n",
        "                    'facility': props.get('FAC'),\n",
        "                    'place': props.get('GPE'),\n",
        "                    'latitude': float(latitude) if latitude not in (None, '') else None,\n",
        "                    'longitude': float(longitude) if longitude not in (None, '') else None,\n",
        "                    'time': parsed_time,\n",
        "                    'geometry_type': geometry.get('type'),\n",
        "                })\n",
        "            events[filename.split('.')[0]] = features\n",
        "        return events\n",
        "\n",
        "    def _load_cities(self) -> list:\n",
        "        data = json.loads((self.base_path / 'us_cities.geojson').read_text())\n",
        "        cities = []\n",
        "        for feature in data.get('features', []):\n",
        "            props = feature.get('properties', {})\n",
        "            cities.append({\n",
        "                'geonameid': props.get('geonameid'),\n",
        "                'name': props.get('name'),\n",
        "                'latitude': float(props.get('latitude')),\n",
        "                'longitude': float(props.get('longitude')),\n",
        "                'population': (int(props.get('population'))\n",
        "                               if props.get('population') not in (None, '') else None),\n",
        "                'timezone': props.get('timezone'),\n",
        "                'pop_category': props.get('pop_category'),\n",
        "            })\n",
        "        return cities\n",
        "\n",
        "    def describe_sources(self) -> Dict[str, Any]:\n",
        "        summary = {'events': {}, 'cities': {}}\n",
        "        for event_name, features in self.events.items():\n",
        "            facilities = {f['facility'] for f in features if f['facility']}\n",
        "            places = {f['place'] for f in features if f['place']}\n",
        "            times = [f['time'] for f in features if f['time']]\n",
        "            summary['events'][event_name] = {\n",
        "                'records': len(features),\n",
        "                'unique_facilities': len(facilities),\n",
        "                'unique_places': len(places),\n",
        "                'time_range': (\n",
        "                    (min(times).isoformat(), max(times).isoformat()) if times else None\n",
        "                ),\n",
        "            }\n",
        "        summary['cities'] = {\n",
        "            'records': len(self.cities),\n",
        "            'population_min': min(\n",
        "                (c['population'] for c in self.cities if c['population'] is not None),\n",
        "                default=None,\n",
        "            ),\n",
        "            'population_max': max(\n",
        "                (c['population'] for c in self.cities if c['population'] is not None),\n",
        "                default=None,\n",
        "            ),\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "    def iter_synthetic_records(self, size: int, seed: Optional[int] = None) -> Iterator[Dict[str, Any]]:\n",
        "        rng = random.Random(seed)\n",
        "        narrative_templates = [\n",
        "            \"{event} update: {facility} in {place} is coordinating support with leaders in {city} (pop {population:,}).\",\n",
        "            \"Emergency crews from {facility} ({event}) are staging near {city}, {timezone} timezone, to assist {place} region.\",\n",
        "            \"{city} (population {population:,}) is receiving {event} briefings about {facility} operations near {place}.\",\n",
        "            \"Situation report: {facility} teams tied to {event} are aligning with {city} officials to cover {place}.\",\n",
        "        ]\n",
        "        for idx in range(1, size + 1):\n",
        "            event_name, event_feature = rng.choice(self.event_pool)\n",
        "            city = rng.choice(self.cities)\n",
        "            event_time = event_feature.get('time')\n",
        "            if event_time:\n",
        "                jitter = timedelta(minutes=rng.randint(-240, 240))\n",
        "                jittered_time = event_time + jitter\n",
        "            else:\n",
        "                jittered_time = None\n",
        "            base_lat = (\n",
        "                event_feature.get('latitude')\n",
        "                if event_feature.get('latitude') is not None\n",
        "                else city['latitude']\n",
        "            )\n",
        "            base_lon = (\n",
        "                event_feature.get('longitude')\n",
        "                if event_feature.get('longitude') is not None\n",
        "                else city['longitude']\n",
        "            )\n",
        "            lat_noise = rng.gauss(0, 0.18)\n",
        "            lon_noise = rng.gauss(0, 0.18)\n",
        "            synthetic_lat = max(-90, min(90, base_lat + lat_noise))\n",
        "            synthetic_lon = max(-180, min(180, base_lon + lon_noise))\n",
        "            distance_km = _haversine(synthetic_lat, synthetic_lon, city['latitude'], city['longitude'])\n",
        "            template = rng.choice(narrative_templates)\n",
        "            narrative = template.format(\n",
        "                event=event_name.title(),\n",
        "                facility=event_feature.get('facility') or 'local teams',\n",
        "                place=event_feature.get('place') or 'the impact area',\n",
        "                city=city['name'],\n",
        "                population=city['population'] or 0,\n",
        "                timezone=city['timezone'] or 'UTC',\n",
        "            )\n",
        "            yield {\n",
        "                'sample_id': idx,\n",
        "                'event': event_name,\n",
        "                'event_time': jittered_time.isoformat() if jittered_time else None,\n",
        "                'facility': event_feature.get('facility'),\n",
        "                'place_mention': event_feature.get('place'),\n",
        "                'geometry_type': event_feature.get('geometry_type'),\n",
        "                'synthetic_latitude': round(synthetic_lat, 6),\n",
        "                'synthetic_longitude': round(synthetic_lon, 6),\n",
        "                'reference_latitude': event_feature.get('latitude'),\n",
        "                'reference_longitude': event_feature.get('longitude'),\n",
        "                'city_name': city['name'],\n",
        "                'city_geonameid': city['geonameid'],\n",
        "                'city_population': city['population'],\n",
        "                'city_timezone': city['timezone'],\n",
        "                'city_pop_category': city['pop_category'],\n",
        "                'distance_to_city_km': round(distance_km, 2),\n",
        "                'urgency_score': round(rng.uniform(0.2, 0.98), 3),\n",
        "                'narrative': narrative,\n",
        "                'data_source': 'synthetic_fusion_v1',\n",
        "            }\n",
        "\n",
        "    def generate_dataset(self, size: int, seed: Optional[int] = None) -> list:\n",
        "        return list(self.iter_synthetic_records(size, seed=seed))\n",
        "\n",
        "    def write_csv(self, output_path: Path, size: int, seed: Optional[int] = None) -> Path:\n",
        "        import csv\n",
        "        iterator = self.iter_synthetic_records(size, seed=seed)\n",
        "        try:\n",
        "            first = next(iterator)\n",
        "        except StopIteration as exc:\n",
        "            raise ValueError('Requested dataset size must be positive.') from exc\n",
        "        fieldnames = list(first.keys())\n",
        "        output_path = Path(output_path)\n",
        "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with output_path.open('w', newline='', encoding='utf-8') as handle:\n",
        "            writer = csv.DictWriter(handle, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerow(first)\n",
        "            for record in iterator:\n",
        "                writer.writerow(record)\n",
        "        return output_path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "generator = SyntheticTweetDatasetGenerator(BASE_PATH)\n",
        "source_summary = generator.describe_sources()\n",
        "import pprint\n",
        "pprint.pprint(source_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure dataset size\n",
        "\n",
        "Adjust `SAMPLE_SIZE` (and optionally the random `SEED`) to drive how many synthetic records are produced. Increase the number into the millions to stress-test downstream analytics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "SAMPLE_SIZE = 5000  # Change this to scale the dataset size (e.g., 100_000 or 1_000_000)\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "synthetic_records = generator.generate_dataset(SAMPLE_SIZE, seed=SEED)\n",
        "len(synthetic_records)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "for record in synthetic_records[:5]:\n",
        "    print(json.dumps(record, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "event_counts = Counter(rec['event'] for rec in synthetic_records)\n",
        "facility_counts = Counter(rec['facility'] for rec in synthetic_records if rec['facility'])\n",
        "urgency_values = [rec['urgency_score'] for rec in synthetic_records]\n",
        "distance_values = [rec['distance_to_city_km'] for rec in synthetic_records]\n",
        "print('Event counts:', event_counts)\n",
        "print('Top facilities:', facility_counts.most_common(10))\n",
        "print(f\"Urgency range: {min(urgency_values):.3f} -> {max(urgency_values):.3f}\")\n",
        "print(f\"Distance range: {min(distance_values):.1f} -> {max(distance_values):.1f} km\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: export directly to CSV\n",
        "\n",
        "Run the next cell to stream records straight to disk without holding them all in memory. Adjust `CSV_SAMPLE_SIZE` to the volume you need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "OUTPUT_DIR = Path('..') / 'data' / 'generated_samples'\n",
        "CSV_SAMPLE_SIZE = 10000  # Tweak as needed for large exports\n",
        "csv_path = generator.write_csv(OUTPUT_DIR / f'synthetic_samples_{CSV_SAMPLE_SIZE}.csv',\n",
        "                                 size=CSV_SAMPLE_SIZE, seed=SEED)\n",
        "csv_path"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}