{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCGIS TWEET PROCESSOR - COMPLETE CONVERSION\n",
    "\n",
    "## Overview\n",
    "This notebook converts **ALL** functionality from `test.ipynb` to work entirely within an ArcGIS Pro/arcpy environment.\n",
    "\n",
    "### What This Notebook Does:\n",
    "1. Load tweets (GeoJSON), cities (CSV), states/counties (shapefiles)\n",
    "2. Parse GPE field (comma-separated) and match place names **EXACTLY** (case-insensitive)\n",
    "3. Count mentions by entity with **PRIORITY HIERARCHY**: State > County > City\n",
    "4. Spatial cascade from tweet point locations\n",
    "5. Create temporal bins (4-hour intervals)\n",
    "6. Export incremental and cumulative shapefiles\n",
    "\n",
    "### Matching Logic:\n",
    "- **Exact matching only** - no fuzzy matching\n",
    "- Matches state names (full or abbreviation), case-insensitive\n",
    "- **Priority hierarchy**: If a string matches a state, it cannot also match a county/city\n",
    "- Comma-separated strings are parsed individually\n",
    "\n",
    "### Requirements:\n",
    "- **ArcGIS Pro 3.x** with arcpy\n",
    "- **No external dependencies** - pure Python + arcpy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n",
      "ArcPy version: 3.5\n",
      "Working directory: C:\\WINDOWS\\system32\n",
      "\n",
      "Matching method: EXACT (case-insensitive)\n",
      "Priority hierarchy: State > County > City\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "IMPORTS AND ENVIRONMENT SETUP\n",
    "\"\"\"\n",
    "import arcpy\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set overwrite output\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "print(\"✓ Imports loaded\")\n",
    "print(f\"ArcPy version: {arcpy.GetInstallInfo()['Version']}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(\"\\nMatching method: EXACT (case-insensitive)\")\n",
    "print(\"Priority hierarchy: State > County > City\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Path Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Path management functions loaded (hardcoded project path).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PATH MANAGEMENT FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "def get_project_root():\n",
    "    \"\"\"Get project root directory\"\"\"\n",
    "    # Hardcode project path to avoid working directory issues\n",
    "    return r\"C:\\users\\colto\\documents\\github\\tweet_project\"\n",
    "\n",
    "\n",
    "def get_data_file_path(*path_segments):\n",
    "    \"\"\"Build path to data files from project root\"\"\"\n",
    "    project_root = get_project_root()\n",
    "    return os.path.join(project_root, *path_segments)\n",
    "\n",
    "\n",
    "print(\"✓ Path management functions loaded (hardcoded project path).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Data Loading Functions (ArcPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T20:32:14.003290Z",
     "start_time": "2025-11-04T20:32:13.991290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loading functions defined (GeoJSON from local, others from GDB).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DATA LOADING FUNCTIONS\n",
    "GeoJSON from local path, everything else from GDB\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "def load_tweets_geojson(workspace=\"in_memory\"):\n",
    "    \"\"\"\n",
    "    Load helene.geojson from LOCAL PATH by reading JSON and creating points\n",
    "    Returns: Feature class path\n",
    "    \"\"\"\n",
    "    print(\"Loading tweets from helene.geojson...\")\n",
    "    geojson_path = os.path.join(os.path.abspath(workspace), \"data\", \"geojson\", \"helene.geojson\")\n",
    "    if not os.path.exists(geojson_path):\n",
    "        raise FileNotFoundError(f\"GeoJSON file not found: {geojson_path}\")\n",
    "    print(f\"  Path: {geojson_path}\")\n",
    "\n",
    "    # Read GeoJSON file\n",
    "    with open(geojson_path, 'r') as f:\n",
    "        geojson_data = json.load(f)\n",
    "\n",
    "    # Create feature class\n",
    "    tweets_fc = os.path.join(workspace, \"tweets_helene\")\n",
    "\n",
    "    # Create empty point feature class\n",
    "    arcpy.management.CreateFeatureclass(\n",
    "        os.path.dirname(tweets_fc) if workspace != \"in_memory\" else workspace,\n",
    "        os.path.basename(tweets_fc),\n",
    "        \"POINT\",\n",
    "        spatial_reference=arcpy.SpatialReference(4326)\n",
    "    )\n",
    "\n",
    "    # Add fields for attributes\n",
    "    arcpy.management.AddField(tweets_fc, \"GPE\", \"TEXT\", field_length=500)\n",
    "    arcpy.management.AddField(tweets_fc, \"FAC\", \"TEXT\", field_length=500)\n",
    "    arcpy.management.AddField(tweets_fc, \"LOC\", \"TEXT\", field_length=500)\n",
    "    arcpy.management.AddField(tweets_fc, \"time\", \"TEXT\", field_length=50)\n",
    "    arcpy.management.AddField(tweets_fc, \"Latitude\", \"DOUBLE\")\n",
    "    arcpy.management.AddField(tweets_fc, \"Longitude\", \"DOUBLE\")\n",
    "\n",
    "    # Insert features\n",
    "    fields = ['SHAPE@XY', 'GPE', 'FAC', 'LOC', 'time', 'Latitude', 'Longitude']\n",
    "\n",
    "    with arcpy.da.InsertCursor(tweets_fc, fields) as cursor:\n",
    "        for feature in geojson_data['features']:\n",
    "            props = feature['properties']\n",
    "            coords = feature['geometry']['coordinates']\n",
    "\n",
    "            # GeoJSON is [lon, lat]\n",
    "            lon = coords[0]\n",
    "            lat = coords[1]\n",
    "\n",
    "            cursor.insertRow([\n",
    "                (lon, lat),  # SHAPE@XY\n",
    "                props.get('GPE', ''),\n",
    "                props.get('FAC', ''),\n",
    "                props.get('LOC', ''),\n",
    "                props.get('time', ''),\n",
    "                lat,\n",
    "                lon\n",
    "            ])\n",
    "\n",
    "    count = int(arcpy.management.GetCount(tweets_fc).getOutput(0))\n",
    "    print(f\"  ✓ Loaded {count} tweet features\")\n",
    "\n",
    "    return tweets_fc\n",
    "\n",
    "\n",
    "def load_cities_csv(workspace=\"in_memory\"):\n",
    "    \"\"\"Load cities from GDB feature class\"\"\"\n",
    "    print(\"Loading cities from GDB...\")\n",
    "    \n",
    "    cities_fc = os.path.join(workspace, \"us_cities\")\n",
    "    arcpy.management.CopyFeatures(\"cities1000\", cities_fc)\n",
    "    \n",
    "    count = int(arcpy.management.GetCount(cities_fc).getOutput(0))\n",
    "    print(f\"  ✓ Loaded {count} city features\")\n",
    "    \n",
    "    return cities_fc\n",
    "\n",
    "\n",
    "def load_states_shapefile(workspace=\"in_memory\"):\n",
    "    \"\"\"Load states from GDB feature class\"\"\"\n",
    "    print(\"Loading states from GDB...\")\n",
    "    \n",
    "    states_fc = os.path.join(workspace, \"us_states\")\n",
    "    arcpy.management.CopyFeatures(\"cb_2023_us_state_20m\", states_fc)\n",
    "    \n",
    "    count = int(arcpy.management.GetCount(states_fc).getOutput(0))\n",
    "    print(f\"  ✓ Loaded {count} state features\")\n",
    "    \n",
    "    return states_fc\n",
    "\n",
    "\n",
    "def load_counties_shapefile(workspace=\"in_memory\"):\n",
    "    \"\"\"Load counties from GDB feature class\"\"\"\n",
    "    print(\"Loading counties from GDB...\")\n",
    "    \n",
    "    counties_fc = os.path.join(workspace, \"us_counties\")\n",
    "    arcpy.management.CopyFeatures(\"cb_2023_us_county_20m\", counties_fc)\n",
    "    \n",
    "    count = int(arcpy.management.GetCount(counties_fc).getOutput(0))\n",
    "    print(f\"  ✓ Loaded {count} county features\")\n",
    "    \n",
    "    return counties_fc\n",
    "\n",
    "\n",
    "print(\"✓ Data loading functions defined (GeoJSON from local, others from GDB).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Place Name Preprocessing and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Place name preprocessing functions defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PLACE NAME PREPROCESSING AND PARSING\n",
    "Exact matching only - case insensitive\n",
    "\"\"\"\n",
    "\n",
    "def normalize_place_name(name):\n",
    "    \"\"\"\n",
    "    Normalize place names for EXACT matching (case-insensitive)\n",
    "    Simply uppercase and strip whitespace\n",
    "    \"\"\"\n",
    "    if not name or name == 'NAN' or name == '':\n",
    "        return None\n",
    "    \n",
    "    name = str(name).upper().strip()\n",
    "    \n",
    "    return name if name else None\n",
    "\n",
    "\n",
    "def parse_gpe_entities(gpe_string):\n",
    "    \"\"\"\n",
    "    Split GPE field by commas into individual place mentions\n",
    "    Returns list of normalized place names\n",
    "    \"\"\"\n",
    "    if not gpe_string or str(gpe_string).strip() == '':\n",
    "        return []\n",
    "    \n",
    "    gpe_string = str(gpe_string).strip()\n",
    "    entities = []\n",
    "    \n",
    "    # Split by comma\n",
    "    for part in gpe_string.split(','):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        \n",
    "        normalized = normalize_place_name(part)\n",
    "        if normalized and len(normalized) > 0:\n",
    "            entities.append(normalized)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen, clean = set(), []\n",
    "    for e in entities:\n",
    "        if e not in seen:\n",
    "            clean.append(e)\n",
    "            seen.add(e)\n",
    "    \n",
    "    return clean\n",
    "\n",
    "\n",
    "print(\"✓ Place name preprocessing functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Create Lookup Dictionaries with Priority Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Lookup dictionary function defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CREATE LOOKUP DICTIONARIES FROM FEATURE CLASSES\n",
    "Priority: State > County > City\n",
    "\"\"\"\n",
    "\n",
    "def create_lookup_dictionaries(states_fc, counties_fc, cities_fc):\n",
    "    \"\"\"\n",
    "    Build name->attributes lookup dictionaries from feature classes\n",
    "    \n",
    "    States: Indexed by full name (uppercase) AND abbreviation (uppercase)\n",
    "    Counties: Indexed by name (uppercase)\n",
    "    Cities: Indexed by name (uppercase)\n",
    "    \n",
    "    Returns: state_lookup, county_lookup, city_lookup\n",
    "    \"\"\"\n",
    "    print(\"Building lookup dictionaries...\")\n",
    "    \n",
    "    state_lookup = {}\n",
    "    county_lookup = {}\n",
    "    city_lookup = {}\n",
    "    \n",
    "    # Build states lookup (both full name and abbreviation)\n",
    "    print(\"  Building state lookup...\")\n",
    "    with arcpy.da.SearchCursor(states_fc, ['NAME', 'STUSPS', 'STATEFP', 'SHAPE@']) as cursor:\n",
    "        for row in cursor:\n",
    "            state_name = normalize_place_name(row[0])\n",
    "            state_abbrev = normalize_place_name(row[1])\n",
    "            \n",
    "            state_data = {\n",
    "                'NAME': row[0],\n",
    "                'STUSPS': row[1],\n",
    "                'STATEFP': row[2],\n",
    "                'geometry': row[3]\n",
    "            }\n",
    "            \n",
    "            # Index by full name\n",
    "            if state_name:\n",
    "                state_lookup[state_name] = state_data\n",
    "            \n",
    "            # Index by abbreviation\n",
    "            if state_abbrev:\n",
    "                state_lookup[state_abbrev] = state_data\n",
    "    \n",
    "    # Build counties lookup\n",
    "    print(\"  Building county lookup...\")\n",
    "    with arcpy.da.SearchCursor(counties_fc, ['NAME', 'GEOID', 'STATEFP', 'SHAPE@']) as cursor:\n",
    "        for row in cursor:\n",
    "            county_name = normalize_place_name(row[0])\n",
    "            if county_name:\n",
    "                county_lookup[county_name] = {\n",
    "                    'NAME': row[0],\n",
    "                    'GEOID': row[1],\n",
    "                    'STATEFP': row[2],\n",
    "                    'geometry': row[3]\n",
    "                }\n",
    "    \n",
    "    # Build cities lookup\n",
    "    print(\"  Building city lookup...\")\n",
    "    with arcpy.da.SearchCursor(cities_fc, ['name', 'geonameid', 'population', 'SHAPE@']) as cursor:\n",
    "        for row in cursor:\n",
    "            city_name = normalize_place_name(row[0])\n",
    "            if city_name:\n",
    "                city_lookup[city_name] = {\n",
    "                    'name': row[0],\n",
    "                    'geonameid': row[1],\n",
    "                    'population': row[2],\n",
    "                    'geometry': row[3]\n",
    "                }\n",
    "    \n",
    "    print(f\"  ✓ States: {len(state_lookup)} entries (names + abbreviations)\")\n",
    "    print(f\"  ✓ Counties: {len(county_lookup)} entries\")\n",
    "    print(f\"  ✓ Cities: {len(city_lookup)} entries\")\n",
    "    \n",
    "    return state_lookup, county_lookup, city_lookup\n",
    "\n",
    "\n",
    "print(\"✓ Lookup dictionary function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Exact Matching with Priority Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Priority matching function defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXACT MATCHING WITH PRIORITY HIERARCHY\n",
    "Priority: State > County > City\n",
    "\"\"\"\n",
    "\n",
    "def match_entity_with_priority(entity, state_lookup, county_lookup, city_lookup):\n",
    "    \"\"\"\n",
    "    Match entity to geographic level with priority hierarchy.\n",
    "    \n",
    "    Priority:\n",
    "    1. State (full name or abbreviation)\n",
    "    2. County (only if NOT a state)\n",
    "    3. City (only if NOT a state or county)\n",
    "    \n",
    "    Returns: (match_data, match_type) where match_type is 'state', 'county', 'city', or None\n",
    "    \"\"\"\n",
    "    # Priority 1: Check if it's a state\n",
    "    if entity in state_lookup:\n",
    "        return state_lookup[entity], 'state'\n",
    "    \n",
    "    # Priority 2: Check if it's a county (only if not a state)\n",
    "    if entity in county_lookup:\n",
    "        return county_lookup[entity], 'county'\n",
    "    \n",
    "    # Priority 3: Check if it's a city (only if not a state or county)\n",
    "    if entity in city_lookup:\n",
    "        return city_lookup[entity], 'city'\n",
    "    \n",
    "    # No match found\n",
    "    return None, None\n",
    "\n",
    "\n",
    "print(\"✓ Priority matching function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Count Mentions with Hierarchical Cascade and Temporal Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Counting function defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "COUNT MENTIONS WITH HIERARCHICAL CASCADE - TEMPORAL\n",
    "This is the core logic with priority hierarchy and spatial cascade\n",
    "\"\"\"\n",
    "\n",
    "def count_mentions_with_cascade_temporal(tweets_fc, state_lookup, county_lookup, city_lookup,\n",
    "                                         states_fc, counties_fc, cities_fc):\n",
    "    \"\"\"\n",
    "    Count mentions by time bin WITH hierarchical cascade\n",
    "    \n",
    "    TEXT-BASED MATCHING (from GPE field):\n",
    "    - Priority: State > County > City (exclusive)\n",
    "    - Exact match only (case-insensitive)\n",
    "    - Comma-separated parsing\n",
    "    \n",
    "    SPATIAL CASCADE (from tweet point):\n",
    "    - Each tweet point finds its containing county → +1 to county\n",
    "    - Each county cascades to its state → +1 to state\n",
    "    - Each tweet point finds nearest city (within 50km) → +1 to city\n",
    "    \n",
    "    Returns: time_bins, temporal_state_mentions, temporal_county_mentions, temporal_city_mentions,\n",
    "             + detail dictionaries\n",
    "    \"\"\"\n",
    "    print(\"\\nCounting tweet mentions by time bin WITH HIERARCHICAL CASCADE...\")\n",
    "    \n",
    "    # Get all tweets with their attributes\n",
    "    tweets_data = []\n",
    "    fields = ['SHAPE@', 'GPE', 'time']\n",
    "    \n",
    "    with arcpy.da.SearchCursor(tweets_fc, fields) as cursor:\n",
    "        for row in cursor:\n",
    "            tweets_data.append({\n",
    "                'geometry': row[0],\n",
    "                'GPE': row[1],\n",
    "                'time': row[2]\n",
    "            })\n",
    "    \n",
    "    print(f\"  Processing {len(tweets_data)} tweets...\")\n",
    "    \n",
    "    # Parse time and create 4-hour bins\n",
    "    for tweet in tweets_data:\n",
    "        time_str = str(tweet['time'])\n",
    "        # Handle datetime parsing\n",
    "        try:\n",
    "            dt = datetime.fromisoformat(time_str.replace('+00:00', ''))\n",
    "        except:\n",
    "            try:\n",
    "                dt = datetime.strptime(time_str.split('+')[0], '%Y-%m-%d %H:%M:%S')\n",
    "            except:\n",
    "                dt = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Floor to 4-hour bin\n",
    "        hour_bin = (dt.hour // 4) * 4\n",
    "        tweet['bin'] = dt.replace(hour=hour_bin, minute=0, second=0, microsecond=0)\n",
    "        tweet['time_dt'] = dt\n",
    "    \n",
    "    # Get unique time bins\n",
    "    time_bins = sorted(set([t['bin'] for t in tweets_data]))\n",
    "    print(f\"  Time bins: {len(time_bins)}\")\n",
    "    \n",
    "    # Initialize dictionaries for each time bin\n",
    "    temporal_state_mentions = {tb: {} for tb in time_bins}\n",
    "    temporal_county_mentions = {tb: {} for tb in time_bins}\n",
    "    temporal_city_mentions = {tb: {} for tb in time_bins}\n",
    "    \n",
    "    temporal_state_details = {tb: {} for tb in time_bins}\n",
    "    temporal_county_details = {tb: {} for tb in time_bins}\n",
    "    temporal_city_details = {tb: {} for tb in time_bins}\n",
    "    \n",
    "    # Create spatial index for counties\n",
    "    print(\"  Creating county spatial index...\")\n",
    "    county_geoms = {}\n",
    "    with arcpy.da.SearchCursor(counties_fc, ['GEOID', 'STATEFP', 'NAME', 'SHAPE@']) as cursor:\n",
    "        for row in cursor:\n",
    "            county_geoms[row[0]] = {\n",
    "                'geoid': row[0],\n",
    "                'statefp': row[1],\n",
    "                'name': row[2],\n",
    "                'geometry': row[3]\n",
    "            }\n",
    "    \n",
    "    # Create spatial index for states\n",
    "    print(\"  Creating state spatial index...\")\n",
    "    state_geoms = {}\n",
    "    with arcpy.da.SearchCursor(states_fc, ['STUSPS', 'NAME', 'STATEFP', 'SHAPE@']) as cursor:\n",
    "        for row in cursor:\n",
    "            state_geoms[row[2]] = {  # Key by STATEFP\n",
    "                'stusps': row[0],\n",
    "                'name': row[1],\n",
    "                'statefp': row[2],\n",
    "                'geometry': row[3]\n",
    "            }\n",
    "    \n",
    "    # Create spatial index for cities\n",
    "    print(\"  Creating city spatial index...\")\n",
    "    city_geoms = {}\n",
    "    with arcpy.da.SearchCursor(cities_fc, ['geonameid', 'name', 'SHAPE@']) as cursor:\n",
    "        for row in cursor:\n",
    "            city_geoms[row[0]] = {\n",
    "                'geonameid': row[0],\n",
    "                'name': row[1],\n",
    "                'geometry': row[2]\n",
    "            }\n",
    "    \n",
    "    # Process each tweet\n",
    "    print(\"  Processing tweets with priority matching...\")\n",
    "    for idx, tweet in enumerate(tweets_data):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"    Tweet {idx}/{len(tweets_data)}\")\n",
    "        \n",
    "        time_bin = tweet['bin']\n",
    "        entities = parse_gpe_entities(tweet['GPE'])\n",
    "        original_gpe = str(tweet['GPE']) if tweet['GPE'] else ''\n",
    "        tweet_time = str(tweet['time'])\n",
    "        tweet_point = tweet['geometry']\n",
    "        \n",
    "        # === PART 1: COUNT MENTIONS (text-based with PRIORITY) ===\n",
    "        for entity in entities:\n",
    "            match_data, match_type = match_entity_with_priority(entity, state_lookup, county_lookup, city_lookup)\n",
    "            \n",
    "            if match_type == 'state':\n",
    "                state_code = match_data['STUSPS']\n",
    "                temporal_state_mentions[time_bin][state_code] = temporal_state_mentions[time_bin].get(state_code, 0) + 1\n",
    "                \n",
    "                if state_code not in temporal_state_details[time_bin]:\n",
    "                    temporal_state_details[time_bin][state_code] = []\n",
    "                temporal_state_details[time_bin][state_code].append({\n",
    "                    'original_gpe': original_gpe,\n",
    "                    'matched_entity': entity,\n",
    "                    'time': tweet_time\n",
    "                })\n",
    "            \n",
    "            elif match_type == 'county':\n",
    "                county_id = match_data['GEOID']\n",
    "                temporal_county_mentions[time_bin][county_id] = temporal_county_mentions[time_bin].get(county_id, 0) + 1\n",
    "                \n",
    "                if county_id not in temporal_county_details[time_bin]:\n",
    "                    temporal_county_details[time_bin][county_id] = []\n",
    "                temporal_county_details[time_bin][county_id].append({\n",
    "                    'original_gpe': original_gpe,\n",
    "                    'matched_entity': entity,\n",
    "                    'time': tweet_time\n",
    "                })\n",
    "            \n",
    "            elif match_type == 'city':\n",
    "                city_id = match_data['geonameid']\n",
    "                temporal_city_mentions[time_bin][city_id] = temporal_city_mentions[time_bin].get(city_id, 0) + 1\n",
    "                \n",
    "                if city_id not in temporal_city_details[time_bin]:\n",
    "                    temporal_city_details[time_bin][city_id] = []\n",
    "                temporal_city_details[time_bin][city_id].append({\n",
    "                    'original_gpe': original_gpe,\n",
    "                    'matched_entity': entity,\n",
    "                    'time': tweet_time\n",
    "                })\n",
    "        \n",
    "        # === PART 2: CASCADE FROM TWEET POINT (spatial-based) ===\n",
    "        # Find containing county\n",
    "        containing_county = None\n",
    "        for county_id, county_data in county_geoms.items():\n",
    "            if county_data['geometry'].contains(tweet_point):\n",
    "                containing_county = county_data\n",
    "                break\n",
    "        \n",
    "        if containing_county:\n",
    "            county_geoid = containing_county['geoid']\n",
    "            county_statefp = containing_county['statefp']\n",
    "            county_name = containing_county['name']\n",
    "            \n",
    "            # CASCADE: Increment county count\n",
    "            temporal_county_mentions[time_bin][county_geoid] = temporal_county_mentions[time_bin].get(county_geoid, 0) + 1\n",
    "            \n",
    "            if county_geoid not in temporal_county_details[time_bin]:\n",
    "                temporal_county_details[time_bin][county_geoid] = []\n",
    "            temporal_county_details[time_bin][county_geoid].append({\n",
    "                'original_gpe': f'[CASCADE from point in {county_name}]',\n",
    "                'matched_entity': f'{county_name} County',\n",
    "                'time': tweet_time\n",
    "            })\n",
    "            \n",
    "            # CASCADE: Find containing state\n",
    "            if county_statefp in state_geoms:\n",
    "                state_data = state_geoms[county_statefp]\n",
    "                state_code = state_data['stusps']\n",
    "                state_name = state_data['name']\n",
    "                \n",
    "                # CASCADE: Increment state count\n",
    "                temporal_state_mentions[time_bin][state_code] = temporal_state_mentions[time_bin].get(state_code, 0) + 1\n",
    "                \n",
    "                if state_code not in temporal_state_details[time_bin]:\n",
    "                    temporal_state_details[time_bin][state_code] = []\n",
    "                temporal_state_details[time_bin][state_code].append({\n",
    "                    'original_gpe': f'[CASCADE from point in {state_name}]',\n",
    "                    'matched_entity': state_name,\n",
    "                    'time': tweet_time\n",
    "                })\n",
    "        \n",
    "        # CASCADE: Find nearest city (within 50km)\n",
    "        # Buffer point by ~50km (0.45 degrees)\n",
    "        tweet_buffer = tweet_point.buffer(0.45)\n",
    "        \n",
    "        nearest_city = None\n",
    "        min_distance = float('inf')\n",
    "        \n",
    "        for city_id, city_data in city_geoms.items():\n",
    "            if tweet_buffer.contains(city_data['geometry']):\n",
    "                distance = tweet_point.distanceTo(city_data['geometry'])\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    nearest_city = city_data\n",
    "        \n",
    "        if nearest_city:\n",
    "            city_id = nearest_city['geonameid']\n",
    "            city_name = nearest_city['name']\n",
    "            \n",
    "            # CASCADE: Increment city count\n",
    "            temporal_city_mentions[time_bin][city_id] = temporal_city_mentions[time_bin].get(city_id, 0) + 1\n",
    "            \n",
    "            if city_id not in temporal_city_details[time_bin]:\n",
    "                temporal_city_details[time_bin][city_id] = []\n",
    "            temporal_city_details[time_bin][city_id].append({\n",
    "                'original_gpe': '[CASCADE from nearby point]',\n",
    "                'matched_entity': city_name,\n",
    "                'time': tweet_time\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n  ✓ Found mentions across {len(time_bins)} time bins\")\n",
    "    \n",
    "    return (time_bins, temporal_state_mentions, temporal_county_mentions, temporal_city_mentions,\n",
    "            temporal_state_details, temporal_county_details, temporal_city_details)\n",
    "\n",
    "\n",
    "print(\"✓ Counting function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Export Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Export helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "HELPER FUNCTIONS FOR EXPORTING INDIVIDUAL ENTITY TYPES\n",
    "\"\"\"\n",
    "\n",
    "def export_states_incremental(states_fc, mention_counts, mention_details, bin_label, output_shp):\n",
    "    \"\"\"Export incremental state counts to shapefile\"\"\"\n",
    "    arcpy.management.CreateFeatureclass(\n",
    "        os.path.dirname(output_shp),\n",
    "        os.path.basename(output_shp),\n",
    "        \"POLYGON\",\n",
    "        spatial_reference=arcpy.SpatialReference(4326)\n",
    "    )\n",
    "    \n",
    "    arcpy.management.AddField(output_shp, \"state_name\", \"TEXT\", field_length=100)\n",
    "    arcpy.management.AddField(output_shp, \"state_code\", \"TEXT\", field_length=2)\n",
    "    arcpy.management.AddField(output_shp, \"tweet_cnt\", \"LONG\")\n",
    "    arcpy.management.AddField(output_shp, \"smpl_gpe\", \"TEXT\", field_length=254)\n",
    "    arcpy.management.AddField(output_shp, \"time_bin\", \"TEXT\", field_length=50)\n",
    "    \n",
    "    with arcpy.da.InsertCursor(output_shp, ['SHAPE@', 'state_name', 'state_code', 'tweet_cnt', 'smpl_gpe', 'time_bin']) as ins_cursor:\n",
    "        with arcpy.da.SearchCursor(states_fc, ['NAME', 'STUSPS', 'SHAPE@']) as cursor:\n",
    "            for row in cursor:\n",
    "                state_code = row[1]\n",
    "                if state_code in mention_counts:\n",
    "                    sample_gpe = ' | '.join([d['original_gpe'][:100] for d in mention_details[state_code][:3]])\n",
    "                    ins_cursor.insertRow([\n",
    "                        row[2],\n",
    "                        row[0],\n",
    "                        state_code,\n",
    "                        mention_counts[state_code],\n",
    "                        sample_gpe[:254],\n",
    "                        bin_label\n",
    "                    ])\n",
    "\n",
    "\n",
    "def export_states_cumulative(states_fc, cumulative_counts, bin_label, output_shp):\n",
    "    \"\"\"Export cumulative state counts to shapefile\"\"\"\n",
    "    arcpy.management.CreateFeatureclass(\n",
    "        os.path.dirname(output_shp),\n",
    "        os.path.basename(output_shp),\n",
    "        \"POLYGON\",\n",
    "        spatial_reference=arcpy.SpatialReference(4326)\n",
    "    )\n",
    "    \n",
    "    arcpy.management.AddField(output_shp, \"state_name\", \"TEXT\", field_length=100)\n",
    "    arcpy.management.AddField(output_shp, \"state_code\", \"TEXT\", field_length=2)\n",
    "    arcpy.management.AddField(output_shp, \"cumul_cnt\", \"LONG\")\n",
    "    arcpy.management.AddField(output_shp, \"time_bin\", \"TEXT\", field_length=50)\n",
    "    \n",
    "    with arcpy.da.InsertCursor(output_shp, ['SHAPE@', 'state_name', 'state_code', 'cumul_cnt', 'time_bin']) as ins_cursor:\n",
    "        with arcpy.da.SearchCursor(states_fc, ['NAME', 'STUSPS', 'SHAPE@']) as cursor:\n",
    "            for row in cursor:\n",
    "                state_code = row[1]\n",
    "                if state_code in cumulative_counts:\n",
    "                    ins_cursor.insertRow([\n",
    "                        row[2],\n",
    "                        row[0],\n",
    "                        state_code,\n",
    "                        cumulative_counts[state_code],\n",
    "                        bin_label\n",
    "                    ])\n",
    "\n",
    "\n",
    "def export_counties_incremental(counties_fc, mention_counts, mention_details, bin_label, output_shp):\n",
    "    \"\"\"Export incremental county counts to shapefile\"\"\"\n",
    "    arcpy.management.CreateFeatureclass(\n",
    "        os.path.dirname(output_shp),\n",
    "        os.path.basename(output_shp),\n",
    "        \"POLYGON\",\n",
    "        spatial_reference=arcpy.SpatialReference(4326)\n",
    "    )\n",
    "    \n",
    "    arcpy.management.AddField(output_shp, \"cnty_name\", \"TEXT\", field_length=100)\n",
    "    arcpy.management.AddField(output_shp, \"cnty_id\", \"TEXT\", field_length=5)\n",
    "    arcpy.management.AddField(output_shp, \"state_fp\", \"TEXT\", field_length=2)\n",
    "    arcpy.management.AddField(output_shp, \"tweet_cnt\", \"LONG\")\n",
    "    arcpy.management.AddField(output_shp, \"smpl_gpe\", \"TEXT\", field_length=254)\n",
    "    arcpy.management.AddField(output_shp, \"time_bin\", \"TEXT\", field_length=50)\n",
    "    \n",
    "    with arcpy.da.InsertCursor(output_shp, ['SHAPE@', 'cnty_name', 'cnty_id', 'state_fp', 'tweet_cnt', 'smpl_gpe', 'time_bin']) as ins_cursor:\n",
    "        with arcpy.da.SearchCursor(counties_fc, ['NAME', 'GEOID', 'STATEFP', 'SHAPE@']) as cursor:\n",
    "            for row in cursor:\n",
    "                county_id = row[1]\n",
    "                if county_id in mention_counts:\n",
    "                    sample_gpe = ' | '.join([d['original_gpe'][:100] for d in mention_details[county_id][:3]])\n",
    "                    ins_cursor.insertRow([\n",
    "                        row[3],\n",
    "                        row[0],\n",
    "                        county_id,\n",
    "                        row[2],\n",
    "                        mention_counts[county_id],\n",
    "                        sample_gpe[:254],\n",
    "                        bin_label\n",
    "                    ])\n",
    "\n",
    "\n",
    "def export_counties_cumulative(counties_fc, cumulative_counts, bin_label, output_shp):\n",
    "    \"\"\"Export cumulative county counts to shapefile\"\"\"\n",
    "    arcpy.management.CreateFeatureclass(\n",
    "        os.path.dirname(output_shp),\n",
    "        os.path.basename(output_shp),\n",
    "        \"POLYGON\",\n",
    "        spatial_reference=arcpy.SpatialReference(4326)\n",
    "    )\n",
    "    \n",
    "    arcpy.management.AddField(output_shp, \"cnty_name\", \"TEXT\", field_length=100)\n",
    "    arcpy.management.AddField(output_shp, \"cnty_id\", \"TEXT\", field_length=5)\n",
    "    arcpy.management.AddField(output_shp, \"state_fp\", \"TEXT\", field_length=2)\n",
    "    arcpy.management.AddField(output_shp, \"cumul_cnt\", \"LONG\")\n",
    "    arcpy.management.AddField(output_shp, \"time_bin\", \"TEXT\", field_length=50)\n",
    "    \n",
    "    with arcpy.da.InsertCursor(output_shp, ['SHAPE@', 'cnty_name', 'cnty_id', 'state_fp', 'cumul_cnt', 'time_bin']) as ins_cursor:\n",
    "        with arcpy.da.SearchCursor(counties_fc, ['NAME', 'GEOID', 'STATEFP', 'SHAPE@']) as cursor:\n",
    "            for row in cursor:\n",
    "                county_id = row[1]\n",
    "                if county_id in cumulative_counts:\n",
    "                    ins_cursor.insertRow([\n",
    "                        row[3],\n",
    "                        row[0],\n",
    "                        county_id,\n",
    "                        row[2],\n",
    "                        cumulative_counts[county_id],\n",
    "                        bin_label\n",
    "                    ])\n",
    "\n",
    "\n",
    "def export_cities_incremental(cities_fc, mention_counts, mention_details, bin_label, output_shp):\n",
    "    \"\"\"Export incremental city counts to shapefile\"\"\"\n",
    "    arcpy.management.CreateFeatureclass(\n",
    "        os.path.dirname(output_shp),\n",
    "        os.path.basename(output_shp),\n",
    "        \"POINT\",\n",
    "        spatial_reference=arcpy.SpatialReference(4326)\n",
    "    )\n",
    "    \n",
    "    arcpy.management.AddField(output_shp, \"city_name\", \"TEXT\", field_length=200)\n",
    "    arcpy.management.AddField(output_shp, \"city_id\", \"LONG\")\n",
    "    arcpy.management.AddField(output_shp, \"population\", \"LONG\")\n",
    "    arcpy.management.AddField(output_shp, \"tweet_cnt\", \"LONG\")\n",
    "    arcpy.management.AddField(output_shp, \"mtchd_ent\", \"TEXT\", field_length=254)\n",
    "    arcpy.management.AddField(output_shp, \"orig_gpe\", \"TEXT\", field_length=254)\n",
    "    arcpy.management.AddField(output_shp, \"time_bin\", \"TEXT\", field_length=50)\n",
    "    \n",
    "    with arcpy.da.InsertCursor(output_shp, ['SHAPE@', 'city_name', 'city_id', 'population', 'tweet_cnt', 'mtchd_ent', 'orig_gpe', 'time_bin']) as ins_cursor:\n",
    "        with arcpy.da.SearchCursor(cities_fc, ['name', 'geonameid', 'population', 'SHAPE@']) as cursor:\n",
    "            for row in cursor:\n",
    "                city_id = row[1]\n",
    "                if city_id in mention_counts:\n",
    "                    matched_entities = '; '.join([d['matched_entity'] for d in mention_details[city_id]])\n",
    "                    orig_gpe = ' | '.join([d['original_gpe'] for d in mention_details[city_id]])\n",
    "                    \n",
    "                    ins_cursor.insertRow([\n",
    "                        row[3],\n",
    "                        row[0],\n",
    "                        city_id,\n",
    "                        row[2],\n",
    "                        mention_counts[city_id],\n",
    "                        matched_entities[:254],\n",
    "                        orig_gpe[:254],\n",
    "                        bin_label\n",
    "                    ])\n",
    "\n",
    "\n",
    "def export_cities_cumulative(cities_fc, cumulative_counts, bin_label, output_shp):\n",
    "    \"\"\"Export cumulative city counts to shapefile\"\"\"\n",
    "    arcpy.management.CreateFeatureclass(\n",
    "        os.path.dirname(output_shp),\n",
    "        os.path.basename(output_shp),\n",
    "        \"POINT\",\n",
    "        spatial_reference=arcpy.SpatialReference(4326)\n",
    "    )\n",
    "    \n",
    "    arcpy.management.AddField(output_shp, \"city_name\", \"TEXT\", field_length=200)\n",
    "    arcpy.management.AddField(output_shp, \"city_id\", \"LONG\")\n",
    "    arcpy.management.AddField(output_shp, \"population\", \"LONG\")\n",
    "    arcpy.management.AddField(output_shp, \"cumul_cnt\", \"LONG\")\n",
    "    arcpy.management.AddField(output_shp, \"time_bin\", \"TEXT\", field_length=50)\n",
    "    \n",
    "    with arcpy.da.InsertCursor(output_shp, ['SHAPE@', 'city_name', 'city_id', 'population', 'cumul_cnt', 'time_bin']) as ins_cursor:\n",
    "        with arcpy.da.SearchCursor(cities_fc, ['name', 'geonameid', 'population', 'SHAPE@']) as cursor:\n",
    "            for row in cursor:\n",
    "                city_id = row[1]\n",
    "                if city_id in cumulative_counts:\n",
    "                    ins_cursor.insertRow([\n",
    "                        row[3],\n",
    "                        row[0],\n",
    "                        city_id,\n",
    "                        row[2],\n",
    "                        cumulative_counts[city_id],\n",
    "                        bin_label\n",
    "                    ])\n",
    "\n",
    "\n",
    "print(\"✓ Export helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Main Export Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Main export function defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXPORT TEMPORAL DATA TO SHAPEFILES\n",
    "\"\"\"\n",
    "\n",
    "def export_temporal_to_shapefiles(time_bins, temporal_state_mentions, temporal_county_mentions,\n",
    "                                  temporal_city_mentions, temporal_state_details, temporal_county_details,\n",
    "                                  temporal_city_details, states_fc, counties_fc, cities_fc,\n",
    "                                  output_dir='arcgis_outputs'):\n",
    "    \"\"\"\n",
    "    Export temporal (4-hour binned) data for states, counties, and cities.\n",
    "    Creates BOTH incremental and cumulative count shapefiles\n",
    "    \"\"\"\n",
    "    \n",
    "    temporal_dir = os.path.join(output_dir, 'temporal_4hour_bins')\n",
    "    incremental_dir = os.path.join(temporal_dir, 'incremental')\n",
    "    cumulative_dir = os.path.join(temporal_dir, 'cumulative')\n",
    "    \n",
    "    os.makedirs(incremental_dir, exist_ok=True)\n",
    "    os.makedirs(cumulative_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EXPORTING TEMPORAL DATA - INCREMENTAL & CUMULATIVE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTime bins: {len(time_bins)}\")\n",
    "    print(f\"Output directory: {temporal_dir}\")\n",
    "    \n",
    "    cumulative_state_counts = {}\n",
    "    cumulative_county_counts = {}\n",
    "    cumulative_city_counts = {}\n",
    "    \n",
    "    incremental_bin_files = {'states': [], 'counties': [], 'cities': []}\n",
    "    cumulative_bin_files = {'states': [], 'counties': [], 'cities': []}\n",
    "    \n",
    "    for idx, bin_time in enumerate(time_bins):\n",
    "        bin_str = bin_time.strftime('%Y%m%d_%H%M')\n",
    "        bin_label = bin_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"\\n  Processing time bin {idx+1}/{len(time_bins)}: {bin_label}\")\n",
    "        \n",
    "        # Update cumulative counts\n",
    "        for state_code, count in temporal_state_mentions[bin_time].items():\n",
    "            cumulative_state_counts[state_code] = cumulative_state_counts.get(state_code, 0) + count\n",
    "        \n",
    "        for county_id, count in temporal_county_mentions[bin_time].items():\n",
    "            cumulative_county_counts[county_id] = cumulative_county_counts.get(county_id, 0) + count\n",
    "        \n",
    "        for city_id, count in temporal_city_mentions[bin_time].items():\n",
    "            cumulative_city_counts[city_id] = cumulative_city_counts.get(city_id, 0) + count\n",
    "        \n",
    "        # STATES\n",
    "        if temporal_state_mentions[bin_time]:\n",
    "            states_inc_shp = os.path.join(incremental_dir, f'states_inc_{bin_str}.shp')\n",
    "            export_states_incremental(states_fc, temporal_state_mentions[bin_time],\n",
    "                                     temporal_state_details[bin_time], bin_label, states_inc_shp)\n",
    "            incremental_bin_files['states'].append(states_inc_shp)\n",
    "            print(f\"    States incremental: {int(arcpy.management.GetCount(states_inc_shp).getOutput(0))} features\")\n",
    "        \n",
    "        states_cum_shp = os.path.join(cumulative_dir, f'states_cum_{bin_str}.shp')\n",
    "        export_states_cumulative(states_fc, cumulative_state_counts, bin_label, states_cum_shp)\n",
    "        cumulative_bin_files['states'].append(states_cum_shp)\n",
    "        print(f\"    States cumulative: {int(arcpy.management.GetCount(states_cum_shp).getOutput(0))} features\")\n",
    "        \n",
    "        # COUNTIES\n",
    "        if temporal_county_mentions[bin_time]:\n",
    "            counties_inc_shp = os.path.join(incremental_dir, f'counties_inc_{bin_str}.shp')\n",
    "            export_counties_incremental(counties_fc, temporal_county_mentions[bin_time],\n",
    "                                       temporal_county_details[bin_time], bin_label, counties_inc_shp)\n",
    "            incremental_bin_files['counties'].append(counties_inc_shp)\n",
    "            print(f\"    Counties incremental: {int(arcpy.management.GetCount(counties_inc_shp).getOutput(0))} features\")\n",
    "        \n",
    "        counties_cum_shp = os.path.join(cumulative_dir, f'counties_cum_{bin_str}.shp')\n",
    "        export_counties_cumulative(counties_fc, cumulative_county_counts, bin_label, counties_cum_shp)\n",
    "        cumulative_bin_files['counties'].append(counties_cum_shp)\n",
    "        print(f\"    Counties cumulative: {int(arcpy.management.GetCount(counties_cum_shp).getOutput(0))} features\")\n",
    "        \n",
    "        # CITIES\n",
    "        if temporal_city_mentions[bin_time]:\n",
    "            cities_inc_shp = os.path.join(incremental_dir, f'cities_inc_{bin_str}.shp')\n",
    "            export_cities_incremental(cities_fc, temporal_city_mentions[bin_time],\n",
    "                                     temporal_city_details[bin_time], bin_label, cities_inc_shp)\n",
    "            incremental_bin_files['cities'].append(cities_inc_shp)\n",
    "            print(f\"    Cities incremental: {int(arcpy.management.GetCount(cities_inc_shp).getOutput(0))} features\")\n",
    "        \n",
    "        cities_cum_shp = os.path.join(cumulative_dir, f'cities_cum_{bin_str}.shp')\n",
    "        export_cities_cumulative(cities_fc, cumulative_city_counts, bin_label, cities_cum_shp)\n",
    "        cumulative_bin_files['cities'].append(cities_cum_shp)\n",
    "        print(f\"    Cities cumulative: {int(arcpy.management.GetCount(cities_cum_shp).getOutput(0))} features\")\n",
    "    \n",
    "    # Merge files\n",
    "    print(f\"\\n  Creating master files by merging shapefiles...\")\n",
    "    \n",
    "    if incremental_bin_files['states']:\n",
    "        arcpy.management.Merge(incremental_bin_files['states'], os.path.join(incremental_dir, 'states_INCREMENTAL_ALL.shp'))\n",
    "        print(f\"    ✓ States incremental master\")\n",
    "    \n",
    "    if cumulative_bin_files['states']:\n",
    "        arcpy.management.Merge(cumulative_bin_files['states'], os.path.join(cumulative_dir, 'states_CUMULATIVE_ALL.shp'))\n",
    "        print(f\"    ✓ States cumulative master\")\n",
    "    \n",
    "    if incremental_bin_files['counties']:\n",
    "        arcpy.management.Merge(incremental_bin_files['counties'], os.path.join(incremental_dir, 'counties_INCREMENTAL_ALL.shp'))\n",
    "        print(f\"    ✓ Counties incremental master\")\n",
    "    \n",
    "    if cumulative_bin_files['counties']:\n",
    "        arcpy.management.Merge(cumulative_bin_files['counties'], os.path.join(cumulative_dir, 'counties_CUMULATIVE_ALL.shp'))\n",
    "        print(f\"    ✓ Counties cumulative master\")\n",
    "    \n",
    "    if incremental_bin_files['cities']:\n",
    "        arcpy.management.Merge(incremental_bin_files['cities'], os.path.join(incremental_dir, 'cities_INCREMENTAL_ALL.shp'))\n",
    "        print(f\"    ✓ Cities incremental master\")\n",
    "    \n",
    "    if cumulative_bin_files['cities']:\n",
    "        arcpy.management.Merge(cumulative_bin_files['cities'], os.path.join(cumulative_dir, 'cities_CUMULATIVE_ALL.shp'))\n",
    "        print(f\"    ✓ Cities cumulative master\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TEMPORAL EXPORT COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nFiles saved to: {os.path.abspath(temporal_dir)}\")\n",
    "    print(f\"\\nTo use in ArcGIS Pro:\")\n",
    "    print(f\"  1. Add *_INCREMENTAL_ALL.shp or *_CUMULATIVE_ALL.shp\")\n",
    "    print(f\"  2. Enable time using 'time_bin' field\")\n",
    "    print(f\"  3. Set time step to 4 hours\")\n",
    "    print(f\"  4. Animate!\")\n",
    "\n",
    "\n",
    "print(\"✓ Main export function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: MAIN EXECUTION - Run All Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ARCGIS TWEET PROCESSOR - EXACT MATCHING WITH PRIORITY\n",
      "============================================================\n",
      "\n",
      "STEP 1: Loading Data\n",
      "------------------------------------------------------------\n",
      "Loading tweets from helene.geojson...\n"
     ]
    },
    {
     "ename": "<class 'FileNotFoundError'>",
     "evalue": "GeoJSON file not found: C:\\WINDOWS\\system32\\in_memory\\data\\geojson\\helene.geojson",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTEP 1: Loading Data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m tweets_fc \u001b[38;5;241m=\u001b[39m load_tweets_geojson(workspace)\n\u001b[0;32m     16\u001b[0m cities_fc \u001b[38;5;241m=\u001b[39m load_cities_csv(workspace)\n\u001b[0;32m     17\u001b[0m states_fc \u001b[38;5;241m=\u001b[39m load_states_shapefile(workspace)\n",
      "Cell \u001b[1;32mIn[43], line 16\u001b[0m, in \u001b[0;36mload_tweets_geojson\u001b[1;34m(workspace)\u001b[0m\n\u001b[0;32m     14\u001b[0m geojson_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(workspace), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeojson\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelene.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(geojson_path):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeoJSON file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeojson_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeojson_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Read GeoJSON file\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: GeoJSON file not found: C:\\WINDOWS\\system32\\in_memory\\data\\geojson\\helene.geojson"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MAIN EXECUTION - RUNS ENTIRE WORKFLOW\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ARCGIS TWEET PROCESSOR - EXACT MATCHING WITH PRIORITY\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "workspace = \"in_memory\"\n",
    "\n",
    "# STEP 1: Load all data\n",
    "print(\"STEP 1: Loading Data\")\n",
    "print(\"-\" * 60)\n",
    "tweets_fc = load_tweets_geojson(workspace)\n",
    "cities_fc = load_cities_csv(workspace)\n",
    "states_fc = load_states_shapefile(workspace)\n",
    "counties_fc = load_counties_shapefile(workspace)\n",
    "print()\n",
    "\n",
    "# STEP 2: Create lookup dictionaries\n",
    "print(\"STEP 2: Creating Lookup Dictionaries\")\n",
    "print(\"-\" * 60)\n",
    "state_lookup, county_lookup, city_lookup = create_lookup_dictionaries(\n",
    "    states_fc, counties_fc, cities_fc\n",
    ")\n",
    "print()\n",
    "\n",
    "# STEP 3: Count mentions with temporal binning and cascade\n",
    "print(\"STEP 3: Counting Mentions with Priority Hierarchy and Cascade\")\n",
    "print(\"-\" * 60)\n",
    "(time_bins, temporal_state_mentions, temporal_county_mentions, temporal_city_mentions,\n",
    " temporal_state_details, temporal_county_details, temporal_city_details) = \\\n",
    "    count_mentions_with_cascade_temporal(\n",
    "        tweets_fc, state_lookup, county_lookup, city_lookup,\n",
    "        states_fc, counties_fc, cities_fc\n",
    "    )\n",
    "print()\n",
    "\n",
    "# STEP 4: Export temporal data\n",
    "print(\"STEP 4: Exporting Temporal Data\")\n",
    "print(\"-\" * 60)\n",
    "export_temporal_to_shapefiles(\n",
    "    time_bins, temporal_state_mentions, temporal_county_mentions, temporal_city_mentions,\n",
    "    temporal_state_details, temporal_county_details, temporal_city_details,\n",
    "    states_fc, counties_fc, cities_fc\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROCESSING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTime range: {time_bins[0].strftime('%Y-%m-%d %H:%M:%S')} to {time_bins[-1].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total time bins: {len(time_bins)}\")\n",
    "print(f\"\\nAll outputs saved to: arcgis_outputs/temporal_4hour_bins/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
