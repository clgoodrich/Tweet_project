{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurricane Social Media Analysis Pipeline\n",
    "## Complete Self-Contained Notebook for ArcGIS Pro\n",
    "\n",
    "This notebook contains all code needed to process hurricane-related tweets and generate time-sliced raster datasets.\n",
    "\n",
    "**Features:**\n",
    "- Multi-level geographic matching (STATE/COUNTY/CITY/FACILITY)\n",
    "- Kernel Density Estimation (KDE) for city points\n",
    "- Hierarchical weighted rasterization\n",
    "- Time-binned incremental and cumulative outputs\n",
    "- Full GeoTIFF export for ArcGIS Pro visualization\n",
    "\n",
    "**Requirements:**\n",
    "- geopandas\n",
    "- rasterio\n",
    "- fuzzywuzzy\n",
    "- scipy\n",
    "- numpy\n",
    "- pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Configuration\n",
    "\n",
    "Modify these paths and parameters as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully\n",
      "Project root: C:\\users\\colto\\documents\\github\\tweet_project\n",
      "Output directory: C:\\users\\colto\\documents\\github\\tweet_project\\rasters_output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "# Base Path Discovery\n",
    "# Update this to your project root if needed\n",
    "LOCAL_PATH = r\"C:\\users\\colto\\documents\\github\\tweet_project\"\n",
    "\n",
    "# Core Data Directories\n",
    "DATA_DIR = os.path.join(LOCAL_PATH, \"data\")\n",
    "GEOJSON_DIR = os.path.join(DATA_DIR, \"geojson\")\n",
    "SHAPE_FILES_DIR = os.path.join(DATA_DIR, \"shape_files\")\n",
    "OUTPUT_DIR = os.path.join(LOCAL_PATH, \"rasters_output\")\n",
    "\n",
    "# Input File Paths\n",
    "FRANCINE_PATH = os.path.join(GEOJSON_DIR, \"francine.geojson\")\n",
    "HELENE_PATH = os.path.join(GEOJSON_DIR, \"helene.geojson\")\n",
    "STATES_PATH = os.path.join(SHAPE_FILES_DIR, \"cb_2023_us_state_20m.shp\")\n",
    "COUNTIES_PATH = os.path.join(SHAPE_FILES_DIR, \"cb_2023_us_county_20m.shp\")\n",
    "CITIES_PATH = os.path.join(SHAPE_FILES_DIR, \"US_Cities.shp\")\n",
    "\n",
    "# Raster/Projection Settings\n",
    "TARGET_CRS = \"EPSG:3857\"  # Web Mercator\n",
    "CELL_SIZE_M = 1000  # 1km cells\n",
    "\n",
    "# Hierarchical Weights (relative influence by admin level)\n",
    "WEIGHTS: Dict[str, int] = {\n",
    "    \"STATE\": 2,\n",
    "    \"COUNTY\": 5,\n",
    "    \"CITY\": 10,\n",
    "    \"FACILITY\": 10,\n",
    "}\n",
    "\n",
    "# Fuzzy Matching & Time Binning\n",
    "FUZZY_THRESHOLD = 75  # Global matching threshold (0-100)\n",
    "FUZZY_THRESHOLD_CONTEXTUAL = 70  # Contextual (within-state) threshold\n",
    "TIME_BIN_HOURS = 4  # Temporal bin width\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Project root: {LOCAL_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'ModuleNotFoundError'>",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrasterio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_bounds\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import re\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.features import rasterize\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hurricane_data() -> Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Load hurricane GeoJSON files and standardize timestamps, time bins, and labels.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LOADING HURRICANE DATA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\nLoading Francine data from: {FRANCINE_PATH}\")\n",
    "    francine_gdf = gpd.read_file(FRANCINE_PATH)\n",
    "\n",
    "    print(f\"Loading Helene data from: {HELENE_PATH}\")\n",
    "    helene_gdf = gpd.read_file(HELENE_PATH)\n",
    "\n",
    "    print(\"\\nStandardizing timestamps to UTC...\")\n",
    "    francine_gdf[\"timestamp\"] = pd.to_datetime(francine_gdf[\"time\"], utc=True)\n",
    "    helene_gdf[\"timestamp\"] = pd.to_datetime(helene_gdf[\"time\"], utc=True)\n",
    "\n",
    "    time_bin_str = f\"{TIME_BIN_HOURS}h\"\n",
    "    print(f\"Grouping data into {TIME_BIN_HOURS}-hour bins...\")\n",
    "    francine_gdf[\"time_bin\"] = francine_gdf[\"timestamp\"].dt.floor(time_bin_str)\n",
    "    helene_gdf[\"time_bin\"] = helene_gdf[\"timestamp\"].dt.floor(time_bin_str)\n",
    "\n",
    "    francine_gdf[\"unix_timestamp\"] = francine_gdf[\"time_bin\"].astype(\"int64\") // 1000\n",
    "    helene_gdf[\"unix_timestamp\"] = helene_gdf[\"time_bin\"].astype(\"int64\") // 1000\n",
    "\n",
    "    francine_gdf[\"bin_label\"] = francine_gdf[\"time_bin\"].dt.strftime(\"%Y%m%d_%H%M\")\n",
    "    helene_gdf[\"bin_label\"] = helene_gdf[\"time_bin\"].dt.strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "    print(f\"\\nLoaded {len(francine_gdf)} Francine tweets\")\n",
    "    print(f\"Loaded {len(helene_gdf)} Helene tweets\")\n",
    "\n",
    "    return francine_gdf, helene_gdf\n",
    "\n",
    "\n",
    "def load_reference_shapefiles() -> Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Load reference shapefiles for states, counties, and cities.\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading reference shapefiles...\")\n",
    "\n",
    "    states_gdf = gpd.read_file(STATES_PATH)\n",
    "    print(f\"  Loaded {len(states_gdf)} states\")\n",
    "\n",
    "    counties_gdf = gpd.read_file(COUNTIES_PATH)\n",
    "    print(f\"  Loaded {len(counties_gdf)} counties\")\n",
    "\n",
    "    cities_gdf = gpd.read_file(CITIES_PATH)\n",
    "    print(f\"  Loaded {len(cities_gdf)} cities\")\n",
    "\n",
    "    return states_gdf, counties_gdf, cities_gdf\n",
    "\n",
    "\n",
    "def create_timestamp_dictionaries(\n",
    "    francine_gdf: gpd.GeoDataFrame, helene_gdf: gpd.GeoDataFrame\n",
    ") -> Tuple[Dict[int, pd.Timestamp], Dict[int, pd.Timestamp]]:\n",
    "    \"\"\"\n",
    "    Create lookup dictionaries mapping unix_timestamp -> time_bin.\n",
    "    \"\"\"\n",
    "    francine_dict = dict(zip(francine_gdf[\"unix_timestamp\"], francine_gdf[\"time_bin\"]))\n",
    "    helene_dict = dict(zip(helene_gdf[\"unix_timestamp\"], helene_gdf[\"time_bin\"]))\n",
    "    return francine_dict, helene_dict\n",
    "\n",
    "\n",
    "def get_time_bins(gdf: gpd.GeoDataFrame) -> List[int]:\n",
    "    \"\"\"\n",
    "    Get sorted unique unix_timestamp bins from a GeoDataFrame.\n",
    "    \"\"\"\n",
    "    return sorted(gdf[\"unix_timestamp\"].unique())\n",
    "\n",
    "\n",
    "print(\"Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Geographic Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_place_name(name: Any) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Standardize place names for better matching.\n",
    "    \"\"\"\n",
    "    if pd.isna(name) or name == \"NAN\":\n",
    "        return None\n",
    "\n",
    "    name = str(name).upper().strip()\n",
    "\n",
    "    # Abbreviation expansions\n",
    "    name = re.sub(r\"\\bST\\.?\\b\", \"SAINT\", name)\n",
    "    name = re.sub(r\"\\bMT\\.?\\b\", \"MOUNT\", name)\n",
    "    name = re.sub(r\"\\bFT\\.?\\b\", \"FORT\", name)\n",
    "    name = re.sub(r\"\\bN\\.?\\b\", \"NORTH\", name)\n",
    "    name = re.sub(r\"\\bS\\.?\\b\", \"SOUTH\", name)\n",
    "    name = re.sub(r\"\\bE\\.?\\b\", \"EAST\", name)\n",
    "    name = re.sub(r\"\\bW\\.?\\b\", \"WEST\", name)\n",
    "\n",
    "    # Remove punctuation; collapse whitespace\n",
    "    name = re.sub(r\"[^\\w\\s]\", \"\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "\n",
    "    return name.strip()\n",
    "\n",
    "\n",
    "def parse_gpe_entities(gpe_string: Any) -> List[Optional[str]]:\n",
    "    \"\"\"\n",
    "    Parse a free-text GPE field into a list of cleaned candidate entities.\n",
    "    \"\"\"\n",
    "    if not gpe_string or pd.isna(gpe_string) or str(gpe_string).strip() == \"\":\n",
    "        return []\n",
    "\n",
    "    gpe_string = str(gpe_string).strip()\n",
    "    entities: List[Optional[str]] = []\n",
    "\n",
    "    parts = [part.strip() for part in gpe_string.split(\",\")]\n",
    "\n",
    "    for part in parts:\n",
    "        if part:\n",
    "            sub_parts = re.split(r\"[;&|]\", part)\n",
    "            for sub_part in sub_parts:\n",
    "                sub_part = sub_part.strip()\n",
    "                if sub_part and len(sub_part) > 1:\n",
    "                    entities.append(preprocess_place_name(sub_part))\n",
    "\n",
    "    clean_entities: List[str] = []\n",
    "    seen: set[str] = set()\n",
    "    for entity in entities:\n",
    "        if entity and entity not in seen:\n",
    "            clean_entities.append(entity)\n",
    "            seen.add(entity)\n",
    "\n",
    "    return clean_entities\n",
    "\n",
    "\n",
    "def create_hierarchical_lookups(\n",
    "    states_gdf: gpd.GeoDataFrame,\n",
    "    counties_gdf: gpd.GeoDataFrame,\n",
    "    cities_gdf: gpd.GeoDataFrame,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create hierarchical lookup dictionaries for fuzzy matching.\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating hierarchical lookup dictionaries...\")\n",
    "\n",
    "    # States\n",
    "    state_lookup: Dict[str, Any] = {}\n",
    "    state_abbrev_to_name: Dict[str, str] = {}\n",
    "    state_name_to_abbrev: Dict[str, str] = {}\n",
    "\n",
    "    for _, row in states_gdf.iterrows():\n",
    "        state_name = preprocess_place_name(row[\"NAME\"])\n",
    "        if state_name:\n",
    "            state_lookup[state_name] = row.geometry\n",
    "            if \"STUSPS\" in row:\n",
    "                abbrev = str(row[\"STUSPS\"]).upper()\n",
    "                state_abbrev_to_name[abbrev] = state_name\n",
    "                state_name_to_abbrev[state_name] = abbrev\n",
    "                state_lookup[abbrev] = row.geometry\n",
    "\n",
    "    # Counties\n",
    "    county_by_state: Dict[str, Dict[str, Any]] = {}\n",
    "    county_lookup: Dict[str, Any] = {}\n",
    "\n",
    "    for _, row in counties_gdf.iterrows():\n",
    "        county_name = preprocess_place_name(row[\"NAME\"])\n",
    "        state_fips = row.get(\"STATEFP\", \"\")\n",
    "\n",
    "        if county_name:\n",
    "            county_lookup[county_name] = row.geometry\n",
    "\n",
    "            state_name = None\n",
    "            if \"STATE_NAME\" in row:\n",
    "                state_name = preprocess_place_name(row[\"STATE_NAME\"])\n",
    "            else:\n",
    "                for _, s_row in states_gdf.iterrows():\n",
    "                    if s_row.get(\"STATEFP\", \"\") == state_fips:\n",
    "                        state_name = preprocess_place_name(s_row[\"NAME\"])\n",
    "                        break\n",
    "\n",
    "            if state_name:\n",
    "                county_by_state.setdefault(state_name, {})\n",
    "                county_by_state[state_name][county_name] = row.geometry\n",
    "\n",
    "    # Cities (as points for KDE)\n",
    "    city_by_state: Dict[str, Dict[str, Any]] = {}\n",
    "    city_lookup: Dict[str, Any] = {}\n",
    "\n",
    "    for _, row in cities_gdf.iterrows():\n",
    "        city_name = preprocess_place_name(row[\"NAME\"])\n",
    "        state_abbrev = str(row.get(\"ST\", \"\")).upper()\n",
    "\n",
    "        if city_name:\n",
    "            if hasattr(row.geometry, 'centroid'):\n",
    "                city_lookup[city_name] = row.geometry.centroid\n",
    "            else:\n",
    "                city_lookup[city_name] = row.geometry\n",
    "\n",
    "            if state_abbrev in state_abbrev_to_name:\n",
    "                state_full = state_abbrev_to_name[state_abbrev]\n",
    "                city_by_state.setdefault(state_full, {})\n",
    "                if hasattr(row.geometry, 'centroid'):\n",
    "                    city_by_state[state_full][city_name] = row.geometry.centroid\n",
    "                else:\n",
    "                    city_by_state[state_full][city_name] = row.geometry\n",
    "\n",
    "    return {\n",
    "        \"state_lookup\": state_lookup,\n",
    "        \"county_lookup\": county_lookup,\n",
    "        \"city_lookup\": city_lookup,\n",
    "        \"county_by_state\": county_by_state,\n",
    "        \"city_by_state\": city_by_state,\n",
    "        \"state_abbrev_to_name\": state_abbrev_to_name,\n",
    "        \"state_name_to_abbrev\": state_name_to_abbrev,\n",
    "    }\n",
    "\n",
    "\n",
    "def fuzzy_match_entity(\n",
    "    entity: Optional[str],\n",
    "    candidates: Dict[str, Any],\n",
    "    threshold: int = FUZZY_THRESHOLD,\n",
    ") -> Tuple[Optional[str], int]:\n",
    "    \"\"\"\n",
    "    Fuzzy match an entity against candidate names.\n",
    "    \"\"\"\n",
    "    if not entity or not candidates:\n",
    "        return None, 0\n",
    "\n",
    "    if entity in candidates:\n",
    "        return entity, 100\n",
    "\n",
    "    match = process.extractOne(entity, candidates.keys(), scorer=fuzz.ratio)\n",
    "    if match and match[1] >= threshold:\n",
    "        return match[0], match[1]\n",
    "\n",
    "    return None, 0\n",
    "\n",
    "\n",
    "print(\"Geographic matching functions defined (1/3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Geographic Matching Functions (continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_geographic_matches(\n",
    "    entities: List[str],\n",
    "    lookups: Dict[str, Any],\n",
    ") -> List[Tuple[str, Optional[str], Any, int]]:\n",
    "    \"\"\"\n",
    "    Find ALL geographic matches (STATE, COUNTY, CITY) for the given entities.\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        return []\n",
    "\n",
    "    state_lookup = lookups[\"state_lookup\"]\n",
    "    county_lookup = lookups[\"county_lookup\"]\n",
    "    city_lookup = lookups[\"city_lookup\"]\n",
    "    county_by_state = lookups[\"county_by_state\"]\n",
    "    city_by_state = lookups[\"city_by_state\"]\n",
    "\n",
    "    all_matches: List[Tuple[str, Optional[str], Any, int]] = []\n",
    "    found_states: set[str] = set()\n",
    "\n",
    "    # STATES\n",
    "    for entity in entities:\n",
    "        state_match, state_score = fuzzy_match_entity(entity, state_lookup, threshold=75)\n",
    "        if state_match:\n",
    "            all_matches.append((\"STATE\", state_match, state_lookup[state_match], state_score))\n",
    "            found_states.add(state_match)\n",
    "\n",
    "    # COUNTIES\n",
    "    for entity in entities:\n",
    "        county_match, county_score = fuzzy_match_entity(entity, county_lookup, threshold=75)\n",
    "        if county_match:\n",
    "            all_matches.append((\"COUNTY\", county_match, county_lookup[county_match], county_score))\n",
    "\n",
    "        for state_name in found_states:\n",
    "            if state_name in county_by_state:\n",
    "                state_counties = county_by_state[state_name]\n",
    "                state_county_match, state_county_score = fuzzy_match_entity(\n",
    "                    entity, state_counties, threshold=70\n",
    "                )\n",
    "                if state_county_match and state_county_score > county_score:\n",
    "                    all_matches = [\n",
    "                        m for m in all_matches\n",
    "                        if not (m[0] == \"COUNTY\" and m[1] == county_match)\n",
    "                    ]\n",
    "                    all_matches.append(\n",
    "                        (\"COUNTY\", state_county_match, state_counties[state_county_match], state_county_score)\n",
    "                    )\n",
    "\n",
    "    # CITIES\n",
    "    for entity in entities:\n",
    "        city_match, city_score = fuzzy_match_entity(entity, city_lookup, threshold=75)\n",
    "        if city_match:\n",
    "            all_matches.append((\"CITY\", city_match, city_lookup[city_match], city_score))\n",
    "\n",
    "        for state_name in found_states:\n",
    "            if state_name in city_by_state:\n",
    "                state_cities = city_by_state[state_name]\n",
    "                state_city_match, state_city_score = fuzzy_match_entity(\n",
    "                    entity, state_cities, threshold=70\n",
    "                )\n",
    "                if state_city_match and state_city_score > city_score:\n",
    "                    all_matches = [\n",
    "                        m for m in all_matches\n",
    "                        if not (m[0] == \"CITY\" and m[1] == city_match)\n",
    "                    ]\n",
    "                    all_matches.append(\n",
    "                        (\"CITY\", state_city_match, state_cities[state_city_match], state_city_score)\n",
    "                    )\n",
    "\n",
    "    # De-duplicate\n",
    "    unique_matches: List[Tuple[str, Optional[str], Any, int]] = []\n",
    "    seen_combinations: set[Tuple[str, Optional[str]]] = set()\n",
    "    for match in all_matches:\n",
    "        combo = (match[0], match[1])\n",
    "        if combo not in seen_combinations:\n",
    "            unique_matches.append(match)\n",
    "            seen_combinations.add(combo)\n",
    "\n",
    "    return unique_matches\n",
    "\n",
    "\n",
    "def multi_level_assign_scale_levels(\n",
    "    row: pd.Series,\n",
    "    lookups: Dict[str, Any],\n",
    ") -> List[Tuple[str, Optional[str], Any, int]]:\n",
    "    \"\"\"\n",
    "    Return ALL geographic scale levels that match this tweet.\n",
    "    \"\"\"\n",
    "    gpe = str(row.get(\"GPE\", \"\")).strip()\n",
    "    fac = str(row.get(\"FAC\", \"\")).strip()\n",
    "\n",
    "    matches: List[Tuple[str, Optional[str], Any, int]] = []\n",
    "\n",
    "    entities = parse_gpe_entities(gpe)\n",
    "    if entities:\n",
    "        geo_matches = find_all_geographic_matches(entities, lookups)\n",
    "        matches.extend(geo_matches)\n",
    "\n",
    "    if fac and fac not in [\"nan\", \"NAN\", \"\"]:\n",
    "        matches.append((\"FACILITY\", fac, row.geometry, 100))\n",
    "\n",
    "    if not matches:\n",
    "        matches.append((\"UNMATCHED\", None, row.geometry, 0))\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "def expand_tweets_by_matches(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    lookups: Dict[str, Any],\n",
    "    dataset_name: str,\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Expand the GeoDataFrame so each tweet creates multiple rows (one per match).\n",
    "    \"\"\"\n",
    "    print(f\"\\nExpanding {dataset_name} tweets by geographic matches...\")\n",
    "\n",
    "    expanded_rows: List[pd.Series] = []\n",
    "\n",
    "    for idx, row in gdf.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"  Processing tweet {idx}...\")\n",
    "\n",
    "        matches = multi_level_assign_scale_levels(row, lookups)\n",
    "\n",
    "        for scale, name, geom, score in matches:\n",
    "            new_row = row.copy()\n",
    "            new_row[\"scale_level\"] = scale\n",
    "            new_row[\"matched_name\"] = name\n",
    "            new_row[\"matched_geom\"] = geom\n",
    "            new_row[\"match_score\"] = score\n",
    "            new_row[\"original_index\"] = idx\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    expanded_gdf = gpd.GeoDataFrame(expanded_rows, crs=gdf.crs)\n",
    "\n",
    "    print(f\"  Expanded from {len(gdf)} to {len(expanded_gdf)} rows\")\n",
    "\n",
    "    scale_counts = expanded_gdf[\"scale_level\"].value_counts()\n",
    "    print(f\"\\n  Scale level distribution:\")\n",
    "    for scale, count in scale_counts.items():\n",
    "        print(f\"    {scale}: {count}\")\n",
    "\n",
    "    return expanded_gdf\n",
    "\n",
    "\n",
    "def create_interval_counts(gdf: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group tweets by time intervals and scale level for aggregation.\n",
    "    \"\"\"\n",
    "    grouped = gdf.groupby([\"unix_timestamp\", \"scale_level\", \"matched_name\"])\n",
    "    interval_counts = grouped.agg({\"matched_geom\": \"first\"}).reset_index()\n",
    "\n",
    "    count_series = grouped.size()\n",
    "    interval_counts[\"count\"] = count_series.values\n",
    "\n",
    "    interval_counts = interval_counts.sort_values(\"unix_timestamp\")\n",
    "\n",
    "    interval_counts[\"cumulative_count\"] = (\n",
    "        interval_counts.groupby([\"scale_level\", \"matched_name\"])[\"count\"].cumsum()\n",
    "    )\n",
    "\n",
    "    return interval_counts\n",
    "\n",
    "\n",
    "print(\"Geographic matching functions defined (2/3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Rasterization Functions (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_master_grid(\n",
    "    francine_gdf: gpd.GeoDataFrame,\n",
    "    helene_gdf: gpd.GeoDataFrame,\n",
    "    states_gdf: gpd.GeoDataFrame,\n",
    "    counties_gdf: gpd.GeoDataFrame,\n",
    "    cities_gdf: gpd.GeoDataFrame,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create the master grid canvas and projected geometry lookups.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CREATING MASTER GRID CANVAS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"\\nProjecting datasets to {TARGET_CRS}...\")\n",
    "    francine_proj = francine_gdf.to_crs(TARGET_CRS)\n",
    "    helene_proj = helene_gdf.to_crs(TARGET_CRS)\n",
    "\n",
    "    states_proj = states_gdf.to_crs(TARGET_CRS)\n",
    "    counties_proj = counties_gdf.to_crs(TARGET_CRS)\n",
    "    cities_proj = cities_gdf.to_crs(TARGET_CRS)\n",
    "\n",
    "    print(\"\\nCalculating master extent...\")\n",
    "    francine_bounds = francine_proj.total_bounds\n",
    "    helene_bounds = helene_proj.total_bounds\n",
    "\n",
    "    minx = min(francine_bounds[0], helene_bounds[0])\n",
    "    miny = min(francine_bounds[1], helene_bounds[1])\n",
    "    maxx = max(francine_bounds[2], helene_bounds[2])\n",
    "    maxy = max(francine_bounds[3], helene_bounds[3])\n",
    "\n",
    "    width = int(np.ceil((maxx - minx) / CELL_SIZE_M))\n",
    "    height = int(np.ceil((maxy - miny) / CELL_SIZE_M))\n",
    "\n",
    "    print(f\"\\nGrid Configuration:\")\n",
    "    print(f\"  Cell size: {CELL_SIZE_M:,} meters\")\n",
    "    print(f\"  Grid dimensions: {width} x {height} cells\")\n",
    "    print(f\"  Total cells: {width * height:,}\")\n",
    "\n",
    "    master_transform = from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "    area_km2 = (width * height * CELL_SIZE_M * CELL_SIZE_M) / 1_000_000\n",
    "    print(f\"\\nCoverage area: {area_km2:,.2f} kmÂ²\")\n",
    "\n",
    "    print(\"\\nCreating projected geometry lookups...\")\n",
    "    state_lookup_proj = dict(zip(states_proj[\"NAME\"].str.upper(), states_proj.geometry))\n",
    "    county_lookup_proj = dict(zip(counties_proj[\"NAME\"].str.upper(), counties_proj.geometry))\n",
    "    cities_lookup_proj = dict(zip(cities_proj[\"NAME\"].str.upper(), cities_proj.geometry.centroid))\n",
    "\n",
    "    grid_params: Dict[str, Any] = {\n",
    "        \"crs\": TARGET_CRS,\n",
    "        \"cell_size\": CELL_SIZE_M,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"bounds\": (minx, miny, maxx, maxy),\n",
    "        \"transform\": master_transform,\n",
    "        \"state_lookup_proj\": state_lookup_proj,\n",
    "        \"county_lookup_proj\": county_lookup_proj,\n",
    "        \"cities_lookup_proj\": cities_lookup_proj,\n",
    "        \"francine_proj\": francine_proj,\n",
    "        \"helene_proj\": helene_proj,\n",
    "    }\n",
    "\n",
    "    print(\"\\nMaster grid canvas ready [OK]\")\n",
    "    return grid_params\n",
    "\n",
    "\n",
    "def create_facility_raster(\n",
    "    data: gpd.GeoDataFrame,\n",
    "    grid_params: Dict[str, Any],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a Gaussian kernel density-like raster for facility points.\n",
    "    \"\"\"\n",
    "    facility_grid = np.zeros((grid_params[\"height\"], grid_params[\"width\"]), dtype=np.float32)\n",
    "    facility_data = data[data[\"scale_level\"] == \"FACILITY\"]\n",
    "\n",
    "    if len(facility_data) == 0:\n",
    "        return facility_grid\n",
    "\n",
    "    facility_counts = facility_data.groupby(\"matched_name\")[\"count\"].sum()\n",
    "\n",
    "    sigma_meters = 2 * grid_params[\"cell_size\"]\n",
    "    sigma_pixels = sigma_meters / grid_params[\"cell_size\"]\n",
    "    facility_multiplier = WEIGHTS[\"FACILITY\"]\n",
    "\n",
    "    facilities_processed = 0\n",
    "    for facility_name, tweet_count in facility_counts.items():\n",
    "        facility_rows = facility_data[facility_data[\"matched_name\"] == facility_name]\n",
    "        if len(facility_rows) == 0:\n",
    "            continue\n",
    "\n",
    "        facility_point = facility_rows.iloc[0][\"matched_geom\"]\n",
    "        if hasattr(facility_point, \"x\") and hasattr(facility_point, \"y\"):\n",
    "            point_geoseries = gpd.GeoSeries([facility_point], crs=\"EPSG:4326\")\n",
    "            point_proj = point_geoseries.to_crs(grid_params[\"crs\"]).iloc[0]\n",
    "\n",
    "            px = (point_proj.x - grid_params[\"bounds\"][0]) / grid_params[\"cell_size\"]\n",
    "            py = (grid_params[\"bounds\"][3] - point_proj.y) / grid_params[\"cell_size\"]\n",
    "\n",
    "            if 0 <= px < grid_params[\"width\"] and 0 <= py < grid_params[\"height\"]:\n",
    "                point_grid = np.zeros((grid_params[\"height\"], grid_params[\"width\"]), dtype=np.float32)\n",
    "                point_grid[int(py), int(px)] = float(tweet_count)\n",
    "\n",
    "                kernel_grid = gaussian_filter(point_grid, sigma=sigma_pixels, mode=\"constant\", cval=0.0)\n",
    "                facility_grid += kernel_grid * facility_multiplier\n",
    "                facilities_processed += 1\n",
    "\n",
    "    return facility_grid\n",
    "\n",
    "\n",
    "def create_city_kde_raster(\n",
    "    city_data: pd.DataFrame,\n",
    "    cities_lookup_proj: Dict[str, Any],\n",
    "    grid_params: Dict[str, Any]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a raster layer for cities using Kernel Density Estimation (KDE).\n",
    "    \"\"\"\n",
    "    city_grid = np.zeros((grid_params[\"height\"], grid_params[\"width\"]), dtype=np.float32)\n",
    "\n",
    "    city_counts = city_data.groupby(\"matched_name\")[\"count\"].sum()\n",
    "\n",
    "    if len(city_counts) == 0:\n",
    "        return city_grid\n",
    "\n",
    "    sigma_meters = 3 * grid_params[\"cell_size\"]\n",
    "    sigma_pixels = sigma_meters / grid_params[\"cell_size\"]\n",
    "\n",
    "    print(f\"      Creating city KDE raster with sigma={sigma_pixels:.2f} pixels...\")\n",
    "\n",
    "    cities_processed = 0\n",
    "\n",
    "    for city_name, tweet_count in city_counts.items():\n",
    "        if city_name not in cities_lookup_proj:\n",
    "            continue\n",
    "\n",
    "        city_point = cities_lookup_proj[city_name]\n",
    "\n",
    "        if not hasattr(city_point, 'x') or not hasattr(city_point, 'y'):\n",
    "            continue\n",
    "\n",
    "        px = (city_point.x - grid_params[\"bounds\"][0]) / grid_params[\"cell_size\"]\n",
    "        py = (grid_params[\"bounds\"][3] - city_point.y) / grid_params[\"cell_size\"]\n",
    "\n",
    "        if 0 <= px < grid_params[\"width\"] and 0 <= py < grid_params[\"height\"]:\n",
    "            point_grid = np.zeros((grid_params[\"height\"], grid_params[\"width\"]), dtype=np.float32)\n",
    "            point_grid[int(py), int(px)] = tweet_count\n",
    "\n",
    "            kernel_grid = gaussian_filter(point_grid, sigma=sigma_pixels, mode='constant', cval=0)\n",
    "\n",
    "            city_grid += kernel_grid * np.log1p(float(tweet_count)) * WEIGHTS[\"CITY\"]\n",
    "\n",
    "            cities_processed += 1\n",
    "\n",
    "    print(f\"        Processed {cities_processed}/{len(city_counts)} cities\")\n",
    "    if np.max(city_grid) > 0:\n",
    "        print(f\"        City KDE max: {np.max(city_grid):.2f}\")\n",
    "\n",
    "    return city_grid\n",
    "\n",
    "\n",
    "print(\"Rasterization functions defined (1/2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Rasterization Functions (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchical_rasters(\n",
    "    data: gpd.GeoDataFrame,\n",
    "    grid_params: Dict[str, Any],\n",
    "    time_bin: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create hierarchically weighted rasters with automatic parent state inclusion.\n",
    "    \"\"\"\n",
    "    output_grid = np.zeros((grid_params[\"height\"], grid_params[\"width\"]), dtype=np.float32)\n",
    "    states_to_include: set[str] = set()\n",
    "\n",
    "    state_lookup_proj = grid_params[\"state_lookup_proj\"]\n",
    "    county_lookup_proj = grid_params[\"county_lookup_proj\"]\n",
    "    cities_lookup_proj = grid_params[\"cities_lookup_proj\"]\n",
    "\n",
    "    # Identify states\n",
    "    state_data = data[data[\"scale_level\"] == \"STATE\"]\n",
    "    if len(state_data) > 0:\n",
    "        states_to_include.update(state_data[\"matched_name\"].unique())\n",
    "\n",
    "    county_data = data[data[\"scale_level\"] == \"COUNTY\"]\n",
    "    for county_name in county_data[\"matched_name\"].unique():\n",
    "        if county_name in county_lookup_proj:\n",
    "            county_geom = county_lookup_proj[county_name]\n",
    "            for state_name, state_geom in state_lookup_proj.items():\n",
    "                if state_geom.contains(county_geom.centroid):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "\n",
    "    city_data = data[data[\"scale_level\"] == \"CITY\"]\n",
    "    for city_name in city_data[\"matched_name\"].unique():\n",
    "        if city_name in cities_lookup_proj:\n",
    "            city_geom = cities_lookup_proj[city_name]\n",
    "            for state_name, state_geom in state_lookup_proj.items():\n",
    "                if state_geom.contains(city_geom.centroid):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "\n",
    "    # Rasterize STATES\n",
    "    for state_name in states_to_include:\n",
    "        if state_name in state_lookup_proj:\n",
    "            state_geom = state_lookup_proj[state_name]\n",
    "            mask = rasterize(\n",
    "                [(state_geom, 1)],\n",
    "                out_shape=(grid_params[\"height\"], grid_params[\"width\"]),\n",
    "                transform=grid_params[\"transform\"],\n",
    "                fill=0,\n",
    "                dtype=np.float32,\n",
    "                all_touched=True,\n",
    "            )\n",
    "\n",
    "            if state_name in state_data[\"matched_name\"].values:\n",
    "                tweet_count = state_data[state_data[\"matched_name\"] == state_name][\"count\"].sum()\n",
    "            else:\n",
    "                tweet_count = 1\n",
    "\n",
    "            base_value = np.log1p(float(tweet_count)) * WEIGHTS[\"STATE\"]\n",
    "            output_grid += mask * base_value\n",
    "\n",
    "    # Rasterize COUNTIES\n",
    "    if len(county_data) > 0:\n",
    "        county_counts = county_data.groupby(\"matched_name\")[\"count\"].sum()\n",
    "        for county_name, tweet_count in county_counts.items():\n",
    "            if county_name in county_lookup_proj:\n",
    "                mask = rasterize(\n",
    "                    [(county_lookup_proj[county_name], 1)],\n",
    "                    out_shape=(grid_params[\"height\"], grid_params[\"width\"]),\n",
    "                    transform=grid_params[\"transform\"],\n",
    "                    fill=0,\n",
    "                    dtype=np.float32,\n",
    "                    all_touched=True,\n",
    "                )\n",
    "                output_grid += mask * np.log1p(float(tweet_count)) * WEIGHTS[\"COUNTY\"]\n",
    "\n",
    "    # Rasterize CITIES using KDE\n",
    "    if len(city_data) > 0:\n",
    "        city_grid = create_city_kde_raster(city_data, cities_lookup_proj, grid_params)\n",
    "        output_grid += city_grid\n",
    "\n",
    "    # Add FACILITIES\n",
    "    facility_data = data[data[\"scale_level\"] == \"FACILITY\"]\n",
    "    if len(facility_data) > 0:\n",
    "        output_grid += create_facility_raster(data, grid_params)\n",
    "\n",
    "    return output_grid\n",
    "\n",
    "\n",
    "def save_raster(\n",
    "    grid: np.ndarray,\n",
    "    output_dir: str,\n",
    "    hurricane_name: str,\n",
    "    time_bin: int,\n",
    "    raster_type: str,\n",
    "    timestamp_dict: Dict[int, pd.Timestamp],\n",
    "    grid_params: Dict[str, Any],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save a single-band raster to GTiff in a type-specific subfolder.\n",
    "    \"\"\"\n",
    "    type_dir = os.path.join(output_dir, raster_type)\n",
    "    os.makedirs(type_dir, exist_ok=True)\n",
    "\n",
    "    time_str = timestamp_dict[time_bin].strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{hurricane_name}_tweets_{time_str}.tif\"\n",
    "    filepath = os.path.join(type_dir, filename)\n",
    "\n",
    "    with rasterio.open(\n",
    "        filepath,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=grid_params[\"height\"],\n",
    "        width=grid_params[\"width\"],\n",
    "        count=1,\n",
    "        dtype=grid.dtype,\n",
    "        crs=grid_params[\"crs\"],\n",
    "        transform=grid_params[\"transform\"],\n",
    "        compress=\"lzw\",\n",
    "    ) as dst:\n",
    "        dst.write(grid, 1)\n",
    "\n",
    "    print(f\"    Saved: {raster_type}/{filename}\")\n",
    "\n",
    "\n",
    "def process_hurricane(\n",
    "    hurricane_name: str,\n",
    "    gdf_proj: gpd.GeoDataFrame,\n",
    "    interval_counts: pd.DataFrame,\n",
    "    time_bins: list,\n",
    "    timestamp_dict: Dict[int, pd.Timestamp],\n",
    "    grid_params: Dict[str, Any],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Process a single hurricane over all time bins and write rasters.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"PROCESSING: {hurricane_name.upper()}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    hurricane_dir = os.path.join(OUTPUT_DIR, hurricane_name.lower())\n",
    "    os.makedirs(hurricane_dir, exist_ok=True)\n",
    "\n",
    "    cumulative_grid = np.zeros((grid_params[\"height\"], grid_params[\"width\"]), dtype=np.float32)\n",
    "\n",
    "    for idx, time_bin in enumerate(time_bins):\n",
    "        print(f\"\\nTime Bin {idx + 1}/{len(time_bins)}\")\n",
    "\n",
    "        current_data = interval_counts[interval_counts[\"unix_timestamp\"] == time_bin]\n",
    "        tweet_count = len(current_data)\n",
    "        print(f\"  Tweets in this bin: {tweet_count}\")\n",
    "\n",
    "        incremental_grid = create_hierarchical_rasters(current_data, grid_params, time_bin)\n",
    "        cumulative_grid += incremental_grid\n",
    "\n",
    "        save_raster(\n",
    "            incremental_grid,\n",
    "            hurricane_dir,\n",
    "            hurricane_name,\n",
    "            time_bin,\n",
    "            \"increment\",\n",
    "            timestamp_dict,\n",
    "            grid_params,\n",
    "        )\n",
    "        save_raster(\n",
    "            cumulative_grid,\n",
    "            hurricane_dir,\n",
    "            hurricane_name,\n",
    "            time_bin,\n",
    "            \"cumulative\",\n",
    "            timestamp_dict,\n",
    "            grid_params,\n",
    "        )\n",
    "\n",
    "        print(f\"  Incremental max value: {np.max(incremental_grid):.2f}\")\n",
    "        print(f\"  Cumulative  max value: {np.max(cumulative_grid):.2f}\")\n",
    "\n",
    "    print(f\"\\n{hurricane_name.upper()} processing complete!\")\n",
    "    return hurricane_dir\n",
    "\n",
    "\n",
    "print(\"Rasterization functions defined (2/2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PIPELINE EXECUTION\n",
    "\n",
    "Run the following cells sequentially to execute the full pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Hurricane Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "francine_gdf, helene_gdf = load_hurricane_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build Time Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "francine_dict, helene_dict = create_timestamp_dictionaries(francine_gdf, helene_gdf)\n",
    "print(\"Timestamp dictionaries created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load Reference Shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_gdf, counties_gdf, cities_gdf = load_reference_shapefiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Hierarchical Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups = create_hierarchical_lookups(states_gdf, counties_gdf, cities_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Expand Tweets by Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "francine_gdf = expand_tweets_by_matches(francine_gdf, lookups, \"FRANCINE\")\n",
    "helene_gdf = expand_tweets_by_matches(helene_gdf, lookups, \"HELENE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create Interval Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "francine_interval_counts = create_interval_counts(francine_gdf)\n",
    "helene_interval_counts = create_interval_counts(helene_gdf)\n",
    "print(\"Interval counts created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Collect Time Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "francine_time_bins = get_time_bins(francine_gdf)\n",
    "helene_time_bins = get_time_bins(helene_gdf)\n",
    "print(f\"Francine time bins: {len(francine_time_bins)}\")\n",
    "print(f\"Helene time bins: {len(helene_time_bins)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Build Master Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = create_master_grid(\n",
    "    francine_gdf, helene_gdf, states_gdf, counties_gdf, cities_gdf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Ensure Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory ready: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Process Hurricane Francine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "francine_output = process_hurricane(\n",
    "    hurricane_name='francine',\n",
    "    gdf_proj=grid_params['francine_proj'],\n",
    "    interval_counts=francine_interval_counts,\n",
    "    time_bins=francine_time_bins,\n",
    "    timestamp_dict=francine_dict,\n",
    "    grid_params=grid_params,\n",
    ")\n",
    "print(f\"\\nFrancine output: {francine_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Process Hurricane Helene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helene_output = process_hurricane(\n",
    "    hurricane_name='helene',\n",
    "    gdf_proj=grid_params['helene_proj'],\n",
    "    interval_counts=helene_interval_counts,\n",
    "    time_bins=helene_time_bins,\n",
    "    timestamp_dict=helene_dict,\n",
    "    grid_params=grid_params,\n",
    ")\n",
    "print(f\"\\nHelene output: {helene_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nOutput Directories:\")\n",
    "print(f\"  Francine: {francine_output}\")\n",
    "print(f\"  Helene:   {helene_output}\")\n",
    "\n",
    "# Count files\n",
    "francine_inc = len(glob.glob(os.path.join(francine_output, 'increment', '*.tif')))\n",
    "francine_cum = len(glob.glob(os.path.join(francine_output, 'cumulative', '*.tif')))\n",
    "helene_inc = len(glob.glob(os.path.join(helene_output, 'increment', '*.tif')))\n",
    "helene_cum = len(glob.glob(os.path.join(helene_output, 'cumulative', '*.tif')))\n",
    "\n",
    "print(f\"\\nRasters Created:\")\n",
    "print(f\"  Francine: {francine_inc} incremental + {francine_cum} cumulative\")\n",
    "print(f\"  Helene:   {helene_inc} incremental + {helene_cum} cumulative\")\n",
    "print(f\"  Total:    {francine_inc + francine_cum + helene_inc + helene_cum} GeoTIFF files\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Add raster datasets to ArcGIS Pro map\")\n",
    "print(f\"  2. Configure symbology (stretch, color ramp)\")\n",
    "print(f\"  3. Enable time slider for animation\")\n",
    "print(f\"  4. Export animations as needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional: Quick Visualization\n",
    "\n",
    "View a sample raster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "sample_raster = glob.glob(os.path.join(francine_output, 'cumulative', '*.tif'))[0]\n",
    "\n",
    "with rasterio.open(sample_raster) as src:\n",
    "    data = src.read(1)\n",
    "    \n",
    "    data_masked = np.ma.masked_equal(data, 0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(data_masked, cmap='YlOrRd', interpolation='nearest')\n",
    "    plt.colorbar(label='Tweet Activity')\n",
    "    plt.title(f'Sample: {os.path.basename(sample_raster)}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Min (non-zero): {data[data > 0].min():.2f}\")\n",
    "    print(f\"  Max: {data.max():.2f}\")\n",
    "    print(f\"  Mean (non-zero): {data[data > 0].mean():.2f}\")\n",
    "    print(f\"  Non-zero cells: {np.count_nonzero(data):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
