{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Processing with ArcPy - CORRECT METHODOLOGY\n",
    "\n",
    "**Key Understanding**: The GeoJSON coordinates are DERIVED from GPE text, not original tweet locations!\n",
    "\n",
    "**Methodology**:\n",
    "1. Extract GPE text field from GeoJSON (ignore geometry - it's just geocoded GPE)\n",
    "2. Parse GPE text and fuzzy match to reference geographies\n",
    "3. Count mentions and merge to reference geography polygons\n",
    "4. Export with temporal binning\n",
    "\n",
    "**No spatial operations needed** - this is pure text matching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace: C:\\Users\\colto\\Documents\\tw_project\\tw_project\\data\\tw_project.gdb\n",
      "ArcGIS Pro: 3.5\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SETUP\n",
    "# ==============================================================================\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "project_root = os.getcwd()\n",
    "gdb_path = os.path.join(project_root, 'data', 'tw_project.gdb')\n",
    "\n",
    "if not arcpy.Exists(gdb_path):\n",
    "    arcpy.management.CreateFileGDB(os.path.dirname(gdb_path), os.path.basename(gdb_path))\n",
    "\n",
    "arcpy.env.workspace = gdb_path\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "print(f\"Workspace: {gdb_path}\")\n",
    "print(f\"ArcGIS Pro: {arcpy.GetInstallInfo()['Version']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tweets from GeoJSON...\n",
      "\n",
      "Loaded 3007 tweets\n",
      "\n",
      "Sample tweets:\n",
      "  1. GPE: 'Florida' | Time: 2024-09-26 22:59:53+00:00\n",
      "  2. GPE: 'Florida' | Time: 2024-09-26 22:59:49+00:00\n",
      "  3. GPE: 'Florida' | Time: 2024-09-26 22:59:47+00:00\n",
      "  4. GPE: 'Tallahassee, Tampa' | Time: 2024-09-26 22:59:43+00:00\n",
      "  5. GPE: 'Florida' | Time: 2024-09-26 22:59:37+00:00\n",
      "\n",
      "✓ Tweets loaded (GPE text only, geometry ignored)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# LOAD TWEETS FROM GEOJSON (Extract GPE text only)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading tweets from GeoJSON...\\n\")\n",
    "\n",
    "helene_geojson = os.path.join(project_root, 'data', 'geojson', 'helene.geojson')\n",
    "\n",
    "# Load GeoJSON and extract just GPE and time fields\n",
    "tweets_data = []\n",
    "with open(helene_geojson, 'r') as f:\n",
    "    geojson = json.load(f)\n",
    "    \n",
    "for feature in geojson['features']:\n",
    "    props = feature['properties']\n",
    "    tweets_data.append({\n",
    "        'GPE': props.get('GPE', ''),\n",
    "        'time': props.get('time', ''),\n",
    "        'FAC': props.get('FAC', ''),\n",
    "        'LOC': props.get('LOC', '')\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(tweets_data)} tweets\")\n",
    "print(f\"\\nSample tweets:\")\n",
    "for i in range(min(5, len(tweets_data))):\n",
    "    print(f\"  {i+1}. GPE: '{tweets_data[i]['GPE']}' | Time: {tweets_data[i]['time']}\")\n",
    "\n",
    "print(f\"\\n✓ Tweets loaded (GPE text only, geometry ignored)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Text parsing functions loaded\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TEXT PARSING FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def preprocess_place_name(name):\n",
    "    \"\"\"Standardize place names for matching\"\"\"\n",
    "    if not name or str(name).strip() == '' or str(name).upper() == 'NAN':\n",
    "        return None\n",
    "    name = str(name).upper().strip()\n",
    "    name = re.sub(r'\\bST\\.?\\b', 'SAINT', name)\n",
    "    name = re.sub(r'\\bMT\\.?\\b', 'MOUNT', name)\n",
    "    name = re.sub(r'\\bFT\\.?\\b', 'FORT', name)\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    return name.strip()\n",
    "\n",
    "def parse_gpe_entities(gpe_string):\n",
    "    \"\"\"Split GPE field into individual place mentions\"\"\"\n",
    "    if not gpe_string or str(gpe_string).strip() == '':\n",
    "        return []\n",
    "    gpe_string = str(gpe_string).strip()\n",
    "    entities = []\n",
    "    for part in [p.strip() for p in gpe_string.split(',')]:\n",
    "        if not part:\n",
    "            continue\n",
    "        for sub in re.split(r'[;&|]', part):\n",
    "            sub = preprocess_place_name(sub)\n",
    "            if sub and len(sub) > 1:\n",
    "                entities.append(sub)\n",
    "    # Remove duplicates\n",
    "    seen, clean = set(), []\n",
    "    for e in entities:\n",
    "        if e not in seen:\n",
    "            clean.append(e)\n",
    "            seen.add(e)\n",
    "    return clean\n",
    "\n",
    "def simple_fuzzy_match(query, target):\n",
    "    \"\"\"Simple fuzzy matching\"\"\"\n",
    "    query = query.upper()\n",
    "    target = target.upper()\n",
    "    \n",
    "    if query == target:\n",
    "        return 1.0\n",
    "    if query in target or target in query:\n",
    "        return 0.9\n",
    "    \n",
    "    q_set = set(query)\n",
    "    t_set = set(target)\n",
    "    intersection = len(q_set & t_set)\n",
    "    union = len(q_set | t_set)\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def match_entity(entity, lookup_dict, threshold=0.85):\n",
    "    \"\"\"Match entity to lookup dictionary\"\"\"\n",
    "    if entity in lookup_dict:\n",
    "        return lookup_dict[entity], 1.0\n",
    "    \n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for key in lookup_dict.keys():\n",
    "        score = simple_fuzzy_match(entity, key)\n",
    "        if score > best_score and score >= threshold:\n",
    "            best_score = score\n",
    "            best_match = key\n",
    "    \n",
    "    if best_match:\n",
    "        return lookup_dict[best_match], best_score\n",
    "    \n",
    "    return None, 0\n",
    "\n",
    "print(\"✓ Text parsing functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reference geographies...\n",
      "\n",
      "States: 52\n",
      "Counties: 3222\n",
      "Cities: 17244\n",
      "\n",
      "✓ Reference geography loaded\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# LOAD REFERENCE GEOGRAPHY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading reference geographies...\\n\")\n",
    "\n",
    "# States\n",
    "states_shp = os.path.join(project_root, 'data', 'shape_files', 'cb_2023_us_state_20m.shp')\n",
    "states_fc = os.path.join(gdb_path, 'us_states')\n",
    "if arcpy.Exists(states_fc):\n",
    "    arcpy.management.Delete(states_fc)\n",
    "arcpy.conversion.FeatureClassToFeatureClass(states_shp, gdb_path, 'us_states')\n",
    "print(f\"States: {arcpy.management.GetCount(states_fc)[0]}\")\n",
    "\n",
    "# Counties  \n",
    "counties_shp = os.path.join(project_root, 'data', 'shape_files', 'cb_2023_us_county_20m.shp')\n",
    "counties_fc = os.path.join(gdb_path, 'us_counties')\n",
    "if arcpy.Exists(counties_fc):\n",
    "    arcpy.management.Delete(counties_fc)\n",
    "arcpy.conversion.FeatureClassToFeatureClass(counties_shp, gdb_path, 'us_counties')\n",
    "print(f\"Counties: {arcpy.management.GetCount(counties_fc)[0]}\")\n",
    "\n",
    "# Cities\n",
    "cities_csv = os.path.join(project_root, 'data', 'tables', 'cities1000.csv')\n",
    "cities_temp = os.path.join(gdb_path, 'us_cities_temp')\n",
    "cities_fc = os.path.join(gdb_path, 'us_cities')\n",
    "\n",
    "if arcpy.Exists(cities_temp):\n",
    "    arcpy.management.Delete(cities_temp)\n",
    "if arcpy.Exists(cities_fc):\n",
    "    arcpy.management.Delete(cities_fc)\n",
    "\n",
    "arcpy.management.XYTableToPoint(cities_csv, cities_temp, 'longitude', 'latitude',\n",
    "                                coordinate_system=arcpy.SpatialReference(4326))\n",
    "arcpy.analysis.Select(cities_temp, cities_fc,\n",
    "                     \"country_code = 'US' AND feature_class = 'P' AND population IS NOT NULL\")\n",
    "arcpy.management.Delete(cities_temp)\n",
    "print(f\"Cities: {arcpy.management.GetCount(cities_fc)[0]}\")\n",
    "\n",
    "print(\"\\n✓ Reference geography loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building lookup dictionaries...\n",
      "\n",
      "States: 104 entries\n",
      "Counties: 1915 entries\n",
      "Cities: 12256 entries\n",
      "\n",
      "✓ Lookup dictionaries created\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# BUILD LOOKUP DICTIONARIES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Building lookup dictionaries...\\n\")\n",
    "\n",
    "state_lookup = {}  # name -> STUSPS\n",
    "with arcpy.da.SearchCursor(states_fc, ['NAME', 'STUSPS']) as cursor:\n",
    "    for row in cursor:\n",
    "        name_clean = preprocess_place_name(row[0])\n",
    "        if name_clean:\n",
    "            state_lookup[name_clean] = row[1]\n",
    "        # Also add abbreviation\n",
    "        state_lookup[row[1]] = row[1]\n",
    "\n",
    "county_lookup = {}  # name -> GEOID\n",
    "with arcpy.da.SearchCursor(counties_fc, ['NAME', 'GEOID']) as cursor:\n",
    "    for row in cursor:\n",
    "        name_clean = preprocess_place_name(row[0])\n",
    "        if name_clean:\n",
    "            county_lookup[name_clean] = row[1]\n",
    "\n",
    "city_lookup = {}  # name -> geonameid\n",
    "with arcpy.da.SearchCursor(cities_fc, ['name', 'geonameid']) as cursor:\n",
    "    for row in cursor:\n",
    "        name_clean = preprocess_place_name(row[0])\n",
    "        if name_clean:\n",
    "            city_lookup[name_clean] = row[1]\n",
    "\n",
    "print(f\"States: {len(state_lookup)} entries\")\n",
    "print(f\"Counties: {len(county_lookup)} entries\")\n",
    "print(f\"Cities: {len(city_lookup)} entries\")\n",
    "print(\"\\n✓ Lookup dictionaries created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating time bins...\n",
      "\n",
      "Found 11 time bins\n",
      "Range: 2024-09-26 00:00:00+00:00 to 2024-09-27 16:00:00+00:00\n",
      "\n",
      "Time bins:\n",
      "  2024-09-26 00:00:00\n",
      "  2024-09-26 04:00:00\n",
      "  2024-09-26 08:00:00\n",
      "  2024-09-26 12:00:00\n",
      "  2024-09-26 16:00:00\n",
      "  2024-09-26 20:00:00\n",
      "  2024-09-27 00:00:00\n",
      "  2024-09-27 04:00:00\n",
      "  2024-09-27 08:00:00\n",
      "  2024-09-27 12:00:00\n",
      "  2024-09-27 16:00:00\n",
      "\n",
      "✓ Time bins created\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TIME BINNING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Creating time bins...\\n\")\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Parse times and create bins\n",
    "for tweet in tweets_data:\n",
    "    if tweet['time']:\n",
    "        dt = pd.to_datetime(tweet['time'])\n",
    "        # Floor to 4-hour boundary\n",
    "        bin_time = dt.floor('4h')\n",
    "        tweet['bin'] = bin_time\n",
    "        tweet['bin_str'] = bin_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        tweet['bin'] = None\n",
    "        tweet['bin_str'] = None\n",
    "\n",
    "# Get unique bins\n",
    "time_bins = sorted(list(set([t['bin'] for t in tweets_data if t['bin'] is not None])))\n",
    "\n",
    "print(f\"Found {len(time_bins)} time bins\")\n",
    "if len(time_bins) > 0:\n",
    "    print(f\"Range: {time_bins[0]} to {time_bins[-1]}\")\n",
    "    print(f\"\\nTime bins:\")\n",
    "    for tb in time_bins:\n",
    "        print(f\"  {tb.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\n✓ Time bins created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COUNTING TEXT MENTIONS\n",
      "======================================================================\n",
      "\n",
      "  Processing 100/3007...\n",
      "  Processing 200/3007...\n",
      "  Processing 300/3007...\n",
      "  Processing 400/3007...\n",
      "  Processing 500/3007...\n",
      "  Processing 600/3007...\n",
      "  Processing 700/3007...\n",
      "  Processing 800/3007...\n",
      "  Processing 900/3007...\n",
      "  Processing 1000/3007...\n",
      "  Processing 1100/3007...\n",
      "  Processing 1200/3007...\n",
      "  Processing 1300/3007...\n",
      "  Processing 1400/3007...\n",
      "  Processing 1500/3007...\n",
      "  Processing 1600/3007...\n",
      "  Processing 1700/3007...\n",
      "  Processing 1800/3007...\n",
      "  Processing 1900/3007...\n",
      "  Processing 2000/3007...\n",
      "  Processing 2100/3007...\n",
      "  Processing 2200/3007...\n",
      "  Processing 2300/3007...\n",
      "  Processing 2400/3007...\n",
      "  Processing 2500/3007...\n",
      "  Processing 2600/3007...\n",
      "  Processing 2700/3007...\n",
      "  Processing 2800/3007...\n",
      "  Processing 2900/3007...\n",
      "  Processing 3000/3007...\n",
      "\n",
      "✓ Processed all 3007 tweets\n",
      "\n",
      "======================================================================\n",
      "COUNTING COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# COUNT MENTIONS BY TIME BIN (Pure text matching - NO spatial operations)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COUNTING TEXT MENTIONS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Initialize temporal tracking\n",
    "temporal_state_data = {tb: defaultdict(lambda: {'count': 0, 'samples': []}) for tb in time_bins}\n",
    "temporal_county_data = {tb: defaultdict(lambda: {'count': 0, 'samples': []}) for tb in time_bins}\n",
    "temporal_city_data = {tb: defaultdict(lambda: {'count': 0, 'samples': []}) for tb in time_bins}\n",
    "\n",
    "processed = 0\n",
    "total_tweets = len(tweets_data)\n",
    "\n",
    "for tweet in tweets_data:\n",
    "    processed += 1\n",
    "    if processed % 100 == 0:\n",
    "        print(f\"  Processing {processed}/{total_tweets}...\")\n",
    "    \n",
    "    time_bin = tweet['bin']\n",
    "    if not time_bin:\n",
    "        continue\n",
    "    \n",
    "    gpe_text = tweet['GPE']\n",
    "    entities = parse_gpe_entities(gpe_text)\n",
    "    \n",
    "    for entity in entities:\n",
    "        # Try state match\n",
    "        state_match, score = match_entity(entity, state_lookup, 0.90)\n",
    "        if state_match:\n",
    "            temporal_state_data[time_bin][state_match]['count'] += 1\n",
    "            temporal_state_data[time_bin][state_match]['samples'].append({\n",
    "                'entity': entity,\n",
    "                'gpe': gpe_text[:100] if gpe_text else ''\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Try county match\n",
    "        county_match, score = match_entity(entity, county_lookup, 0.85)\n",
    "        if county_match:\n",
    "            temporal_county_data[time_bin][county_match]['count'] += 1\n",
    "            temporal_county_data[time_bin][county_match]['samples'].append({\n",
    "                'entity': entity,\n",
    "                'gpe': gpe_text[:100] if gpe_text else ''\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Try city match\n",
    "        city_match, score = match_entity(entity, city_lookup, 0.85)\n",
    "        if city_match:\n",
    "            temporal_city_data[time_bin][city_match]['count'] += 1\n",
    "            temporal_city_data[time_bin][city_match]['samples'].append({\n",
    "                'entity': entity,\n",
    "                'gpe': gpe_text[:100] if gpe_text else ''\n",
    "            })\n",
    "\n",
    "print(f\"\\n✓ Processed all {processed} tweets\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COUNTING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VALIDATION\n",
      "======================================================================\n",
      "\n",
      "States mentioned: 35\n",
      "\n",
      "Top 10 states:\n",
      "  FL: 2166\n",
      "  GA: 377\n",
      "  LA: 277\n",
      "  PA: 137\n",
      "  OR: 115\n",
      "  MA: 82\n",
      "  NC: 70\n",
      "  IN: 65\n",
      "  CO: 59\n",
      "  SC: 55\n",
      "\n",
      "Expected (from test.ipynb):\n",
      "  FL: 2,156\n",
      "  GA: 369\n",
      "  NC: 69\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# VALIDATION: Display counts\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Total counts\n",
    "total_state_counts = defaultdict(int)\n",
    "for tb in time_bins:\n",
    "    for state_code, data in temporal_state_data[tb].items():\n",
    "        total_state_counts[state_code] += data['count']\n",
    "\n",
    "print(f\"States mentioned: {len(total_state_counts)}\")\n",
    "print(f\"\\nTop 10 states:\")\n",
    "top_states = sorted(total_state_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for state_code, count in top_states:\n",
    "    print(f\"  {state_code}: {count}\")\n",
    "\n",
    "print(f\"\\nExpected (from test.ipynb):\")\n",
    "print(f\"  FL: 2,156\")\n",
    "print(f\"  GA: 369\")\n",
    "print(f\"  NC: 69\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXPORTING TEMPORAL FEATURE CLASSES\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Exporting states...\n",
      "  Bin 1/11: 2024-09-26 00:00:00\n",
      "  Bin 2/11: 2024-09-26 04:00:00\n",
      "  Bin 3/11: 2024-09-26 08:00:00\n",
      "  Bin 4/11: 2024-09-26 12:00:00\n",
      "  Bin 5/11: 2024-09-26 16:00:00\n",
      "  Bin 6/11: 2024-09-26 20:00:00\n",
      "  Bin 7/11: 2024-09-27 00:00:00\n",
      "  Bin 8/11: 2024-09-27 04:00:00\n",
      "  Bin 9/11: 2024-09-27 08:00:00\n",
      "  Bin 10/11: 2024-09-27 12:00:00\n",
      "  Bin 11/11: 2024-09-27 16:00:00\n",
      "\n",
      "Exporting counties...\n",
      "  Bin 1/11: 2024-09-26 00:00:00\n",
      "  Bin 2/11: 2024-09-26 04:00:00\n",
      "  Bin 3/11: 2024-09-26 08:00:00\n",
      "  Bin 4/11: 2024-09-26 12:00:00\n",
      "  Bin 5/11: 2024-09-26 16:00:00\n",
      "  Bin 6/11: 2024-09-26 20:00:00\n",
      "  Bin 7/11: 2024-09-27 00:00:00\n",
      "  Bin 8/11: 2024-09-27 04:00:00\n",
      "  Bin 9/11: 2024-09-27 08:00:00\n",
      "  Bin 10/11: 2024-09-27 12:00:00\n",
      "  Bin 11/11: 2024-09-27 16:00:00\n",
      "\n",
      "Exporting cities...\n",
      "  Bin 1/11: 2024-09-26 00:00:00\n",
      "  Bin 2/11: 2024-09-26 04:00:00\n",
      "  Bin 3/11: 2024-09-26 08:00:00\n",
      "  Bin 4/11: 2024-09-26 12:00:00\n",
      "  Bin 5/11: 2024-09-26 16:00:00\n",
      "  Bin 6/11: 2024-09-26 20:00:00\n",
      "  Bin 7/11: 2024-09-27 00:00:00\n",
      "  Bin 8/11: 2024-09-27 04:00:00\n",
      "  Bin 9/11: 2024-09-27 08:00:00\n",
      "  Bin 10/11: 2024-09-27 12:00:00\n",
      "  Bin 11/11: 2024-09-27 16:00:00\n",
      "\n",
      "✓ Temporal feature classes created\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# EXPORT: Create temporal feature classes\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORTING TEMPORAL FEATURE CLASSES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def export_temporal(geography_fc, geography_name, id_field, temporal_data_dict):\n",
    "    print(f\"\\nExporting {geography_name}...\")\n",
    "    \n",
    "    incremental_fcs = []\n",
    "    cumulative_fcs = []\n",
    "    cumulative_counts = {}\n",
    "    \n",
    "    for idx, time_bin in enumerate(time_bins):\n",
    "        bin_str = time_bin.strftime('%Y%m%d_%H%M')\n",
    "        bin_label = time_bin.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"  Bin {idx+1}/{len(time_bins)}: {bin_label}\")\n",
    "        \n",
    "        # INCREMENTAL\n",
    "        inc_name = f\"{geography_name}_inc_{bin_str}\"\n",
    "        inc_fc = os.path.join(gdb_path, inc_name)\n",
    "        arcpy.management.Copy(geography_fc, inc_fc)\n",
    "        \n",
    "        arcpy.management.AddField(inc_fc, 'tweet_cnt', 'LONG')\n",
    "        arcpy.management.AddField(inc_fc, 'smpl_ment', 'TEXT', field_length=254)\n",
    "        arcpy.management.AddField(inc_fc, 'time_bin_str', 'TEXT', field_length=50)\n",
    "        \n",
    "        with arcpy.da.UpdateCursor(inc_fc, [id_field, 'tweet_cnt', 'smpl_ment', 'time_bin_str']) as cursor:\n",
    "            for row in cursor:\n",
    "                entity_id = row[0]\n",
    "                if entity_id in temporal_data_dict[time_bin]:\n",
    "                    data = temporal_data_dict[time_bin][entity_id]\n",
    "                    row[1] = data['count']\n",
    "                    samples = data['samples'][:3]\n",
    "                    row[2] = '; '.join([s['entity'][:50] for s in samples])\n",
    "                else:\n",
    "                    row[1] = 0\n",
    "                    row[2] = ''\n",
    "                row[3] = bin_label\n",
    "                cursor.updateRow(row)\n",
    "        \n",
    "        incremental_fcs.append(inc_fc)\n",
    "        \n",
    "        # Update cumulative\n",
    "        for entity_id, data in temporal_data_dict[time_bin].items():\n",
    "            cumulative_counts[entity_id] = cumulative_counts.get(entity_id, 0) + data['count']\n",
    "        \n",
    "        # CUMULATIVE\n",
    "        cum_name = f\"{geography_name}_cum_{bin_str}\"\n",
    "        cum_fc = os.path.join(gdb_path, cum_name)\n",
    "        arcpy.management.Copy(geography_fc, cum_fc)\n",
    "        \n",
    "        arcpy.management.AddField(cum_fc, 'cumul_cnt', 'LONG')\n",
    "        arcpy.management.AddField(cum_fc, 'time_bin_str', 'TEXT', field_length=50)\n",
    "        \n",
    "        with arcpy.da.UpdateCursor(cum_fc, [id_field, 'cumul_cnt', 'time_bin_str']) as cursor:\n",
    "            for row in cursor:\n",
    "                entity_id = row[0]\n",
    "                row[1] = cumulative_counts.get(entity_id, 0)\n",
    "                row[2] = bin_label\n",
    "                cursor.updateRow(row)\n",
    "        \n",
    "        cumulative_fcs.append(cum_fc)\n",
    "    \n",
    "    return incremental_fcs, cumulative_fcs\n",
    "\n",
    "states_inc, states_cum = export_temporal(states_fc, 'states', 'STUSPS', temporal_state_data)\n",
    "counties_inc, counties_cum = export_temporal(counties_fc, 'counties', 'GEOID', temporal_county_data)\n",
    "cities_inc, cities_cum = export_temporal(cities_fc, 'cities', 'geonameid', temporal_city_data)\n",
    "\n",
    "print(\"\\n✓ Temporal feature classes created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging master files...\n",
      "\n",
      "states_INCREMENTAL_ALL: 572 records\n",
      "counties_INCREMENTAL_ALL: 35442 records\n",
      "cities_INCREMENTAL_ALL: 189684 records\n",
      "\n",
      "states_CUMULATIVE_ALL: 572 records\n",
      "counties_CUMULATIVE_ALL: 35442 records\n",
      "cities_CUMULATIVE_ALL: 189684 records\n",
      "\n",
      "======================================================================\n",
      "✓ COMPLETE - All outputs in tw_project.gdb\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MERGE MASTER FILES\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\nMerging master files...\\n\")\n",
    "\n",
    "# Incremental\n",
    "states_inc_all = os.path.join(gdb_path, 'states_INCREMENTAL_ALL')\n",
    "arcpy.management.Merge(states_inc, states_inc_all)\n",
    "print(f\"states_INCREMENTAL_ALL: {arcpy.management.GetCount(states_inc_all)[0]} records\")\n",
    "\n",
    "counties_inc_all = os.path.join(gdb_path, 'counties_INCREMENTAL_ALL')\n",
    "arcpy.management.Merge(counties_inc, counties_inc_all)\n",
    "print(f\"counties_INCREMENTAL_ALL: {arcpy.management.GetCount(counties_inc_all)[0]} records\")\n",
    "\n",
    "cities_inc_all = os.path.join(gdb_path, 'cities_INCREMENTAL_ALL')\n",
    "arcpy.management.Merge(cities_inc, cities_inc_all)\n",
    "print(f\"cities_INCREMENTAL_ALL: {arcpy.management.GetCount(cities_inc_all)[0]} records\")\n",
    "\n",
    "# Cumulative\n",
    "states_cum_all = os.path.join(gdb_path, 'states_CUMULATIVE_ALL')\n",
    "arcpy.management.Merge(states_cum, states_cum_all)\n",
    "print(f\"\\nstates_CUMULATIVE_ALL: {arcpy.management.GetCount(states_cum_all)[0]} records\")\n",
    "\n",
    "counties_cum_all = os.path.join(gdb_path, 'counties_CUMULATIVE_ALL')\n",
    "arcpy.management.Merge(counties_cum, counties_cum_all)\n",
    "print(f\"counties_CUMULATIVE_ALL: {arcpy.management.GetCount(counties_cum_all)[0]} records\")\n",
    "\n",
    "cities_cum_all = os.path.join(gdb_path, 'cities_CUMULATIVE_ALL')\n",
    "arcpy.management.Merge(cities_cum, cities_cum_all)\n",
    "print(f\"cities_CUMULATIVE_ALL: {arcpy.management.GetCount(cities_cum_all)[0]} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ COMPLETE - All outputs in tw_project.gdb\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
