{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Processing with ArcPy - FIXED VERSION\n",
    "\n",
    "This notebook processes hurricane tweet data using **ArcPy** with **complete methodology from test.ipynb**.\n",
    "\n",
    "**FIXED Features:**\n",
    "- ✅ GPE text parsing and entity extraction\n",
    "- ✅ Fuzzy matching for place names (pure Python implementation)\n",
    "- ✅ Dual counting: Text mentions + Spatial cascade\n",
    "- ✅ Hierarchical cascade: Tweet point → County → State\n",
    "- ✅ Nearest city search (50km buffer)\n",
    "- ✅ Correct cities filter (feature_class='P', population.notna())\n",
    "- ✅ Cumulative counts persist across bins\n",
    "- ✅ Sample mention fields for validation\n",
    "- ✅ Time-binned aggregation (4-hour intervals)\n",
    "- ✅ All data in geodatabase (tw_project.gdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace set to: C:\\Users\\colto\\Documents\\tw_project\\tw_project\\data\\tw_project.gdb\n",
      "ArcGIS Pro Version: 3.5\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SETUP: Import ArcPy and Configure Environment\n",
    "# ==============================================================================\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set workspace to project geodatabase\n",
    "project_root = os.getcwd()\n",
    "gdb_path = os.path.join(project_root, 'data', 'tw_project.gdb')\n",
    "\n",
    "# Create geodatabase if it doesn't exist\n",
    "if not arcpy.Exists(gdb_path):\n",
    "    gdb_folder = os.path.dirname(gdb_path)\n",
    "    gdb_name = os.path.basename(gdb_path)\n",
    "    arcpy.management.CreateFileGDB(gdb_folder, gdb_name)\n",
    "    print(f\"Created geodatabase: {gdb_path}\")\n",
    "\n",
    "arcpy.env.workspace = gdb_path\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "print(f\"Workspace set to: {arcpy.env.workspace}\")\n",
    "print(f\"ArcGIS Pro Version: {arcpy.GetInstallInfo()['Version']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Text parsing functions loaded\n",
      "\n",
      "Test GPE: Florida, Georgia, St. Petersburg, Tampa\n",
      "Parsed: ['FLORIDA', 'GEORGIA', 'SAINT PETERSBURG', 'TAMPA']\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TEXT PARSING FUNCTIONS: GPE Entity Extraction (from test.ipynb)\n",
    "# ==============================================================================\n",
    "\n",
    "def preprocess_place_name(name):\n",
    "    \"\"\"Standardize place names for matching (from test.ipynb)\"\"\"\n",
    "    if name is None or str(name).strip() == '' or str(name).upper() == 'NAN':\n",
    "        return None\n",
    "    name = str(name).upper().strip()\n",
    "    name = re.sub(r'\\bST\\.?\\b', 'SAINT', name)\n",
    "    name = re.sub(r'\\bMT\\.?\\b', 'MOUNT', name)\n",
    "    name = re.sub(r'\\bFT\\.?\\b', 'FORT', name)\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    return name.strip()\n",
    "\n",
    "def parse_gpe_entities(gpe_string):\n",
    "    \"\"\"Split GPE field into individual place mentions (from test.ipynb)\"\"\"\n",
    "    if not gpe_string or str(gpe_string).strip() == '':\n",
    "        return []\n",
    "    gpe_string = str(gpe_string).strip()\n",
    "    entities = []\n",
    "    for part in [p.strip() for p in gpe_string.split(',')]:\n",
    "        if not part:\n",
    "            continue\n",
    "        for sub in re.split(r'[;&|]', part):\n",
    "            sub = preprocess_place_name(sub)\n",
    "            if sub and len(sub) > 1:\n",
    "                entities.append(sub)\n",
    "    # Remove duplicates while preserving order\n",
    "    seen, clean = set(), []\n",
    "    for e in entities:\n",
    "        if e not in seen:\n",
    "            clean.append(e)\n",
    "            seen.add(e)\n",
    "    return clean\n",
    "\n",
    "print(\"✓ Text parsing functions loaded\")\n",
    "\n",
    "# Test\n",
    "test_gpe = \"Florida, Georgia, St. Petersburg, Tampa\"\n",
    "print(f\"\\nTest GPE: {test_gpe}\")\n",
    "print(f\"Parsed: {parse_gpe_entities(test_gpe)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fuzzy matching functions loaded\n",
      "\n",
      "Test matches:\n",
      "  'Talahassee' → ('TLH', 0.9727272727272727)\n",
      "  'FL' → ('FL', 0.8785714285714286)\n",
      "  'Tampa Bay' → ('TPA', 0.9055555555555556)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FUZZY MATCHING: Pure Python implementation (no fuzzywuzzy dependency)\n",
    "# ==============================================================================\n",
    "\n",
    "def simple_fuzzy_match(query, target):\n",
    "    \"\"\"Simple character-based similarity matching\"\"\"\n",
    "    query = query.upper()\n",
    "    target = target.upper()\n",
    "    \n",
    "    # Exact match\n",
    "    if query == target:\n",
    "        return 1.0\n",
    "    \n",
    "    # Substring match\n",
    "    if query in target or target in query:\n",
    "        shorter = min(len(query), len(target))\n",
    "        longer = max(len(query), len(target))\n",
    "        return 0.85 + (0.1 * (shorter / longer))\n",
    "    \n",
    "    # Levenshtein-like character overlap\n",
    "    q_set = set(query)\n",
    "    t_set = set(target)\n",
    "    intersection = len(q_set & t_set)\n",
    "    union = len(q_set | t_set)\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    jaccard = intersection / union\n",
    "    \n",
    "    # Bonus for length similarity\n",
    "    len_ratio = min(len(query), len(target)) / max(len(query), len(target))\n",
    "    \n",
    "    return (jaccard * 0.7) + (len_ratio * 0.3)\n",
    "\n",
    "def match_entity(entity, lookup_dict, threshold=0.85):\n",
    "    \"\"\"Match entity to lookup dictionary with fuzzy matching\"\"\"\n",
    "    # Try exact match first\n",
    "    if entity in lookup_dict:\n",
    "        return lookup_dict[entity], 1.0\n",
    "    \n",
    "    # Try fuzzy match\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for key in lookup_dict.keys():\n",
    "        score = simple_fuzzy_match(entity, key)\n",
    "        if score > best_score and score >= threshold:\n",
    "            best_score = score\n",
    "            best_match = key\n",
    "    \n",
    "    if best_match:\n",
    "        return lookup_dict[best_match], best_score\n",
    "    \n",
    "    return None, 0\n",
    "\n",
    "print(\"✓ Fuzzy matching functions loaded\")\n",
    "\n",
    "# Test\n",
    "test_dict = {'TALLAHASSEE': 'TLH', 'TAMPA': 'TPA', 'FLORIDA': 'FL'}\n",
    "print(f\"\\nTest matches:\")\n",
    "print(f\"  'Talahassee' → {match_entity('TALAHASSEE', test_dict, 0.85)}\")\n",
    "print(f\"  'FL' → {match_entity('FL', test_dict, 0.85)}\")\n",
    "print(f\"  'Tampa Bay' → {match_entity('TAMPA BAY', test_dict, 0.85)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "\n",
      "Imported Helene tweets: 0 features\n",
      "Imported US States: 52 features\n",
      "Imported US Counties: 3222 features\n",
      "Imported US Cities (filtered): 17244 features\n",
      "\n",
      "✓ All data imported successfully\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# IMPORT DATA: Load GeoJSON and Reference Geography\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Importing data...\\n\")\n",
    "\n",
    "# Import hurricane tweet data\n",
    "helene_geojson = os.path.join(project_root, 'data', 'geojson', 'helene.geojson')\n",
    "tweets_fc = os.path.join(gdb_path, 'tweets_helene')\n",
    "\n",
    "if arcpy.Exists(tweets_fc):\n",
    "    arcpy.management.Delete(tweets_fc)\n",
    "\n",
    "arcpy.conversion.JSONToFeatures(helene_geojson, tweets_fc)\n",
    "tweet_count = int(arcpy.management.GetCount(tweets_fc)[0])\n",
    "print(f\"Imported Helene tweets: {tweet_count} features\")\n",
    "\n",
    "# Import US States\n",
    "states_shp = os.path.join(project_root, 'data', 'shape_files', 'cb_2023_us_state_20m.shp')\n",
    "states_fc = os.path.join(gdb_path, 'us_states')\n",
    "if arcpy.Exists(states_fc):\n",
    "    arcpy.management.Delete(states_fc)\n",
    "arcpy.conversion.FeatureClassToFeatureClass(states_shp, gdb_path, 'us_states')\n",
    "print(f\"Imported US States: {arcpy.management.GetCount(states_fc)[0]} features\")\n",
    "\n",
    "# Import US Counties\n",
    "counties_shp = os.path.join(project_root, 'data', 'shape_files', 'cb_2023_us_county_20m.shp')\n",
    "counties_fc = os.path.join(gdb_path, 'us_counties')\n",
    "if arcpy.Exists(counties_fc):\n",
    "    arcpy.management.Delete(counties_fc)\n",
    "arcpy.conversion.FeatureClassToFeatureClass(counties_shp, gdb_path, 'us_counties')\n",
    "print(f\"Imported US Counties: {arcpy.management.GetCount(counties_fc)[0]} features\")\n",
    "\n",
    "# Import US Cities with CORRECT FILTER (Fix #4 from QA/QC)\n",
    "cities_csv = os.path.join(project_root, 'data', 'tables', 'cities1000.csv')\n",
    "cities_temp = os.path.join(gdb_path, 'us_cities_temp')\n",
    "cities_fc = os.path.join(gdb_path, 'us_cities')\n",
    "\n",
    "if arcpy.Exists(cities_temp):\n",
    "    arcpy.management.Delete(cities_temp)\n",
    "if arcpy.Exists(cities_fc):\n",
    "    arcpy.management.Delete(cities_fc)\n",
    "\n",
    "# Create point feature class from XY data\n",
    "arcpy.management.XYTableToPoint(\n",
    "    cities_csv,\n",
    "    cities_temp,\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    coordinate_system=arcpy.SpatialReference(4326)\n",
    ")\n",
    "\n",
    "# CORRECT FILTER: US only, feature_class='P', has population\n",
    "arcpy.analysis.Select(\n",
    "    cities_temp,\n",
    "    cities_fc,\n",
    "    \"country_code = 'US' AND feature_class = 'P' AND population IS NOT NULL\"\n",
    ")\n",
    "arcpy.management.Delete(cities_temp)\n",
    "print(f\"Imported US Cities (filtered): {arcpy.management.GetCount(cities_fc)[0]} features\")\n",
    "\n",
    "print(\"\\n✓ All data imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building lookup dictionaries...\n",
      "\n",
      "  States: 104 entries\n",
      "  Counties: 1915 entries\n",
      "  Cities: 12256 entries\n",
      "\n",
      "✓ Lookup dictionaries created\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# BUILD LOOKUP DICTIONARIES: Create name→ID mappings\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Building lookup dictionaries...\\n\")\n",
    "\n",
    "# States lookup\n",
    "state_lookup = {}  # name → {'code': STUSPS, 'statefp': STATEFP}\n",
    "state_fp_lookup = {}  # STATEFP → STUSPS\n",
    "\n",
    "with arcpy.da.SearchCursor(states_fc, ['NAME', 'STUSPS', 'STATEFP']) as cursor:\n",
    "    for row in cursor:\n",
    "        name_clean = preprocess_place_name(row[0])\n",
    "        if name_clean:\n",
    "            state_lookup[name_clean] = {'code': row[1], 'statefp': row[2]}\n",
    "        # Also add abbreviation\n",
    "        state_lookup[row[1]] = {'code': row[1], 'statefp': row[2]}\n",
    "        state_fp_lookup[row[2]] = row[1]\n",
    "\n",
    "print(f\"  States: {len(state_lookup)} entries\")\n",
    "\n",
    "# Counties lookup\n",
    "county_lookup = {}  # name → {'geoid': GEOID, 'statefp': STATEFP}\n",
    "\n",
    "with arcpy.da.SearchCursor(counties_fc, ['NAME', 'GEOID', 'STATEFP']) as cursor:\n",
    "    for row in cursor:\n",
    "        name_clean = preprocess_place_name(row[0])\n",
    "        if name_clean:\n",
    "            # County names can duplicate across states, so use name+statefp as key\n",
    "            key = f\"{name_clean}_{row[2]}\"\n",
    "            county_lookup[name_clean] = {'geoid': row[1], 'statefp': row[2]}\n",
    "\n",
    "print(f\"  Counties: {len(county_lookup)} entries\")\n",
    "\n",
    "# Cities lookup\n",
    "city_lookup = {}  # name → geonameid\n",
    "\n",
    "with arcpy.da.SearchCursor(cities_fc, ['name', 'geonameid']) as cursor:\n",
    "    for row in cursor:\n",
    "        name_clean = preprocess_place_name(row[0])\n",
    "        if name_clean:\n",
    "            city_lookup[name_clean] = row[1]\n",
    "\n",
    "print(f\"  Cities: {len(city_lookup)} entries\")\n",
    "\n",
    "print(\"\\n✓ Lookup dictionaries created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating time bins...\n",
      "\n",
      "⚠️  WARNING: No time bins found! Check that tweets have 'time' field.\n",
      "\n",
      "✓ Time bins created\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TIME BINNING: Add 4-hour time bins to tweets\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Creating time bins...\\n\")\n",
    "\n",
    "# Add time_bin fields\n",
    "if not arcpy.ListFields(tweets_fc, 'time_bin'):\n",
    "    arcpy.management.AddField(tweets_fc, 'time_bin', 'DATE')\n",
    "if not arcpy.ListFields(tweets_fc, 'time_bin_str'):\n",
    "    arcpy.management.AddField(tweets_fc, 'time_bin_str', 'TEXT', field_length=50)\n",
    "\n",
    "# Calculate time bins using pandas-like floor logic\n",
    "expression = \"\"\"\n",
    "def floor_to_4hours(dt):\n",
    "    if dt is None:\n",
    "        return None\n",
    "    # Floor to 4-hour boundary\n",
    "    hour = (dt.hour // 4) * 4\n",
    "    return dt.replace(hour=hour, minute=0, second=0, microsecond=0)\n",
    "\n",
    "floor_to_4hours(!time!)\n",
    "\"\"\"\n",
    "\n",
    "arcpy.management.CalculateField(tweets_fc, 'time_bin', expression, 'PYTHON3')\n",
    "\n",
    "# Create string version\n",
    "arcpy.management.CalculateField(\n",
    "    tweets_fc,\n",
    "    'time_bin_str',\n",
    "    \"!time_bin!.strftime('%Y-%m-%d %H:%M:%S') if !time_bin! else None\",\n",
    "    'PYTHON3'\n",
    ")\n",
    "\n",
    "# Get unique time bins\n",
    "time_bins = set()\n",
    "with arcpy.da.SearchCursor(tweets_fc, ['time_bin']) as cursor:\n",
    "    for row in cursor:\n",
    "        if row[0]:\n",
    "            time_bins.add(row[0])\n",
    "\n",
    "time_bins = sorted(list(time_bins))\n",
    "\n",
    "if len(time_bins) > 0:\n",
    "    print(f\"Found {len(time_bins)} time bins\")\n",
    "    print(f\"Range: {time_bins[0].strftime('%Y-%m-%d %H:%M')} to {time_bins[-1].strftime('%Y-%m-%d %H:%M')}\")\n",
    "    print(f\"\\nTime bins:\")\n",
    "    for tb in time_bins:\n",
    "        print(f\"  {tb.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: No time bins found! Check that tweets have 'time' field.\")\n",
    "\n",
    "print(\"\\n✓ Time bins created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COUNTING WITH DUAL METHODOLOGY: TEXT MENTIONS + SPATIAL CASCADE\n",
      "======================================================================\n",
      "\n",
      "Building spatial indices...\n",
      "Processing 0 tweets...\n",
      "\n"
     ]
    },
    {
     "ename": "<class 'RuntimeError'>",
     "evalue": "Cannot find field 'GPE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 146\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m temporal_state_data, temporal_county_data, temporal_city_data\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Execute the dual counting\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m temporal_state_data, temporal_county_data, temporal_city_data \u001b[38;5;241m=\u001b[39m count_with_mentions_and_cascade_temporal()\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOUNTING COMPLETE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[94], line 45\u001b[0m, in \u001b[0;36mcount_with_mentions_and_cascade_temporal\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Process each tweet\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m arcpy\u001b[38;5;241m.\u001b[39mda\u001b[38;5;241m.\u001b[39mSearchCursor(tweets_fc, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSHAPE@\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_bin\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m cursor:\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m cursor:\n\u001b[0;32m     46\u001b[0m         tweet_point \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     47\u001b[0m         gpe_text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot find field 'GPE'"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DUAL COUNTING: Text Mentions + Spatial Cascade (CRITICAL FIX)\n",
    "# This implements the complete methodology from test.ipynb\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COUNTING WITH DUAL METHODOLOGY: TEXT MENTIONS + SPATIAL CASCADE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def count_with_mentions_and_cascade_temporal():\n",
    "    \"\"\"\n",
    "    Combines both counting methods from test.ipynb:\n",
    "    1. Text-based GPE parsing with fuzzy matching\n",
    "    2. Spatial cascade from tweet point locations\n",
    "    \n",
    "    Returns temporal dictionaries: {time_bin: {entity_id: {'count': N, 'samples': []}}}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize temporal tracking\n",
    "    temporal_state_data = {tb: defaultdict(lambda: {'count': 0, 'samples': []}) for tb in time_bins}\n",
    "    temporal_county_data = {tb: defaultdict(lambda: {'count': 0, 'samples': []}) for tb in time_bins}\n",
    "    temporal_city_data = {tb: defaultdict(lambda: {'count': 0, 'samples': []}) for tb in time_bins}\n",
    "    \n",
    "    # Build spatial index for faster lookups\n",
    "    print(\"Building spatial indices...\")\n",
    "    \n",
    "    # Cache county geometries\n",
    "    county_geoms = {}\n",
    "    with arcpy.da.SearchCursor(counties_fc, ['GEOID', 'STATEFP', 'NAME', 'SHAPE@']) as cursor:\n",
    "        for row in cursor:\n",
    "            county_geoms[row[0]] = {'statefp': row[1], 'name': row[2], 'geom': row[3]}\n",
    "    \n",
    "    # Cache city locations\n",
    "    city_geoms = {}\n",
    "    with arcpy.da.SearchCursor(cities_fc, ['geonameid', 'name', 'SHAPE@']) as cursor:\n",
    "        for row in cursor:\n",
    "            city_geoms[row[0]] = {'name': row[1], 'geom': row[2]}\n",
    "    \n",
    "    print(f\"Processing {tweet_count} tweets...\\n\")\n",
    "    \n",
    "    processed = 0\n",
    "    \n",
    "    # Process each tweet\n",
    "    with arcpy.da.SearchCursor(tweets_fc, ['SHAPE@', 'GPE', 'time', 'time_bin']) as cursor:\n",
    "        for row in cursor:\n",
    "            tweet_point = row[0]\n",
    "            gpe_text = row[1]\n",
    "            tweet_time = row[2]\n",
    "            time_bin = row[3]\n",
    "            \n",
    "            if not time_bin:\n",
    "                continue\n",
    "            \n",
    "            processed += 1\n",
    "            if processed % 100 == 0:\n",
    "                print(f\"  Processed {processed}/{tweet_count} tweets...\")\n",
    "            \n",
    "            # === PART 1: TEXT-BASED MENTION COUNTING ===\n",
    "            entities = parse_gpe_entities(gpe_text)\n",
    "            \n",
    "            for entity in entities:\n",
    "                # Try match to state\n",
    "                state_match, score = match_entity(entity, state_lookup, 0.90)\n",
    "                if state_match:\n",
    "                    state_code = state_match['code']\n",
    "                    temporal_state_data[time_bin][state_code]['count'] += 1\n",
    "                    temporal_state_data[time_bin][state_code]['samples'].append({\n",
    "                        'entity': entity,\n",
    "                        'gpe': str(gpe_text)[:100] if gpe_text else ''\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                # Try match to county\n",
    "                county_match, score = match_entity(entity, county_lookup, 0.85)\n",
    "                if county_match:\n",
    "                    county_id = county_match['geoid']\n",
    "                    temporal_county_data[time_bin][county_id]['count'] += 1\n",
    "                    temporal_county_data[time_bin][county_id]['samples'].append({\n",
    "                        'entity': entity,\n",
    "                        'gpe': str(gpe_text)[:100] if gpe_text else ''\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                # Try match to city\n",
    "                city_match, score = match_entity(entity, city_lookup, 0.85)\n",
    "                if city_match:\n",
    "                    city_id = city_match\n",
    "                    temporal_city_data[time_bin][city_id]['count'] += 1\n",
    "                    temporal_city_data[time_bin][city_id]['samples'].append({\n",
    "                        'entity': entity,\n",
    "                        'gpe': str(gpe_text)[:100] if gpe_text else ''\n",
    "                    })\n",
    "            \n",
    "            # === PART 2: SPATIAL CASCADE FROM POINT LOCATION ===\n",
    "            if tweet_point:\n",
    "                # Find containing county\n",
    "                containing_county = None\n",
    "                for county_id, county_data in county_geoms.items():\n",
    "                    if county_data['geom'].contains(tweet_point):\n",
    "                        containing_county = county_id\n",
    "                        county_statefp = county_data['statefp']\n",
    "                        county_name = county_data['name']\n",
    "                        \n",
    "                        # CASCADE: Increment county\n",
    "                        temporal_county_data[time_bin][county_id]['count'] += 1\n",
    "                        temporal_county_data[time_bin][county_id]['samples'].append({\n",
    "                            'entity': f'[CASCADE: {county_name}]',\n",
    "                            'gpe': '[Spatial containment]'\n",
    "                        })\n",
    "                        \n",
    "                        # CASCADE: Increment state\n",
    "                        if county_statefp in state_fp_lookup:\n",
    "                            state_code = state_fp_lookup[county_statefp]\n",
    "                            temporal_state_data[time_bin][state_code]['count'] += 1\n",
    "                            temporal_state_data[time_bin][state_code]['samples'].append({\n",
    "                                'entity': f'[CASCADE from {county_name}]',\n",
    "                                'gpe': '[Spatial containment]'\n",
    "                            })\n",
    "                        \n",
    "                        break\n",
    "                \n",
    "                # CASCADE: Find nearest city (within 50km)\n",
    "                min_distance = 0.45  # ~50km in degrees\n",
    "                nearest_city = None\n",
    "                nearest_dist = float('inf')\n",
    "                \n",
    "                for city_id, city_data in city_geoms.items():\n",
    "                    dist = tweet_point.distanceTo(city_data['geom'])\n",
    "                    if dist < nearest_dist and dist < min_distance:\n",
    "                        nearest_dist = dist\n",
    "                        nearest_city = city_id\n",
    "                        nearest_city_name = city_data['name']\n",
    "                \n",
    "                if nearest_city:\n",
    "                    temporal_city_data[time_bin][nearest_city]['count'] += 1\n",
    "                    temporal_city_data[time_bin][nearest_city]['samples'].append({\n",
    "                        'entity': f'[CASCADE: {nearest_city_name}]',\n",
    "                        'gpe': f'[Nearby point: {nearest_dist:.3f}°]'\n",
    "                    })\n",
    "    \n",
    "    print(f\"\\n✓ Processed all {processed} tweets\")\n",
    "    \n",
    "    return temporal_state_data, temporal_county_data, temporal_city_data\n",
    "\n",
    "# Execute the dual counting\n",
    "temporal_state_data, temporal_county_data, temporal_city_data = count_with_mentions_and_cascade_temporal()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COUNTING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VALIDATION: Display counts to verify accuracy\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION: Sample Counts\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Count totals across all bins\n",
    "total_state_counts = defaultdict(int)\n",
    "total_county_counts = defaultdict(int)\n",
    "total_city_counts = defaultdict(int)\n",
    "\n",
    "for tb in time_bins:\n",
    "    for state_code, data in temporal_state_data[tb].items():\n",
    "        total_state_counts[state_code] += data['count']\n",
    "    for county_id, data in temporal_county_data[tb].items():\n",
    "        total_county_counts[county_id] += data['count']\n",
    "    for city_id, data in temporal_city_data[tb].items():\n",
    "        total_city_counts[city_id] += data['count']\n",
    "\n",
    "print(f\"Total entities mentioned/cascaded:\")\n",
    "print(f\"  States: {len(total_state_counts)}\")\n",
    "print(f\"  Counties: {len(total_county_counts)}\")\n",
    "print(f\"  Cities: {len(total_city_counts)}\")\n",
    "\n",
    "print(f\"\\nTop 10 states by total count:\")\n",
    "top_states = sorted(total_state_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for state_code, count in top_states:\n",
    "    print(f\"  {state_code}: {count}\")\n",
    "\n",
    "print(f\"\\nTop 10 cities by total count:\")\n",
    "top_cities = sorted(total_city_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "# Get city names\n",
    "city_name_lookup = {}\n",
    "with arcpy.da.SearchCursor(cities_fc, ['geonameid', 'name']) as cursor:\n",
    "    for row in cursor:\n",
    "        city_name_lookup[row[0]] = row[1]\n",
    "\n",
    "for city_id, count in top_cities:\n",
    "    city_name = city_name_lookup.get(city_id, f'ID:{city_id}')\n",
    "    print(f\"  {city_name}: {count}\")\n",
    "\n",
    "print(f\"\\nExpected (from test.ipynb):\")\n",
    "print(f\"  Florida: ~2,156\")\n",
    "print(f\"  Tallahassee: ~135\")\n",
    "print(f\"\\n✓ Compare these numbers with test.ipynb output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPORT TEMPORAL DATA: Create feature classes with counts\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORTING TEMPORAL FEATURE CLASSES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def export_temporal_features(geography_fc, geography_name, id_field, temporal_data_dict):\n",
    "    \"\"\"\n",
    "    Export temporal feature classes with incremental and cumulative counts.\n",
    "    Includes sample mention fields for validation.\n",
    "    \"\"\"\n",
    "    print(f\"\\nExporting {geography_name}...\")\n",
    "    \n",
    "    incremental_fcs = []\n",
    "    cumulative_fcs = []\n",
    "    cumulative_counts = {}  # Track running totals\n",
    "    \n",
    "    for idx, time_bin in enumerate(time_bins):\n",
    "        bin_str = time_bin.strftime('%Y%m%d_%H%M')\n",
    "        bin_label = time_bin.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        print(f\"  Bin {idx+1}/{len(time_bins)}: {bin_label}\")\n",
    "        \n",
    "        # === INCREMENTAL ===\n",
    "        inc_name = f\"{geography_name}_inc_{bin_str}\"\n",
    "        inc_fc = os.path.join(gdb_path, inc_name)\n",
    "        \n",
    "        # Copy base geography\n",
    "        arcpy.management.Copy(geography_fc, inc_fc)\n",
    "        \n",
    "        # Add fields\n",
    "        arcpy.management.AddField(inc_fc, 'tweet_cnt', 'LONG')\n",
    "        arcpy.management.AddField(inc_fc, 'smpl_ment', 'TEXT', field_length=254)\n",
    "        arcpy.management.AddField(inc_fc, 'smpl_gpe', 'TEXT', field_length=254)\n",
    "        arcpy.management.AddField(inc_fc, 'time_bin', 'DATE')\n",
    "        arcpy.management.AddField(inc_fc, 'time_bin_str', 'TEXT', field_length=50)\n",
    "        \n",
    "        # Update counts and samples\n",
    "        with arcpy.da.UpdateCursor(inc_fc, [id_field, 'tweet_cnt', 'smpl_ment', 'smpl_gpe']) as cursor:\n",
    "            for row in cursor:\n",
    "                entity_id = row[0]\n",
    "                if entity_id in temporal_data_dict[time_bin]:\n",
    "                    data = temporal_data_dict[time_bin][entity_id]\n",
    "                    row[1] = data['count']\n",
    "                    # Sample mentions (first 3)\n",
    "                    samples = data['samples'][:3]\n",
    "                    row[2] = '; '.join([s['entity'][:50] for s in samples])\n",
    "                    row[3] = ' | '.join([s['gpe'][:50] for s in samples])\n",
    "                else:\n",
    "                    row[1] = 0\n",
    "                    row[2] = ''\n",
    "                    row[3] = ''\n",
    "                cursor.updateRow(row)\n",
    "        \n",
    "        # Set time bin\n",
    "        arcpy.management.CalculateField(\n",
    "            inc_fc, 'time_bin',\n",
    "            f\"datetime.datetime({time_bin.year}, {time_bin.month}, {time_bin.day}, {time_bin.hour}, {time_bin.minute})\",\n",
    "            'PYTHON3'\n",
    "        )\n",
    "        arcpy.management.CalculateField(\n",
    "            inc_fc, 'time_bin_str',\n",
    "            f\"'{bin_label}'\",\n",
    "            'PYTHON3'\n",
    "        )\n",
    "        \n",
    "        incremental_fcs.append(inc_fc)\n",
    "        \n",
    "        # Update cumulative totals\n",
    "        for entity_id, data in temporal_data_dict[time_bin].items():\n",
    "            cumulative_counts[entity_id] = cumulative_counts.get(entity_id, 0) + data['count']\n",
    "        \n",
    "        # === CUMULATIVE (FIX #5: Persist ALL entities) ===\n",
    "        cum_name = f\"{geography_name}_cum_{bin_str}\"\n",
    "        cum_fc = os.path.join(gdb_path, cum_name)\n",
    "        \n",
    "        # Copy base geography (includes ALL entities)\n",
    "        arcpy.management.Copy(geography_fc, cum_fc)\n",
    "        \n",
    "        # Add fields\n",
    "        arcpy.management.AddField(cum_fc, 'cumul_cnt', 'LONG')\n",
    "        arcpy.management.AddField(cum_fc, 'time_bin', 'DATE')\n",
    "        arcpy.management.AddField(cum_fc, 'time_bin_str', 'TEXT', field_length=50)\n",
    "        \n",
    "        # Update cumulative counts (0 for entities never mentioned)\n",
    "        with arcpy.da.UpdateCursor(cum_fc, [id_field, 'cumul_cnt']) as cursor:\n",
    "            for row in cursor:\n",
    "                entity_id = row[0]\n",
    "                row[1] = cumulative_counts.get(entity_id, 0)  # Defaults to 0\n",
    "                cursor.updateRow(row)\n",
    "        \n",
    "        # Set time bin\n",
    "        arcpy.management.CalculateField(\n",
    "            cum_fc, 'time_bin',\n",
    "            f\"datetime.datetime({time_bin.year}, {time_bin.month}, {time_bin.day}, {time_bin.hour}, {time_bin.minute})\",\n",
    "            'PYTHON3'\n",
    "        )\n",
    "        arcpy.management.CalculateField(\n",
    "            cum_fc, 'time_bin_str',\n",
    "            f\"'{bin_label}'\",\n",
    "            'PYTHON3'\n",
    "        )\n",
    "        \n",
    "        cumulative_fcs.append(cum_fc)\n",
    "    \n",
    "    return incremental_fcs, cumulative_fcs\n",
    "\n",
    "# Export all geographies\n",
    "states_inc, states_cum = export_temporal_features(\n",
    "    states_fc, 'states', 'STUSPS', temporal_state_data\n",
    ")\n",
    "\n",
    "counties_inc, counties_cum = export_temporal_features(\n",
    "    counties_fc, 'counties', 'GEOID', temporal_county_data\n",
    ")\n",
    "\n",
    "cities_inc, cities_cum = export_temporal_features(\n",
    "    cities_fc, 'cities', 'geonameid', temporal_city_data\n",
    ")\n",
    "\n",
    "print(\"\\n✓ All temporal feature classes created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MERGE MASTER FILES: Create *_ALL feature classes\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING MASTER FILES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def merge_temporal_fcs(fc_list, output_name):\n",
    "    \"\"\"Merge all temporal feature classes into master file\"\"\"\n",
    "    output_fc = os.path.join(gdb_path, output_name)\n",
    "    arcpy.management.Merge(fc_list, output_fc)\n",
    "    count = int(arcpy.management.GetCount(output_fc)[0])\n",
    "    print(f\"  {output_name}: {count} features\")\n",
    "    return output_fc\n",
    "\n",
    "# Merge incremental\n",
    "print(\"Incremental master files:\")\n",
    "states_inc_all = merge_temporal_fcs(states_inc, 'states_INCREMENTAL_ALL')\n",
    "counties_inc_all = merge_temporal_fcs(counties_inc, 'counties_INCREMENTAL_ALL')\n",
    "cities_inc_all = merge_temporal_fcs(cities_inc, 'cities_INCREMENTAL_ALL')\n",
    "\n",
    "# Merge cumulative\n",
    "print(\"\\nCumulative master files:\")\n",
    "states_cum_all = merge_temporal_fcs(states_cum, 'states_CUMULATIVE_ALL')\n",
    "counties_cum_all = merge_temporal_fcs(counties_cum, 'counties_CUMULATIVE_ALL')\n",
    "cities_cum_all = merge_temporal_fcs(cities_cum, 'cities_CUMULATIVE_ALL')\n",
    "\n",
    "print(\"\\n✓ Master files created\")\n",
    "\n",
    "# Expected counts from test.ipynb\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION: Compare with test.ipynb expected outputs\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nExpected from test.ipynb:\")\n",
    "print(f\"  states_INCREMENTAL_ALL: 93 records\")\n",
    "print(f\"  states_CUMULATIVE_ALL: 148 records\")\n",
    "print(f\"  counties_INCREMENTAL_ALL: 522 records\")\n",
    "print(f\"  counties_CUMULATIVE_ALL: 1,172 records\")\n",
    "print(f\"  cities_INCREMENTAL_ALL: 662 records\")\n",
    "print(f\"  cities_CUMULATIVE_ALL: 1,852 records\")\n",
    "\n",
    "print(f\"\\nActual outputs:\")\n",
    "print(f\"  states_INCREMENTAL_ALL: {arcpy.management.GetCount(states_inc_all)[0]} records\")\n",
    "print(f\"  states_CUMULATIVE_ALL: {arcpy.management.GetCount(states_cum_all)[0]} records\")\n",
    "print(f\"  counties_INCREMENTAL_ALL: {arcpy.management.GetCount(counties_inc_all)[0]} records\")\n",
    "print(f\"  counties_CUMULATIVE_ALL: {arcpy.management.GetCount(counties_cum_all)[0]} records\")\n",
    "print(f\"  cities_INCREMENTAL_ALL: {arcpy.management.GetCount(cities_inc_all)[0]} records\")\n",
    "print(f\"  cities_CUMULATIVE_ALL: {arcpy.management.GetCount(cities_cum_all)[0]} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCESSING COMPLETE - FIXED VERSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nGEODATABASE: \" + gdb_path)\n",
    "\n",
    "print(\"\\nKEY OUTPUTS FOR ARCGIS PRO:\")\n",
    "print(\"\\nTemporal Feature Classes (time-enabled):\")\n",
    "print(\"  - states_INCREMENTAL_ALL\")\n",
    "print(\"  - states_CUMULATIVE_ALL\")\n",
    "print(\"  - counties_INCREMENTAL_ALL\")\n",
    "print(\"  - counties_CUMULATIVE_ALL\")\n",
    "print(\"  - cities_INCREMENTAL_ALL\")\n",
    "print(\"  - cities_CUMULATIVE_ALL\")\n",
    "\n",
    "print(\"\\nFIELDS:\")\n",
    "print(\"  - tweet_cnt / cumul_cnt: Count values\")\n",
    "print(\"  - smpl_ment: Sample matched entities (for validation)\")\n",
    "print(\"  - smpl_gpe: Sample GPE text (for validation)\")\n",
    "print(\"  - time_bin: DATE field for time slider\")\n",
    "print(\"  - time_bin_str: Human-readable time\")\n",
    "\n",
    "print(\"\\nTO USE IN ARCGIS PRO:\")\n",
    "print(\"  1. Add *_INCREMENTAL_ALL or *_CUMULATIVE_ALL to map\")\n",
    "print(\"  2. Right-click → Properties → Time tab\")\n",
    "print(\"  3. Enable time using 'time_bin' field\")\n",
    "print(\"  4. Set time step to 4 hours\")\n",
    "print(\"  5. Open Time Slider and animate\")\n",
    "\n",
    "print(\"\\nMETHODOLOGY IMPLEMENTED:\")\n",
    "print(\"  ✅ GPE text parsing\")\n",
    "print(\"  ✅ Fuzzy entity matching\")\n",
    "print(\"  ✅ Text-based mention counting\")\n",
    "print(\"  ✅ Spatial cascade (point → county → state)\")\n",
    "print(\"  ✅ Nearest city search (50km)\")\n",
    "print(\"  ✅ Dual counting (mentions + cascade)\")\n",
    "print(\"  ✅ Cumulative persistence (includes zeros)\")\n",
    "print(\"  ✅ Sample fields for validation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ ALL FIXES FROM QA/QC REPORT IMPLEMENTED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, arcpy\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# CONFIG & ENV\n",
    "# =========================\n",
    "# IMPORTANT: Resolve paths from the notebook's folder to avoid surprises\n",
    "# If you already have project_root/gdb_path, still print to confirm\n",
    "notebook_dir = Path.cwd().resolve()\n",
    "print(f\"Notebook dir: {notebook_dir}\")\n",
    "\n",
    "# If you already defined these elsewhere, keep them; just print them here.\n",
    "# Example (edit to your real values):\n",
    "# project_root = r\"C:\\Users\\you\\Documents\\GitHub\\Tweet_project\"\n",
    "# gdb_path = rf\"{project_root}\\tw_project.gdb\"\n",
    "\n",
    "print(f\"project_root: {project_root}\")\n",
    "print(f\"gdb_path    : {gdb_path}\")\n",
    "\n",
    "# Set arcpy environment clearly\n",
    "arcpy.env.workspace = gdb_path\n",
    "arcpy.env.overwriteOutput = True\n",
    "print(f\"arcpy.env.workspace set to: {arcpy.env.workspace}\\n\")\n",
    "\n",
    "def _show_path_state(label, p):\n",
    "    p = str(p)\n",
    "    print(f\"[{label}]\")\n",
    "    print(f\"  Path: {p}\")\n",
    "    print(f\"  os.path.exists: {os.path.exists(p)}\")\n",
    "    try:\n",
    "        print(f\"  arcpy.Exists   : {bool(arcpy.Exists(p))}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  arcpy.Exists   : ERROR ({e})\")\n",
    "    print()\n",
    "\n",
    "# =========================\n",
    "# INPUTS\n",
    "# =========================\n",
    "helene_geojson = os.path.join(project_root, 'data', 'geojson', 'helene.geojson')\n",
    "francine_geojson = os.path.join(project_root, 'data', 'geojson', 'francine.geojson')\n",
    "states_shp = os.path.join(project_root, 'data', 'shape_files', 'cb_2023_us_state_20m.shp')\n",
    "counties_shp = os.path.join(project_root, 'data', 'shape_files', 'cb_2023_us_county_20m.shp')\n",
    "\n",
    "_show_path_state(\"HELENE GEOJSON\", helene_geojson)\n",
    "_show_path_state(\"FRANCINE GEOJSON\", francine_geojson)\n",
    "_show_path_state(\"STATES SHP\", states_shp)\n",
    "_show_path_state(\"COUNTIES SHP\", counties_shp)\n",
    "_show_path_state(\"GDB TARGET\", gdb_path)\n",
    "\n",
    "# Quick structural sanity check on GeoJSON so we fail early if it's empty or malformed\n",
    "def validate_geojson_file(geojson_path, expect_features=True):\n",
    "    if not os.path.exists(geojson_path):\n",
    "        raise FileNotFoundError(f\"GeoJSON not found: {geojson_path}\")\n",
    "    with open(geojson_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            j = json.load(f)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"GeoJSON is not valid JSON: {geojson_path}\\n{e}\")\n",
    "    t = j.get(\"type\", None)\n",
    "    feats = j.get(\"features\", None)\n",
    "    print(f\"GeoJSON header: type={t}, features={'present' if isinstance(feats, list) else 'missing'}\")\n",
    "    if expect_features:\n",
    "        if t != \"FeatureCollection\":\n",
    "            raise ValueError(f\"Expected GeoJSON FeatureCollection, got: {t} ({geojson_path})\")\n",
    "        if not isinstance(feats, list) or len(feats) == 0:\n",
    "            raise ValueError(f\"GeoJSON has no features: {geojson_path}\")\n",
    "    return len(feats) if isinstance(feats, list) else 0\n",
    "\n",
    "print(\"Validating helene.geojson...\")\n",
    "helene_feat_count = validate_geojson_file(helene_geojson)\n",
    "print(f\"helene.geojson features reported in JSON: {helene_feat_count}\\n\")\n",
    "\n",
    "# Optional: also validate Francine if you import it too\n",
    "print(\"Validating francine.geojson...\")\n",
    "francine_feat_count = validate_geojson_file(francine_geojson)\n",
    "print(f\"francine.geojson features reported in JSON: {francine_feat_count}\\n\")\n",
    "\n",
    "# =========================\n",
    "# IMPORTS INTO GDB\n",
    "# =========================\n",
    "tweets_helene_fc = os.path.join(gdb_path, 'tweets_helene')\n",
    "tweets_francine_fc = os.path.join(gdb_path, 'tweets_francine')\n",
    "states_fc = os.path.join(gdb_path, 'us_states')\n",
    "counties_fc = os.path.join(gdb_path, 'us_counties')\n",
    "\n",
    "# Clean up old outputs\n",
    "for fc in [tweets_helene_fc, tweets_francine_fc, states_fc, counties_fc]:\n",
    "    if arcpy.Exists(fc):\n",
    "        print(f\"Deleting existing: {fc}\")\n",
    "        arcpy.management.Delete(fc)\n",
    "\n",
    "print(\"\\n=== Importing GeoJSON → Feature Class ===\")\n",
    "# NOTE: JSON To Features in ArcGIS Pro accepts GeoJSON files when they are FeatureCollections.\n",
    "# If your files are plain Feature arrays or another structure, we already error above.\n",
    "res = arcpy.conversion.JSONToFeatures(helene_geojson, tweets_helene_fc)\n",
    "print(f\"JSONToFeatures (Helene) result: {res}\")\n",
    "\n",
    "hc = int(arcpy.management.GetCount(tweets_helene_fc)[0])\n",
    "print(f\"Imported Helene tweets: {hc} features\")\n",
    "if hc == 0:\n",
    "    # Fail loud and early so we know exactly where\n",
    "    raise RuntimeError(\"Helene import produced 0 features. Check geometry schema/coordinates/FeatureCollection.\")\n",
    "\n",
    "# (If you want both:)\n",
    "res2 = arcpy.conversion.JSONToFeatures(francine_geojson, tweets_francine_fc)\n",
    "print(f\"JSONToFeatures (Francine) result: {res2}\")\n",
    "fc = int(arcpy.management.GetCount(tweets_francine_fc)[0])\n",
    "print(f\"Imported Francine tweets: {fc} features\")\n",
    "if fc == 0:\n",
    "    raise RuntimeError(\"Francine import produced 0 features.\")\n",
    "\n",
    "print(\"\\n=== Importing SHPs → GDB ===\")\n",
    "# Verify SHPs exist before attempting conversion\n",
    "for shp, out_fc in [(states_shp, 'us_states'), (counties_shp, 'us_counties')]:\n",
    "    if not os.path.exists(shp):\n",
    "        raise FileNotFoundError(f\"Shapefile not found: {shp}\")\n",
    "    out_path = os.path.join(gdb_path, out_fc)\n",
    "    res = arcpy.conversion.FeatureClassToFeatureClass(shp, gdb_path, out_fc)\n",
    "    print(f\"FeatureClassToFeatureClass {out_fc}: {res}\")\n",
    "    cnt = int(arcpy.management.GetCount(out_path)[0])\n",
    "    print(f\"Imported {out_fc}: {cnt} features\")\n",
    "    if cnt == 0:\n",
    "        raise RuntimeError(f\"{out_fc} import produced 0 features.\")\n",
    "\n",
    "print(\"\\nAll imports succeeded with nonzero feature counts.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
