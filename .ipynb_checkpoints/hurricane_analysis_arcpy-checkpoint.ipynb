{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurricane Social Media Analysis - ArcPy Native Implementation\n",
    "## Complete ArcGIS Pro Native Notebook\n",
    "\n",
    "This notebook uses **primarily ArcPy** for all spatial operations, with minimal external dependencies.\n",
    "\n",
    "**Key Features:**\n",
    "- Native ArcPy feature class and raster operations\n",
    "- Temporary geodatabase objects (automatically cleaned up)\n",
    "- Multi-level geographic matching\n",
    "- Kernel Density Estimation using arcpy.sa.KernelDensity\n",
    "- Hierarchical weighted rasterization\n",
    "- Time-binned GeoTIFF outputs\n",
    "\n",
    "**Dependencies:**\n",
    "- arcpy (included with ArcGIS Pro)\n",
    "- numpy (included with ArcGIS Pro)\n",
    "- fuzzywuzzy (for text matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'ModuleNotFoundError'>",
     "evalue": "No module named 'fuzzywuzzy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Tuple, Any, Optional\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfuzzywuzzy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzz, process\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Allow overwriting outputs\u001b[39;00m\n\u001b[0;32m     11\u001b[0m arcpy\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39moverwriteOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from collections import defaultdict\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Allow overwriting outputs\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# Check out Spatial Analyst extension\n",
    "if arcpy.CheckExtension(\"Spatial\") == \"Available\":\n",
    "    arcpy.CheckOutExtension(\"Spatial\")\n",
    "    print(\"Spatial Analyst extension checked out\")\n",
    "else:\n",
    "    raise Exception(\"Spatial Analyst extension not available\")\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "\n",
    "# Base Path\n",
    "LOCAL_PATH = r\"C:\\users\\colto\\documents\\github\\tweet_project\"\n",
    "\n",
    "# Data Directories\n",
    "DATA_DIR = os.path.join(LOCAL_PATH, \"data\")\n",
    "GEOJSON_DIR = os.path.join(DATA_DIR, \"geojson\")\n",
    "SHAPE_FILES_DIR = os.path.join(DATA_DIR, \"shape_files\")\n",
    "OUTPUT_DIR = os.path.join(LOCAL_PATH, \"rasters_output_arcpy\")\n",
    "\n",
    "# Input Paths\n",
    "FRANCINE_PATH = os.path.join(GEOJSON_DIR, \"francine.geojson\")\n",
    "HELENE_PATH = os.path.join(GEOJSON_DIR, \"helene.geojson\")\n",
    "STATES_PATH = os.path.join(SHAPE_FILES_DIR, \"cb_2023_us_state_20m.shp\")\n",
    "COUNTIES_PATH = os.path.join(SHAPE_FILES_DIR, \"cb_2023_us_county_20m.shp\")\n",
    "CITIES_PATH = os.path.join(SHAPE_FILES_DIR, \"US_Cities.shp\")\n",
    "\n",
    "# Workspace - Create scratch geodatabase for temporary features\n",
    "SCRATCH_GDB = os.path.join(LOCAL_PATH, \"scratch.gdb\")\n",
    "if not arcpy.Exists(SCRATCH_GDB):\n",
    "    arcpy.CreateFileGDB_management(LOCAL_PATH, \"scratch.gdb\")\n",
    "arcpy.env.workspace = SCRATCH_GDB\n",
    "arcpy.env.scratchWorkspace = SCRATCH_GDB\n",
    "\n",
    "# Spatial Reference\n",
    "TARGET_SR = arcpy.SpatialReference(3857)  # Web Mercator\n",
    "WGS84_SR = arcpy.SpatialReference(4326)  # WGS84\n",
    "\n",
    "# Raster Settings\n",
    "CELL_SIZE_M = 1000  # 1km cells\n",
    "\n",
    "# Hierarchical Weights\n",
    "WEIGHTS: Dict[str, int] = {\n",
    "    \"STATE\": 2,\n",
    "    \"COUNTY\": 5,\n",
    "    \"CITY\": 10,\n",
    "    \"FACILITY\": 10,\n",
    "}\n",
    "\n",
    "# Fuzzy Matching & Time\n",
    "FUZZY_THRESHOLD = 75\n",
    "FUZZY_THRESHOLD_CONTEXTUAL = 70\n",
    "TIME_BIN_HOURS = 4\n",
    "\n",
    "# KDE Parameters\n",
    "CITY_KDE_SEARCH_RADIUS = 3000  # 3km search radius for city KDE\n",
    "FACILITY_KDE_SEARCH_RADIUS = 2000  # 2km for facilities\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"Scratch GDB: {SCRATCH_GDB}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_temp_features(pattern=\"temp_*\"):\n",
    "    \"\"\"\n",
    "    Delete temporary feature classes matching pattern.\n",
    "    \"\"\"\n",
    "    arcpy.env.workspace = SCRATCH_GDB\n",
    "    for fc in arcpy.ListFeatureClasses(pattern):\n",
    "        try:\n",
    "            arcpy.Delete_management(fc)\n",
    "        except:\n",
    "            pass\n",
    "    for table in arcpy.ListTables(pattern):\n",
    "        try:\n",
    "            arcpy.Delete_management(table)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def preprocess_place_name(name: Any) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Standardize place names for matching.\n",
    "    \"\"\"\n",
    "    if not name or name == \"NAN\" or str(name).strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    name = str(name).upper().strip()\n",
    "    \n",
    "    # Expand abbreviations\n",
    "    name = re.sub(r\"\\bST\\.?\\b\", \"SAINT\", name)\n",
    "    name = re.sub(r\"\\bMT\\.?\\b\", \"MOUNT\", name)\n",
    "    name = re.sub(r\"\\bFT\\.?\\b\", \"FORT\", name)\n",
    "    name = re.sub(r\"\\bN\\.?\\b\", \"NORTH\", name)\n",
    "    name = re.sub(r\"\\bS\\.?\\b\", \"SOUTH\", name)\n",
    "    name = re.sub(r\"\\bE\\.?\\b\", \"EAST\", name)\n",
    "    name = re.sub(r\"\\bW\\.?\\b\", \"WEST\", name)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    name = re.sub(r\"[^\\w\\s]\", \"\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    \n",
    "    return name.strip()\n",
    "\n",
    "\n",
    "def parse_gpe_entities(gpe_string: Any) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse GPE field into cleaned entity list.\n",
    "    \"\"\"\n",
    "    if not gpe_string or str(gpe_string).strip() == \"\":\n",
    "        return []\n",
    "    \n",
    "    gpe_string = str(gpe_string).strip()\n",
    "    entities: List[str] = []\n",
    "    \n",
    "    parts = [p.strip() for p in gpe_string.split(\",\")]\n",
    "    \n",
    "    for part in parts:\n",
    "        if part:\n",
    "            sub_parts = re.split(r\"[;&|]\", part)\n",
    "            for sub in sub_parts:\n",
    "                sub = sub.strip()\n",
    "                if sub and len(sub) > 1:\n",
    "                    cleaned = preprocess_place_name(sub)\n",
    "                    if cleaned:\n",
    "                        entities.append(cleaned)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for e in entities:\n",
    "        if e not in seen:\n",
    "            unique.append(e)\n",
    "            seen.add(e)\n",
    "    \n",
    "    return unique\n",
    "\n",
    "\n",
    "def floor_to_hours(dt: datetime, hours: int) -> datetime:\n",
    "    \"\"\"\n",
    "    Floor datetime to the nearest time bin.\n",
    "    \"\"\"\n",
    "    hour_floored = (dt.hour // hours) * hours\n",
    "    return dt.replace(hour=hour_floored, minute=0, second=0, microsecond=0)\n",
    "\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Data Loading with ArcPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hurricane_data_arcpy(geojson_path: str, hurricane_name: str) -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load GeoJSON into feature class, add time bins.\n",
    "    Returns: (feature_class_path, metadata_dict)\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading {hurricane_name} from {geojson_path}...\")\n",
    "    \n",
    "    # Convert GeoJSON to feature class\n",
    "    temp_fc = f\"temp_{hurricane_name}_raw\"\n",
    "    arcpy.JSONToFeatures_conversion(geojson_path, temp_fc)\n",
    "    \n",
    "    # Add fields for time processing\n",
    "    arcpy.AddField_management(temp_fc, \"time_bin\", \"DATE\")\n",
    "    arcpy.AddField_management(temp_fc, \"unix_timestamp\", \"LONG\")\n",
    "    arcpy.AddField_management(temp_fc, \"bin_label\", \"TEXT\", field_length=50)\n",
    "    \n",
    "    # Process time fields\n",
    "    time_bins = set()\n",
    "    timestamp_dict = {}\n",
    "    count = 0\n",
    "    \n",
    "    with arcpy.da.UpdateCursor(temp_fc, [\"time\", \"time_bin\", \"unix_timestamp\", \"bin_label\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            if row[0]:  # if time field exists\n",
    "                # Parse time string\n",
    "                try:\n",
    "                    dt = datetime.fromisoformat(str(row[0]).replace('Z', '+00:00'))\n",
    "                except:\n",
    "                    dt = datetime.strptime(str(row[0]), \"%Y-%m-%d %H:%M:%S\")\n",
    "                \n",
    "                # Floor to time bin\n",
    "                time_bin = floor_to_hours(dt, TIME_BIN_HOURS)\n",
    "                unix_ts = int(time_bin.timestamp() * 1000)  # milliseconds\n",
    "                bin_label = time_bin.strftime(\"%Y%m%d_%H%M\")\n",
    "                \n",
    "                row[1] = time_bin\n",
    "                row[2] = unix_ts\n",
    "                row[3] = bin_label\n",
    "                \n",
    "                cursor.updateRow(row)\n",
    "                \n",
    "                time_bins.add(unix_ts)\n",
    "                timestamp_dict[unix_ts] = time_bin\n",
    "                count += 1\n",
    "    \n",
    "    print(f\"  Loaded {count} {hurricane_name} tweets\")\n",
    "    print(f\"  Time bins: {len(time_bins)}\")\n",
    "    \n",
    "    metadata = {\n",
    "        \"count\": count,\n",
    "        \"time_bins\": sorted(list(time_bins)),\n",
    "        \"timestamp_dict\": timestamp_dict\n",
    "    }\n",
    "    \n",
    "    return temp_fc, metadata\n",
    "\n",
    "\n",
    "def load_reference_layers() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load and project reference shapefiles to target SR.\n",
    "    Returns dict of layer names to feature class paths.\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading reference shapefiles...\")\n",
    "    \n",
    "    layers = {}\n",
    "    \n",
    "    # States\n",
    "    states_proj = \"temp_states_proj\"\n",
    "    arcpy.Project_management(STATES_PATH, states_proj, TARGET_SR)\n",
    "    count = int(arcpy.GetCount_management(states_proj)[0])\n",
    "    print(f\"  States: {count}\")\n",
    "    layers[\"states\"] = states_proj\n",
    "    \n",
    "    # Counties\n",
    "    counties_proj = \"temp_counties_proj\"\n",
    "    arcpy.Project_management(COUNTIES_PATH, counties_proj, TARGET_SR)\n",
    "    count = int(arcpy.GetCount_management(counties_proj)[0])\n",
    "    print(f\"  Counties: {count}\")\n",
    "    layers[\"counties\"] = counties_proj\n",
    "    \n",
    "    # Cities - convert to points (centroids)\n",
    "    cities_temp = \"temp_cities_temp\"\n",
    "    cities_proj = \"temp_cities_proj\"\n",
    "    arcpy.Project_management(CITIES_PATH, cities_temp, TARGET_SR)\n",
    "    arcpy.FeatureToPoint_management(cities_temp, cities_proj, \"INSIDE\")\n",
    "    count = int(arcpy.GetCount_management(cities_proj)[0])\n",
    "    print(f\"  Cities (as points): {count}\")\n",
    "    layers[\"cities\"] = cities_proj\n",
    "    \n",
    "    # Clean up temp\n",
    "    if arcpy.Exists(cities_temp):\n",
    "        arcpy.Delete_management(cities_temp)\n",
    "    \n",
    "    return layers\n",
    "\n",
    "\n",
    "print(\"Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Geographic Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchical_lookups_arcpy(layers: Dict[str, str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build lookup dictionaries from feature classes using cursors.\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating hierarchical lookups...\")\n",
    "    \n",
    "    lookups = {\n",
    "        \"state_lookup\": {},\n",
    "        \"county_lookup\": {},\n",
    "        \"city_lookup\": {},\n",
    "        \"county_by_state\": defaultdict(dict),\n",
    "        \"city_by_state\": defaultdict(dict),\n",
    "        \"state_abbrev_to_name\": {},\n",
    "        \"state_name_to_abbrev\": {},\n",
    "    }\n",
    "    \n",
    "    # States\n",
    "    with arcpy.da.SearchCursor(layers[\"states\"], [\"NAME\", \"STUSPS\", \"SHAPE@\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            name = preprocess_place_name(row[0])\n",
    "            abbrev = str(row[1]).upper() if row[1] else None\n",
    "            geom = row[2]\n",
    "            \n",
    "            if name:\n",
    "                lookups[\"state_lookup\"][name] = geom\n",
    "                if abbrev:\n",
    "                    lookups[\"state_lookup\"][abbrev] = geom\n",
    "                    lookups[\"state_abbrev_to_name\"][abbrev] = name\n",
    "                    lookups[\"state_name_to_abbrev\"][name] = abbrev\n",
    "    \n",
    "    # Counties\n",
    "    with arcpy.da.SearchCursor(layers[\"counties\"], [\"NAME\", \"STATEFP\", \"SHAPE@\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            name = preprocess_place_name(row[0])\n",
    "            state_fips = str(row[1]) if row[1] else None\n",
    "            geom = row[2]\n",
    "            \n",
    "            if name:\n",
    "                lookups[\"county_lookup\"][name] = geom\n",
    "                \n",
    "                # Match to state\n",
    "                if state_fips:\n",
    "                    with arcpy.da.SearchCursor(layers[\"states\"], [\"NAME\", \"STATEFP\"]) as s_cursor:\n",
    "                        for s_row in s_cursor:\n",
    "                            if str(s_row[1]) == state_fips:\n",
    "                                state_name = preprocess_place_name(s_row[0])\n",
    "                                if state_name:\n",
    "                                    lookups[\"county_by_state\"][state_name][name] = geom\n",
    "                                break\n",
    "    \n",
    "    # Cities (points)\n",
    "    with arcpy.da.SearchCursor(layers[\"cities\"], [\"NAME\", \"ST\", \"SHAPE@\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            name = preprocess_place_name(row[0])\n",
    "            state_abbrev = str(row[1]).upper() if row[1] else None\n",
    "            geom = row[2]  # Point geometry\n",
    "            \n",
    "            if name:\n",
    "                lookups[\"city_lookup\"][name] = geom\n",
    "                \n",
    "                if state_abbrev and state_abbrev in lookups[\"state_abbrev_to_name\"]:\n",
    "                    state_full = lookups[\"state_abbrev_to_name\"][state_abbrev]\n",
    "                    lookups[\"city_by_state\"][state_full][name] = geom\n",
    "    \n",
    "    print(f\"  States: {len(lookups['state_lookup'])}\")\n",
    "    print(f\"  Counties: {len(lookups['county_lookup'])}\")\n",
    "    print(f\"  Cities: {len(lookups['city_lookup'])}\")\n",
    "    \n",
    "    return lookups\n",
    "\n",
    "\n",
    "def fuzzy_match_entity(\n",
    "    entity: Optional[str],\n",
    "    candidates: Dict[str, Any],\n",
    "    threshold: int = FUZZY_THRESHOLD,\n",
    ") -> Tuple[Optional[str], int]:\n",
    "    \"\"\"\n",
    "    Fuzzy match entity against candidates.\n",
    "    \"\"\"\n",
    "    if not entity or not candidates:\n",
    "        return None, 0\n",
    "    \n",
    "    if entity in candidates:\n",
    "        return entity, 100\n",
    "    \n",
    "    match = process.extractOne(entity, candidates.keys(), scorer=fuzz.ratio)\n",
    "    if match and match[1] >= threshold:\n",
    "        return match[0], match[1]\n",
    "    \n",
    "    return None, 0\n",
    "\n",
    "\n",
    "def find_all_geographic_matches(\n",
    "    entities: List[str],\n",
    "    lookups: Dict[str, Any],\n",
    ") -> List[Tuple[str, Optional[str], Any, int]]:\n",
    "    \"\"\"\n",
    "    Find all STATE/COUNTY/CITY matches.\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        return []\n",
    "    \n",
    "    all_matches: List[Tuple[str, Optional[str], Any, int]] = []\n",
    "    found_states: set[str] = set()\n",
    "    \n",
    "    # STATES\n",
    "    for entity in entities:\n",
    "        state_match, state_score = fuzzy_match_entity(entity, lookups[\"state_lookup\"], threshold=75)\n",
    "        if state_match:\n",
    "            all_matches.append((\"STATE\", state_match, lookups[\"state_lookup\"][state_match], state_score))\n",
    "            found_states.add(state_match)\n",
    "    \n",
    "    # COUNTIES\n",
    "    for entity in entities:\n",
    "        county_match, county_score = fuzzy_match_entity(entity, lookups[\"county_lookup\"], threshold=75)\n",
    "        if county_match:\n",
    "            all_matches.append((\"COUNTY\", county_match, lookups[\"county_lookup\"][county_match], county_score))\n",
    "        \n",
    "        # Contextual within states\n",
    "        for state_name in found_states:\n",
    "            if state_name in lookups[\"county_by_state\"]:\n",
    "                state_counties = lookups[\"county_by_state\"][state_name]\n",
    "                state_county_match, state_county_score = fuzzy_match_entity(\n",
    "                    entity, state_counties, threshold=70\n",
    "                )\n",
    "                if state_county_match and state_county_score > county_score:\n",
    "                    all_matches = [\n",
    "                        m for m in all_matches\n",
    "                        if not (m[0] == \"COUNTY\" and m[1] == county_match)\n",
    "                    ]\n",
    "                    all_matches.append(\n",
    "                        (\"COUNTY\", state_county_match, state_counties[state_county_match], state_county_score)\n",
    "                    )\n",
    "    \n",
    "    # CITIES\n",
    "    for entity in entities:\n",
    "        city_match, city_score = fuzzy_match_entity(entity, lookups[\"city_lookup\"], threshold=75)\n",
    "        if city_match:\n",
    "            all_matches.append((\"CITY\", city_match, lookups[\"city_lookup\"][city_match], city_score))\n",
    "        \n",
    "        # Contextual\n",
    "        for state_name in found_states:\n",
    "            if state_name in lookups[\"city_by_state\"]:\n",
    "                state_cities = lookups[\"city_by_state\"][state_name]\n",
    "                state_city_match, state_city_score = fuzzy_match_entity(\n",
    "                    entity, state_cities, threshold=70\n",
    "                )\n",
    "                if state_city_match and state_city_score > city_score:\n",
    "                    all_matches = [\n",
    "                        m for m in all_matches\n",
    "                        if not (m[0] == \"CITY\" and m[1] == city_match)\n",
    "                    ]\n",
    "                    all_matches.append(\n",
    "                        (\"CITY\", state_city_match, state_cities[state_city_match], state_city_score)\n",
    "                    )\n",
    "    \n",
    "    # De-duplicate\n",
    "    unique_matches: List[Tuple[str, Optional[str], Any, int]] = []\n",
    "    seen: set[Tuple[str, Optional[str]]] = set()\n",
    "    for match in all_matches:\n",
    "        combo = (match[0], match[1])\n",
    "        if combo not in seen:\n",
    "            unique_matches.append(match)\n",
    "            seen.add(combo)\n",
    "    \n",
    "    return unique_matches\n",
    "\n",
    "\n",
    "print(\"Geographic matching functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Tweet Expansion & Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_tweets_arcpy(\n",
    "    input_fc: str,\n",
    "    lookups: Dict[str, Any],\n",
    "    output_fc: str,\n",
    "    hurricane_name: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Expand tweets to multiple matches, create output feature class.\n",
    "    Returns path to output feature class.\n",
    "    \"\"\"\n",
    "    print(f\"\\nExpanding {hurricane_name} tweets by matches...\")\n",
    "    \n",
    "    # Create output feature class schema\n",
    "    arcpy.CreateFeatureclass_management(\n",
    "        SCRATCH_GDB,\n",
    "        output_fc,\n",
    "        \"POINT\",\n",
    "        spatial_reference=TARGET_SR\n",
    "    )\n",
    "    \n",
    "    # Add fields\n",
    "    arcpy.AddField_management(output_fc, \"original_id\", \"LONG\")\n",
    "    arcpy.AddField_management(output_fc, \"unix_timestamp\", \"LONG\")\n",
    "    arcpy.AddField_management(output_fc, \"bin_label\", \"TEXT\", field_length=50)\n",
    "    arcpy.AddField_management(output_fc, \"scale_level\", \"TEXT\", field_length=20)\n",
    "    arcpy.AddField_management(output_fc, \"matched_name\", \"TEXT\", field_length=200)\n",
    "    arcpy.AddField_management(output_fc, \"match_score\", \"SHORT\")\n",
    "    arcpy.AddField_management(output_fc, \"GPE\", \"TEXT\", field_length=500)\n",
    "    arcpy.AddField_management(output_fc, \"FAC\", \"TEXT\", field_length=200)\n",
    "    \n",
    "    # Process tweets\n",
    "    insert_fields = [\n",
    "        \"SHAPE@\", \"original_id\", \"unix_timestamp\", \"bin_label\",\n",
    "        \"scale_level\", \"matched_name\", \"match_score\", \"GPE\", \"FAC\"\n",
    "    ]\n",
    "    \n",
    "    count = 0\n",
    "    expanded_count = 0\n",
    "    scale_counts = defaultdict(int)\n",
    "    \n",
    "    with arcpy.da.SearchCursor(input_fc, [\"OID@\", \"SHAPE@\", \"unix_timestamp\", \"bin_label\", \"GPE\", \"FAC\"]) as s_cursor:\n",
    "        with arcpy.da.InsertCursor(output_fc, insert_fields) as i_cursor:\n",
    "            for row in s_cursor:\n",
    "                oid, geom, unix_ts, bin_label, gpe, fac = row\n",
    "                \n",
    "                if count % 100 == 0:\n",
    "                    print(f\"  Processing tweet {count}...\")\n",
    "                \n",
    "                # Parse GPE entities\n",
    "                entities = parse_gpe_entities(gpe)\n",
    "                matches = []\n",
    "                \n",
    "                if entities:\n",
    "                    matches = find_all_geographic_matches(entities, lookups)\n",
    "                \n",
    "                # Add facility\n",
    "                if fac and str(fac).strip() not in [\"\", \"nan\", \"NAN\"]:\n",
    "                    matches.append((\"FACILITY\", str(fac), geom, 100))\n",
    "                \n",
    "                # If no matches, add UNMATCHED\n",
    "                if not matches:\n",
    "                    matches.append((\"UNMATCHED\", None, geom, 0))\n",
    "                \n",
    "                # Insert all matches\n",
    "                for scale, name, matched_geom, score in matches:\n",
    "                    i_cursor.insertRow([\n",
    "                        geom,  # Use original tweet point location\n",
    "                        oid,\n",
    "                        unix_ts,\n",
    "                        bin_label,\n",
    "                        scale,\n",
    "                        name if name else \"\",\n",
    "                        score,\n",
    "                        gpe if gpe else \"\",\n",
    "                        fac if fac else \"\"\n",
    "                    ])\n",
    "                    expanded_count += 1\n",
    "                    scale_counts[scale] += 1\n",
    "                \n",
    "                count += 1\n",
    "    \n",
    "    print(f\"\\n  Expanded from {count} to {expanded_count} rows\")\n",
    "    print(f\"\\n  Scale distribution:\")\n",
    "    for scale, cnt in sorted(scale_counts.items()):\n",
    "        print(f\"    {scale}: {cnt}\")\n",
    "    \n",
    "    return output_fc\n",
    "\n",
    "\n",
    "def create_interval_counts_dict(expanded_fc: str) -> Dict[Tuple[int, str, str], int]:\n",
    "    \"\"\"\n",
    "    Create interval counts as dictionary.\n",
    "    Key: (unix_timestamp, scale_level, matched_name)\n",
    "    Value: count\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating interval counts...\")\n",
    "    \n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    with arcpy.da.SearchCursor(expanded_fc, [\"unix_timestamp\", \"scale_level\", \"matched_name\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            key = (row[0], row[1], row[2] if row[2] else \"\")\n",
    "            counts[key] += 1\n",
    "    \n",
    "    print(f\"  Created {len(counts)} unique (time, scale, name) combinations\")\n",
    "    \n",
    "    return dict(counts)\n",
    "\n",
    "\n",
    "print(\"Expansion & aggregation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Rasterization with ArcPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grid_extent(francine_fc: str, helene_fc: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate grid extent and parameters.\n",
    "    \"\"\"\n",
    "    print(\"\\nCalculating grid extent...\")\n",
    "    \n",
    "    # Get extents\n",
    "    desc_f = arcpy.Describe(francine_fc)\n",
    "    desc_h = arcpy.Describe(helene_fc)\n",
    "    \n",
    "    extent_f = desc_f.extent\n",
    "    extent_h = desc_h.extent\n",
    "    \n",
    "    # Union extent\n",
    "    minx = min(extent_f.XMin, extent_h.XMin)\n",
    "    miny = min(extent_f.YMin, extent_h.YMin)\n",
    "    maxx = max(extent_f.XMax, extent_h.XMax)\n",
    "    maxy = max(extent_f.YMax, extent_h.YMax)\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    width = int(np.ceil((maxx - minx) / CELL_SIZE_M))\n",
    "    height = int(np.ceil((maxy - miny) / CELL_SIZE_M))\n",
    "    \n",
    "    # Adjust maxx/maxy to align with grid\n",
    "    maxx = minx + (width * CELL_SIZE_M)\n",
    "    maxy = miny + (height * CELL_SIZE_M)\n",
    "    \n",
    "    extent_str = f\"{minx} {miny} {maxx} {maxy}\"\n",
    "    \n",
    "    print(f\"  Cell size: {CELL_SIZE_M} meters\")\n",
    "    print(f\"  Grid: {width} x {height} cells\")\n",
    "    print(f\"  Extent: {extent_str}\")\n",
    "    \n",
    "    return {\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"extent\": extent_str,\n",
    "        \"minx\": minx,\n",
    "        \"miny\": miny,\n",
    "        \"maxx\": maxx,\n",
    "        \"maxy\": maxy,\n",
    "        \"cell_size\": CELL_SIZE_M\n",
    "    }\n",
    "\n",
    "\n",
    "def create_hierarchical_raster_arcpy(\n",
    "    expanded_fc: str,\n",
    "    unix_timestamp: int,\n",
    "    interval_counts: Dict[Tuple[int, str, str], int],\n",
    "    lookups: Dict[str, Any],\n",
    "    reference_layers: Dict[str, str],\n",
    "    grid_params: Dict[str, Any]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create hierarchical weighted raster for single time bin using ArcPy.\n",
    "    \"\"\"\n",
    "    # Initialize output array\n",
    "    output_grid = np.zeros((grid_params[\"height\"], grid_params[\"width\"]), dtype=np.float32)\n",
    "    \n",
    "    # Get data for this time bin\n",
    "    bin_data = {k: v for k, v in interval_counts.items() if k[0] == unix_timestamp}\n",
    "    \n",
    "    if not bin_data:\n",
    "        return output_grid\n",
    "    \n",
    "    # Organize by scale level\n",
    "    state_data = {k[2]: v for k, v in bin_data.items() if k[1] == \"STATE\"}\n",
    "    county_data = {k[2]: v for k, v in bin_data.items() if k[1] == \"COUNTY\"}\n",
    "    city_data = {k[2]: v for k, v in bin_data.items() if k[1] == \"CITY\"}\n",
    "    facility_data = {k[2]: v for k, v in bin_data.items() if k[1] == \"FACILITY\"}\n",
    "    \n",
    "    # Determine states to include\n",
    "    states_to_include = set(state_data.keys())\n",
    "    \n",
    "    # Add parent states for counties/cities\n",
    "    for county_name in county_data.keys():\n",
    "        if county_name in lookups[\"county_lookup\"]:\n",
    "            county_geom = lookups[\"county_lookup\"][county_name]\n",
    "            centroid = county_geom.centroid\n",
    "            for state_name, state_geom in lookups[\"state_lookup\"].items():\n",
    "                if state_geom.contains(centroid):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "    \n",
    "    for city_name in city_data.keys():\n",
    "        if city_name in lookups[\"city_lookup\"]:\n",
    "            city_point = lookups[\"city_lookup\"][city_name]\n",
    "            for state_name, state_geom in lookups[\"state_lookup\"].items():\n",
    "                if state_geom.contains(city_point):\n",
    "                    states_to_include.add(state_name)\n",
    "                    break\n",
    "    \n",
    "    # Rasterize STATES\n",
    "    for state_name in states_to_include:\n",
    "        if state_name in lookups[\"state_lookup\"]:\n",
    "            tweet_count = state_data.get(state_name, 1)\n",
    "            value = np.log1p(float(tweet_count)) * WEIGHTS[\"STATE\"]\n",
    "            \n",
    "            # Create temporary polygon feature for this state\n",
    "            temp_state_fc = \"temp_state_single\"\n",
    "            arcpy.CreateFeatureclass_management(SCRATCH_GDB, temp_state_fc, \"POLYGON\", spatial_reference=TARGET_SR)\n",
    "            arcpy.AddField_management(temp_state_fc, \"value\", \"FLOAT\")\n",
    "            \n",
    "            with arcpy.da.InsertCursor(temp_state_fc, [\"SHAPE@\", \"value\"]) as cursor:\n",
    "                cursor.insertRow([lookups[\"state_lookup\"][state_name], value])\n",
    "            \n",
    "            # Convert to raster\n",
    "            temp_raster = \"temp_state_raster\"\n",
    "            arcpy.PolygonToRaster_conversion(\n",
    "                temp_state_fc,\n",
    "                \"value\",\n",
    "                temp_raster,\n",
    "                cellsize=CELL_SIZE_M\n",
    "            )\n",
    "            \n",
    "            # Convert to numpy and add\n",
    "            arr = arcpy.RasterToNumPyArray(temp_raster, nodata_to_value=0)\n",
    "            if arr.shape == output_grid.shape:\n",
    "                output_grid += arr\n",
    "            \n",
    "            # Cleanup\n",
    "            arcpy.Delete_management(temp_state_fc)\n",
    "            arcpy.Delete_management(temp_raster)\n",
    "    \n",
    "    # Rasterize COUNTIES (similar approach)\n",
    "    if county_data:\n",
    "        for county_name, tweet_count in county_data.items():\n",
    "            if county_name in lookups[\"county_lookup\"]:\n",
    "                value = np.log1p(float(tweet_count)) * WEIGHTS[\"COUNTY\"]\n",
    "                \n",
    "                temp_county_fc = \"temp_county_single\"\n",
    "                arcpy.CreateFeatureclass_management(SCRATCH_GDB, temp_county_fc, \"POLYGON\", spatial_reference=TARGET_SR)\n",
    "                arcpy.AddField_management(temp_county_fc, \"value\", \"FLOAT\")\n",
    "                \n",
    "                with arcpy.da.InsertCursor(temp_county_fc, [\"SHAPE@\", \"value\"]) as cursor:\n",
    "                    cursor.insertRow([lookups[\"county_lookup\"][county_name], value])\n",
    "                \n",
    "                temp_raster = \"temp_county_raster\"\n",
    "                arcpy.PolygonToRaster_conversion(\n",
    "                    temp_county_fc,\n",
    "                    \"value\",\n",
    "                    temp_raster,\n",
    "                    cellsize=CELL_SIZE_M\n",
    "                )\n",
    "                \n",
    "                arr = arcpy.RasterToNumPyArray(temp_raster, nodata_to_value=0)\n",
    "                if arr.shape == output_grid.shape:\n",
    "                    output_grid += arr\n",
    "                \n",
    "                arcpy.Delete_management(temp_county_fc)\n",
    "                arcpy.Delete_management(temp_raster)\n",
    "    \n",
    "    # CITIES using KDE\n",
    "    if city_data:\n",
    "        print(f\"      Creating city KDE...\")\n",
    "        \n",
    "        # Create temp point feature class for cities with tweet counts\n",
    "        temp_city_fc = \"temp_city_points\"\n",
    "        arcpy.CreateFeatureclass_management(SCRATCH_GDB, temp_city_fc, \"POINT\", spatial_reference=TARGET_SR)\n",
    "        arcpy.AddField_management(temp_city_fc, \"weight\", \"FLOAT\")\n",
    "        \n",
    "        cities_processed = 0\n",
    "        with arcpy.da.InsertCursor(temp_city_fc, [\"SHAPE@\", \"weight\"]) as cursor:\n",
    "            for city_name, tweet_count in city_data.items():\n",
    "                if city_name in lookups[\"city_lookup\"]:\n",
    "                    city_point = lookups[\"city_lookup\"][city_name]\n",
    "                    weight = np.log1p(float(tweet_count)) * WEIGHTS[\"CITY\"]\n",
    "                    cursor.insertRow([city_point, weight])\n",
    "                    cities_processed += 1\n",
    "        \n",
    "        if cities_processed > 0:\n",
    "            # Apply Kernel Density\n",
    "            temp_kde = \"temp_city_kde\"\n",
    "            arcpy.sa.KernelDensity(\n",
    "                temp_city_fc,\n",
    "                \"weight\",\n",
    "                temp_kde,\n",
    "                CELL_SIZE_M,\n",
    "                CITY_KDE_SEARCH_RADIUS\n",
    "            ).save(temp_kde)\n",
    "            \n",
    "            # Convert to array and add\n",
    "            arr = arcpy.RasterToNumPyArray(temp_kde, nodata_to_value=0)\n",
    "            if arr.shape == output_grid.shape:\n",
    "                output_grid += arr\n",
    "            \n",
    "            print(f\"        Processed {cities_processed}/{len(city_data)} cities\")\n",
    "            print(f\"        City KDE max: {np.max(arr):.2f}\")\n",
    "            \n",
    "            arcpy.Delete_management(temp_kde)\n",
    "        \n",
    "        arcpy.Delete_management(temp_city_fc)\n",
    "    \n",
    "    # FACILITIES using KDE (similar to cities)\n",
    "    if facility_data:\n",
    "        # Filter facilities that have point geometry in expanded_fc\n",
    "        temp_fac_fc = \"temp_facility_points\"\n",
    "        where_clause = f\"unix_timestamp = {unix_timestamp} AND scale_level = 'FACILITY'\"\n",
    "        arcpy.Select_analysis(expanded_fc, temp_fac_fc, where_clause)\n",
    "        \n",
    "        count = int(arcpy.GetCount_management(temp_fac_fc)[0])\n",
    "        if count > 0:\n",
    "            arcpy.AddField_management(temp_fac_fc, \"kde_weight\", \"FLOAT\")\n",
    "            \n",
    "            with arcpy.da.UpdateCursor(temp_fac_fc, [\"matched_name\", \"kde_weight\"]) as cursor:\n",
    "                for row in cursor:\n",
    "                    fac_name = row[0]\n",
    "                    tweet_count = facility_data.get(fac_name, 1)\n",
    "                    row[1] = float(tweet_count) * WEIGHTS[\"FACILITY\"]\n",
    "                    cursor.updateRow(row)\n",
    "            \n",
    "            temp_fac_kde = \"temp_facility_kde\"\n",
    "            arcpy.sa.KernelDensity(\n",
    "                temp_fac_fc,\n",
    "                \"kde_weight\",\n",
    "                temp_fac_kde,\n",
    "                CELL_SIZE_M,\n",
    "                FACILITY_KDE_SEARCH_RADIUS\n",
    "            ).save(temp_fac_kde)\n",
    "            \n",
    "            arr = arcpy.RasterToNumPyArray(temp_fac_kde, nodata_to_value=0)\n",
    "            if arr.shape == output_grid.shape:\n",
    "                output_grid += arr\n",
    "            \n",
    "            arcpy.Delete_management(temp_fac_kde)\n",
    "        \n",
    "        arcpy.Delete_management(temp_fac_fc)\n",
    "    \n",
    "    return output_grid\n",
    "\n",
    "\n",
    "print(\"Rasterization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Process Hurricane Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hurricane_arcpy(\n",
    "    hurricane_name: str,\n",
    "    expanded_fc: str,\n",
    "    time_bins: List[int],\n",
    "    timestamp_dict: Dict[int, datetime],\n",
    "    interval_counts: Dict[Tuple[int, str, str], int],\n",
    "    lookups: Dict[str, Any],\n",
    "    reference_layers: Dict[str, str],\n",
    "    grid_params: Dict[str, Any]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Process all time bins and write rasters.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"PROCESSING: {hurricane_name.upper()}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # Create output directories\n",
    "    hurricane_dir = os.path.join(OUTPUT_DIR, hurricane_name.lower())\n",
    "    increment_dir = os.path.join(hurricane_dir, \"increment\")\n",
    "    cumulative_dir = os.path.join(hurricane_dir, \"cumulative\")\n",
    "    \n",
    "    os.makedirs(increment_dir, exist_ok=True)\n",
    "    os.makedirs(cumulative_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize cumulative grid\n",
    "    cumulative_grid = np.zeros((grid_params[\"height\"], grid_params[\"width\"]), dtype=np.float32)\n",
    "    \n",
    "    # Lower-left corner for NumPyArrayToRaster\n",
    "    lower_left = arcpy.Point(grid_params[\"minx\"], grid_params[\"miny\"])\n",
    "    \n",
    "    for idx, time_bin in enumerate(time_bins):\n",
    "        print(f\"\\nTime Bin {idx + 1}/{len(time_bins)}\")\n",
    "        \n",
    "        # Count tweets\n",
    "        tweet_count = sum(v for k, v in interval_counts.items() if k[0] == time_bin)\n",
    "        print(f\"  Tweets in bin: {tweet_count}\")\n",
    "        \n",
    "        # Create incremental raster\n",
    "        incremental_grid = create_hierarchical_raster_arcpy(\n",
    "            expanded_fc,\n",
    "            time_bin,\n",
    "            interval_counts,\n",
    "            lookups,\n",
    "            reference_layers,\n",
    "            grid_params\n",
    "        )\n",
    "        \n",
    "        # Accumulate\n",
    "        cumulative_grid += incremental_grid\n",
    "        \n",
    "        # Save rasters\n",
    "        time_str = timestamp_dict[time_bin].strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Incremental\n",
    "        inc_filename = f\"{hurricane_name}_tweets_{time_str}.tif\"\n",
    "        inc_path = os.path.join(increment_dir, inc_filename)\n",
    "        inc_raster = arcpy.NumPyArrayToRaster(\n",
    "            incremental_grid,\n",
    "            lower_left,\n",
    "            CELL_SIZE_M,\n",
    "            CELL_SIZE_M\n",
    "        )\n",
    "        arcpy.DefineProjection_management(inc_raster, TARGET_SR)\n",
    "        inc_raster.save(inc_path)\n",
    "        print(f\"    Saved: increment/{inc_filename}\")\n",
    "        \n",
    "        # Cumulative\n",
    "        cum_filename = f\"{hurricane_name}_tweets_{time_str}.tif\"\n",
    "        cum_path = os.path.join(cumulative_dir, cum_filename)\n",
    "        cum_raster = arcpy.NumPyArrayToRaster(\n",
    "            cumulative_grid,\n",
    "            lower_left,\n",
    "            CELL_SIZE_M,\n",
    "            CELL_SIZE_M\n",
    "        )\n",
    "        arcpy.DefineProjection_management(cum_raster, TARGET_SR)\n",
    "        cum_raster.save(cum_path)\n",
    "        print(f\"    Saved: cumulative/{cum_filename}\")\n",
    "        \n",
    "        print(f\"  Incremental max: {np.max(incremental_grid):.2f}\")\n",
    "        print(f\"  Cumulative max: {np.max(cumulative_grid):.2f}\")\n",
    "    \n",
    "    print(f\"\\n{hurricane_name.upper()} processing complete!\")\n",
    "    return hurricane_dir\n",
    "\n",
    "\n",
    "print(\"Hurricane processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXECUTE PIPELINE\n",
    "\n",
    "Run the following cells to execute the full pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Hurricane Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"HURRICANE SOCIAL MEDIA ANALYSIS - ArcPy Implementation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "francine_fc, francine_meta = load_hurricane_data_arcpy(FRANCINE_PATH, \"francine\")\n",
    "helene_fc, helene_meta = load_hurricane_data_arcpy(HELENE_PATH, \"helene\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Reference Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_layers = load_reference_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Hierarchical Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookups = create_hierarchical_lookups_arcpy(reference_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Expand Tweets by Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "francine_expanded = expand_tweets_arcpy(\n",
    "    francine_fc,\n",
    "    lookups,\n",
    "    \"francine_expanded\",\n",
    "    \"FRANCINE\"\n",
    ")\n",
    "\n",
    "helene_expanded = expand_tweets_arcpy(\n",
    "    helene_fc,\n",
    "    lookups,\n",
    "    \"helene_expanded\",\n",
    "    \"HELENE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create Interval Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "francine_interval_counts = create_interval_counts_dict(francine_expanded)\n",
    "helene_interval_counts = create_interval_counts_dict(helene_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Calculate Grid Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project hurricane data to target SR for extent calculation\n",
    "francine_proj = \"temp_francine_proj\"\n",
    "helene_proj = \"temp_helene_proj\"\n",
    "arcpy.Project_management(francine_fc, francine_proj, TARGET_SR)\n",
    "arcpy.Project_management(helene_fc, helene_proj, TARGET_SR)\n",
    "\n",
    "grid_params = calculate_grid_extent(francine_proj, helene_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Process Hurricane Francine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "francine_output = process_hurricane_arcpy(\n",
    "    \"francine\",\n",
    "    francine_expanded,\n",
    "    francine_meta[\"time_bins\"],\n",
    "    francine_meta[\"timestamp_dict\"],\n",
    "    francine_interval_counts,\n",
    "    lookups,\n",
    "    reference_layers,\n",
    "    grid_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Process Hurricane Helene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helene_output = process_hurricane_arcpy(\n",
    "    \"helene\",\n",
    "    helene_expanded,\n",
    "    helene_meta[\"time_bins\"],\n",
    "    helene_meta[\"timestamp_dict\"],\n",
    "    helene_interval_counts,\n",
    "    lookups,\n",
    "    reference_layers,\n",
    "    grid_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Cleanup & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary feature classes\n",
    "print(\"\\nCleaning up temporary features...\")\n",
    "cleanup_temp_features(\"temp_*\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nOutput Directories:\")\n",
    "print(f\"  Francine: {francine_output}\")\n",
    "print(f\"  Helene:   {helene_output}\")\n",
    "\n",
    "# Count files\n",
    "import glob\n",
    "francine_inc = len(glob.glob(os.path.join(francine_output, 'increment', '*.tif')))\n",
    "francine_cum = len(glob.glob(os.path.join(francine_output, 'cumulative', '*.tif')))\n",
    "helene_inc = len(glob.glob(os.path.join(helene_output, 'increment', '*.tif')))\n",
    "helene_cum = len(glob.glob(os.path.join(helene_output, 'cumulative', '*.tif')))\n",
    "\n",
    "print(f\"\\nRasters Created:\")\n",
    "print(f\"  Francine: {francine_inc} incremental + {francine_cum} cumulative\")\n",
    "print(f\"  Helene:   {helene_inc} incremental + {helene_cum} cumulative\")\n",
    "print(f\"  Total:    {francine_inc + francine_cum + helene_inc + helene_cum} GeoTIFF files\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Rasters are ready in: {OUTPUT_DIR}\")\n",
    "print(f\"  2. Add to ArcGIS Pro map\")\n",
    "print(f\"  3. Configure symbology and time slider\")\n",
    "print(f\"  4. Export animations\")\n",
    "\n",
    "# Check in Spatial Analyst\n",
    "arcpy.CheckInExtension(\"Spatial\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
