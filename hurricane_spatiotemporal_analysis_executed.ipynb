{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurricane Tweet Spatiotemporal Analysis Pipeline\n",
    "## Transforming Social Media Data into Time-Enabled GIS Outputs\n",
    "\n",
    "**Author:** Claude Code  \n",
    "**Date:** 2025-10-29  \n",
    "**Purpose:** Convert hurricane-related tweet streams into ArcGIS Pro-compatible time-enabled spatial outputs\n",
    "\n",
    "---\n",
    "\n",
    "## Solution Overview\n",
    "\n",
    "### Methodology\n",
    "This pipeline implements a **multi-scale vector aggregation approach** with adaptive temporal binning:\n",
    "\n",
    "1. **Spatial Strategy**: Aggregate tweets to state and county boundaries using spatial joins\n",
    "2. **Temporal Strategy**: Adaptive binning (2-hour for Helene, 6-hour for Francine)\n",
    "3. **Entity Resolution**: Hybrid approach combining spatial location with GPE text matching\n",
    "4. **Output Format**: GeoPackage with time-enabled polygon features\n",
    "\n",
    "### Input Data\n",
    "- Hurricane Francine tweets (2,303 records, Sep 9-16, 2024)\n",
    "- Hurricane Helene tweets (3,007 records, Sep 26-27, 2024)\n",
    "- US State boundaries (52 features)\n",
    "- US County boundaries (3,222 features)\n",
    "- Global cities reference database\n",
    "\n",
    "### Output Data\n",
    "- `hurricane_analysis_output.gpkg` containing:\n",
    "  - `helene_states_timeseries`: State-level aggregates over time\n",
    "  - `helene_counties_timeseries`: County-level aggregates over time\n",
    "  - `francine_states_timeseries`: State-level aggregates over time\n",
    "  - `francine_counties_timeseries`: County-level aggregates over time\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:13:02.845235Z",
     "iopub.status.busy": "2025-10-29T20:13:02.845094Z",
     "iopub.status.idle": "2025-10-29T20:13:03.316390Z",
     "shell.execute_reply": "2025-10-29T20:13:03.315661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Setup Complete\n",
      "GeoPandas version: 1.1.1\n",
      "Pandas version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Environment Setup Complete\")\n",
    "print(f\"GeoPandas version: {gpd.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:13:03.342174Z",
     "iopub.status.busy": "2025-10-29T20:13:03.341846Z",
     "iopub.status.idle": "2025-10-29T20:13:03.346532Z",
     "shell.execute_reply": "2025-10-29T20:13:03.346199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully\n",
      "Output will be saved to: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\output\\hurricane_analysis_output.gpkg\n"
     ]
    }
   ],
   "source": [
    "# Input data paths (as specified in requirements)\n",
    "DATA_DIR = Path(r'C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\data')\n",
    "\n",
    "PATHS = {\n",
    "    'helene_tweets': DATA_DIR / 'geojson' / 'helene.geojson',\n",
    "    'francine_tweets': DATA_DIR / 'geojson' / 'francine.geojson',\n",
    "    'states_shp': DATA_DIR / 'shape_files' / 'cb_2023_us_state_20m.shp',\n",
    "    'counties_shp': DATA_DIR / 'shape_files' / 'cb_2023_us_county_20m.shp',\n",
    "    'cities_csv': DATA_DIR / 'tables' / 'cities1000.csv'\n",
    "}\n",
    "\n",
    "# Output path\n",
    "OUTPUT_PATH = Path(r'C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\output')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)\n",
    "OUTPUT_GPKG = OUTPUT_PATH / 'hurricane_analysis_output.gpkg'\n",
    "\n",
    "# Temporal binning configuration (adaptive by hurricane)\n",
    "TIME_CONFIG = {\n",
    "    'helene': {\n",
    "        'bin_hours': 2,  # 2-hour bins for concentrated 2-day event\n",
    "        'description': '2-hour intervals'\n",
    "    },\n",
    "    'francine': {\n",
    "        'bin_hours': 6,  # 6-hour bins for 7-day event\n",
    "        'description': '6-hour intervals'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Target CRS (match reference shapefiles)\n",
    "TARGET_CRS = 'EPSG:4269'  # NAD83\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Output will be saved to: {OUTPUT_GPKG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading Module\n",
    "\n",
    "Load all input datasets and perform initial preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:13:03.348331Z",
     "iopub.status.busy": "2025-10-29T20:13:03.348095Z",
     "iopub.status.idle": "2025-10-29T20:13:04.069614Z",
     "shell.execute_reply": "2025-10-29T20:13:04.068971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODULE 1: DATA LOADING\n",
      "================================================================================\n",
      "Loading Helene tweets from helene.geojson...\n",
      "  Loaded 3007 tweets\n",
      "  Time range: 2024-09-26 02:29:25+00:00 to 2024-09-27 19:59:41+00:00\n",
      "  Duration: 41.5 hours\n",
      "Loading Francine tweets from francine.geojson...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 2303 tweets\n",
      "  Time range: 2024-09-09 11:00:36+00:00 to 2024-09-16 15:24:14+00:00\n",
      "  Duration: 172.4 hours\n",
      "Loading reference boundaries...\n",
      "  Loaded 52 states (CRS: EPSG:4269)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 3222 counties (CRS: EPSG:4269)\n",
      "Loading cities reference...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 17244 US cities (filtered from 161521 global)\n",
      "\n",
      "Data loading complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_tweet_data(geojson_path, hurricane_name):\n",
    "    \"\"\"\n",
    "    Load hurricane tweet GeoJSON and preprocess.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    geojson_path : Path\n",
    "        Path to GeoJSON file\n",
    "    hurricane_name : str\n",
    "        Name of hurricane (for labeling)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame with standardized schema\n",
    "    \"\"\"\n",
    "    print(f\"Loading {hurricane_name} tweets from {geojson_path.name}...\")\n",
    "    \n",
    "    # Load GeoJSON\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    \n",
    "    # Parse time field to datetime\n",
    "    gdf['time'] = pd.to_datetime(gdf['time'])\n",
    "    \n",
    "    # Add hurricane label\n",
    "    gdf['hurricane'] = hurricane_name\n",
    "    \n",
    "    # Ensure CRS is set (should be EPSG:4326 from GeoJSON)\n",
    "    if gdf.crs is None:\n",
    "        gdf.set_crs('EPSG:4326', inplace=True)\n",
    "    \n",
    "    # Reproject to target CRS\n",
    "    gdf = gdf.to_crs(TARGET_CRS)\n",
    "    \n",
    "    # Handle missing GPE/FAC/LOC values\n",
    "    for col in ['GPE', 'FAC', 'LOC']:\n",
    "        if col in gdf.columns:\n",
    "            gdf[col] = gdf[col].fillna('').astype(str)\n",
    "    \n",
    "    print(f\"  Loaded {len(gdf)} tweets\")\n",
    "    print(f\"  Time range: {gdf['time'].min()} to {gdf['time'].max()}\")\n",
    "    print(f\"  Duration: {(gdf['time'].max() - gdf['time'].min()).total_seconds() / 3600:.1f} hours\")\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "\n",
    "def load_reference_boundaries():\n",
    "    \"\"\"\n",
    "    Load state and county boundary shapefiles.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (states_gdf, counties_gdf)\n",
    "    \"\"\"\n",
    "    print(\"Loading reference boundaries...\")\n",
    "    \n",
    "    # Load states\n",
    "    states = gpd.read_file(PATHS['states_shp'])\n",
    "    print(f\"  Loaded {len(states)} states (CRS: {states.crs})\")\n",
    "    \n",
    "    # Load counties\n",
    "    counties = gpd.read_file(PATHS['counties_shp'])\n",
    "    print(f\"  Loaded {len(counties)} counties (CRS: {counties.crs})\")\n",
    "    \n",
    "    # Calculate areas in square kilometers for density calculations\n",
    "    states['area_sqkm'] = states.to_crs('EPSG:5070').geometry.area / 1e6  # Albers Equal Area\n",
    "    counties['area_sqkm'] = counties.to_crs('EPSG:5070').geometry.area / 1e6\n",
    "    \n",
    "    return states, counties\n",
    "\n",
    "\n",
    "def load_cities_reference():\n",
    "    \"\"\"\n",
    "    Load cities reference database (GeoNames format).\n",
    "    Filter to US cities only for efficiency.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with city names and coordinates\n",
    "    \"\"\"\n",
    "    print(\"Loading cities reference...\")\n",
    "    \n",
    "    cities = pd.read_csv(PATHS['cities_csv'])\n",
    "    \n",
    "    # Filter to US cities only\n",
    "    us_cities = cities[cities['country_code'] == 'US'].copy()\n",
    "    \n",
    "    print(f\"  Loaded {len(us_cities)} US cities (filtered from {len(cities)} global)\")\n",
    "    \n",
    "    return us_cities\n",
    "\n",
    "\n",
    "# Execute data loading\n",
    "print(\"=\" * 80)\n",
    "print(\"MODULE 1: DATA LOADING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "helene_tweets = load_tweet_data(PATHS['helene_tweets'], 'Helene')\n",
    "francine_tweets = load_tweet_data(PATHS['francine_tweets'], 'Francine')\n",
    "states_ref, counties_ref = load_reference_boundaries()\n",
    "cities_ref = load_cities_reference()\n",
    "\n",
    "print(\"\\nData loading complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Processing Module\n",
    "\n",
    "Perform spatial joins to assign tweets to counties and states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:13:04.071477Z",
     "iopub.status.busy": "2025-10-29T20:13:04.071315Z",
     "iopub.status.idle": "2025-10-29T20:13:04.104557Z",
     "shell.execute_reply": "2025-10-29T20:13:04.104069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODULE 2: SPATIAL PROCESSING\n",
      "================================================================================\n",
      "\n",
      "Processing Helene tweets...\n",
      "Spatial join: 3007 tweets to boundaries...\n",
      "  18 tweets not in any county, trying state-level join...\n",
      "  Spatial join complete:\n",
      "    2989 tweets matched to counties (99.4%)\n",
      "    2989 tweets matched to states (99.4%)\n",
      "    18 tweets outside all boundaries\n",
      "\n",
      "Processing Francine tweets...\n",
      "Spatial join: 2303 tweets to boundaries...\n",
      "  Spatial join complete:\n",
      "    2303 tweets matched to counties (100.0%)\n",
      "    2303 tweets matched to states (100.0%)\n",
      "    0 tweets outside all boundaries\n",
      "\n",
      "Spatial processing complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def spatial_join_tweets_to_boundaries(tweets_gdf, counties_gdf, states_gdf):\n",
    "    \"\"\"\n",
    "    Spatially join tweets to county and state boundaries.\n",
    "    \n",
    "    This is the primary method for geographic assignment,\n",
    "    using actual tweet coordinates rather than text mentions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tweets_gdf : GeoDataFrame\n",
    "        Tweet points\n",
    "    counties_gdf : GeoDataFrame\n",
    "        County polygons\n",
    "    states_gdf : GeoDataFrame\n",
    "        State polygons\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame with county and state assignments\n",
    "    \"\"\"\n",
    "    print(f\"Spatial join: {len(tweets_gdf)} tweets to boundaries...\")\n",
    "    \n",
    "    # Ensure same CRS\n",
    "    tweets_gdf = tweets_gdf.to_crs(TARGET_CRS)\n",
    "    counties_gdf = counties_gdf.to_crs(TARGET_CRS)\n",
    "    states_gdf = states_gdf.to_crs(TARGET_CRS)\n",
    "    \n",
    "    # Join to counties (gets us state too via county attributes)\n",
    "    tweets_with_county = gpd.sjoin(\n",
    "        tweets_gdf,\n",
    "        counties_gdf[['geometry', 'NAME', 'STUSPS', 'STATE_NAME', 'GEOID']],\n",
    "        how='left',\n",
    "        predicate='within'\n",
    "    )\n",
    "    \n",
    "    # Rename joined columns for clarity\n",
    "    tweets_with_county = tweets_with_county.rename(columns={\n",
    "        'NAME': 'county_name',\n",
    "        'STUSPS': 'state_abbr',\n",
    "        'STATE_NAME': 'state_name',\n",
    "        'GEOID': 'county_geoid'\n",
    "    })\n",
    "    \n",
    "    # For tweets not within any county, try state-level join\n",
    "    # First, separate tweets with and without county assignments\n",
    "    no_county_mask = tweets_with_county['county_name'].isna()\n",
    "    no_county_indices = tweets_with_county[no_county_mask].index\n",
    "    \n",
    "    if len(no_county_indices) > 0:\n",
    "        print(f\"  {len(no_county_indices)} tweets not in any county, trying state-level join...\")\n",
    "        \n",
    "        # Get the original tweet data for orphaned tweets (before any joins)\n",
    "        orphaned_tweets = tweets_gdf.loc[no_county_indices].copy()\n",
    "        \n",
    "        # Join orphaned tweets to states\n",
    "        orphaned_with_state = gpd.sjoin(\n",
    "            orphaned_tweets,\n",
    "            states_gdf[['geometry', 'NAME', 'STUSPS']],\n",
    "            how='left',\n",
    "            predicate='within'\n",
    "        )\n",
    "        \n",
    "        # Update state info for these tweets in the main dataframe\n",
    "        for idx in orphaned_with_state.index:\n",
    "            if pd.notna(orphaned_with_state.loc[idx, 'NAME']):\n",
    "                tweets_with_county.loc[idx, 'state_name'] = orphaned_with_state.loc[idx, 'NAME']\n",
    "                tweets_with_county.loc[idx, 'state_abbr'] = orphaned_with_state.loc[idx, 'STUSPS']\n",
    "    \n",
    "    # Clean up index_right column from spatial join\n",
    "    if 'index_right' in tweets_with_county.columns:\n",
    "        tweets_with_county = tweets_with_county.drop(columns=['index_right'])\n",
    "    \n",
    "    # Report statistics\n",
    "    tweets_with_state = tweets_with_county[tweets_with_county['state_name'].notna()]\n",
    "    tweets_with_county_complete = tweets_with_county[tweets_with_county['county_name'].notna()]\n",
    "    \n",
    "    print(f\"  Spatial join complete:\")\n",
    "    print(f\"    {len(tweets_with_county_complete)} tweets matched to counties ({len(tweets_with_county_complete)/len(tweets_gdf)*100:.1f}%)\")\n",
    "    print(f\"    {len(tweets_with_state)} tweets matched to states ({len(tweets_with_state)/len(tweets_gdf)*100:.1f}%)\")\n",
    "    print(f\"    {len(tweets_gdf) - len(tweets_with_state)} tweets outside all boundaries\")\n",
    "    \n",
    "    return tweets_with_county\n",
    "\n",
    "\n",
    "# Execute spatial processing\n",
    "print(\"=\" * 80)\n",
    "print(\"MODULE 2: SPATIAL PROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nProcessing Helene tweets...\")\n",
    "helene_spatial = spatial_join_tweets_to_boundaries(helene_tweets, counties_ref, states_ref)\n",
    "\n",
    "print(\"\\nProcessing Francine tweets...\")\n",
    "francine_spatial = spatial_join_tweets_to_boundaries(francine_tweets, counties_ref, states_ref)\n",
    "\n",
    "print(\"\\nSpatial processing complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Binning Module\n",
    "\n",
    "Create time bins and assign tweets to temporal intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:13:04.106267Z",
     "iopub.status.busy": "2025-10-29T20:13:04.106072Z",
     "iopub.status.idle": "2025-10-29T20:13:04.132891Z",
     "shell.execute_reply": "2025-10-29T20:13:04.132312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODULE 3: TEMPORAL BINNING\n",
      "================================================================================\n",
      "\n",
      "Binning Helene tweets...\n",
      "Creating 2-hour time bins for Helene...\n",
      "  Time range: 2024-09-26 02:29:25+00:00 to 2024-09-27 19:59:41+00:00\n",
      "  Bin range: 2024-09-26 02:00:00+00:00 to 2024-09-27 20:00:00+00:00\n",
      "  Number of bins: 21\n",
      "  Tweets per bin: min=57, max=239, mean=143.2\n",
      "\n",
      "Binning Francine tweets...\n",
      "Creating 6-hour time bins for Francine...\n",
      "  Time range: 2024-09-09 11:00:36+00:00 to 2024-09-16 15:24:14+00:00\n",
      "  Bin range: 2024-09-09 06:00:00+00:00 to 2024-09-16 18:00:00+00:00\n",
      "  Number of bins: 30\n",
      "  Tweets per bin: min=1, max=445, mean=79.4\n",
      "\n",
      "Temporal binning complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_time_bins(tweets_gdf, hurricane_name, bin_hours):\n",
    "    \"\"\"\n",
    "    Create temporal bins and assign each tweet to a bin.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tweets_gdf : GeoDataFrame\n",
    "        Tweets with time field\n",
    "    hurricane_name : str\n",
    "        Hurricane name (for reporting)\n",
    "    bin_hours : int\n",
    "        Hours per bin\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame with bin assignments and bin metadata DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Creating {bin_hours}-hour time bins for {hurricane_name}...\")\n",
    "    \n",
    "    # Get time range\n",
    "    min_time = tweets_gdf['time'].min()\n",
    "    max_time = tweets_gdf['time'].max()\n",
    "    \n",
    "    # Round min_time down to nearest bin interval\n",
    "    start_time = min_time.floor(f'{bin_hours}h')\n",
    "    \n",
    "    # Round max_time up to nearest bin interval\n",
    "    end_time = (max_time.ceil(f'{bin_hours}h'))\n",
    "    \n",
    "    # Create bin edges\n",
    "    bin_edges = pd.date_range(start=start_time, end=end_time, freq=f'{bin_hours}h')\n",
    "    \n",
    "    print(f\"  Time range: {min_time} to {max_time}\")\n",
    "    print(f\"  Bin range: {start_time} to {end_time}\")\n",
    "    print(f\"  Number of bins: {len(bin_edges) - 1}\")\n",
    "    \n",
    "    # Assign each tweet to a bin\n",
    "    tweets_gdf['time_bin_idx'] = pd.cut(\n",
    "        tweets_gdf['time'],\n",
    "        bins=bin_edges,\n",
    "        labels=range(len(bin_edges) - 1),\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Create bin metadata table\n",
    "    bins_metadata = pd.DataFrame({\n",
    "        'bin_idx': range(len(bin_edges) - 1),\n",
    "        'time_start': bin_edges[:-1],\n",
    "        'time_end': bin_edges[1:]\n",
    "    })\n",
    "    \n",
    "    bins_metadata['time_mid'] = bins_metadata['time_start'] + (bins_metadata['time_end'] - bins_metadata['time_start']) / 2\n",
    "    \n",
    "    # Join bin metadata to tweets\n",
    "    tweets_gdf['time_bin_idx'] = tweets_gdf['time_bin_idx'].astype(float)  # Handle NaN\n",
    "    tweets_with_bins = tweets_gdf.merge(\n",
    "        bins_metadata,\n",
    "        left_on='time_bin_idx',\n",
    "        right_on='bin_idx',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Report bin distribution\n",
    "    bin_counts = tweets_with_bins.groupby('bin_idx').size()\n",
    "    print(f\"  Tweets per bin: min={bin_counts.min()}, max={bin_counts.max()}, mean={bin_counts.mean():.1f}\")\n",
    "    \n",
    "    return tweets_with_bins, bins_metadata\n",
    "\n",
    "\n",
    "# Execute temporal binning\n",
    "print(\"=\" * 80)\n",
    "print(\"MODULE 3: TEMPORAL BINNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nBinning Helene tweets...\")\n",
    "helene_binned, helene_bins = create_time_bins(\n",
    "    helene_spatial,\n",
    "    'Helene',\n",
    "    TIME_CONFIG['helene']['bin_hours']\n",
    ")\n",
    "\n",
    "print(\"\\nBinning Francine tweets...\")\n",
    "francine_binned, francine_bins = create_time_bins(\n",
    "    francine_spatial,\n",
    "    'Francine',\n",
    "    TIME_CONFIG['francine']['bin_hours']\n",
    ")\n",
    "\n",
    "print(\"\\nTemporal binning complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aggregation Module\n",
    "\n",
    "Aggregate tweets to state and county polygons by time bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:13:04.134874Z",
     "iopub.status.busy": "2025-10-29T20:13:04.134726Z",
     "iopub.status.idle": "2025-10-29T20:13:04.193027Z",
     "shell.execute_reply": "2025-10-29T20:13:04.192522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODULE 4: AGGREGATION\n",
      "================================================================================\n",
      "\n",
      "Aggregating Helene data...\n",
      "Aggregating Helene tweets to states...\n",
      "  2989 tweets with state assignments\n",
      "  Created 131 state-time features\n",
      "  States represented: 10\n",
      "  Time bins: 21\n",
      "Aggregating Helene tweets to counties...\n",
      "  2989 tweets with county assignments\n",
      "  Created 561 county-time features\n",
      "  Counties represented: 128\n",
      "  Time bins: 21\n",
      "\n",
      "Aggregating Francine data...\n",
      "Aggregating Francine tweets to states...\n",
      "  2303 tweets with state assignments\n",
      "  Created 129 state-time features\n",
      "  States represented: 10\n",
      "  Time bins: 29\n",
      "Aggregating Francine tweets to counties...\n",
      "  2303 tweets with county assignments\n",
      "  Created 381 county-time features\n",
      "  Counties represented: 83\n",
      "  Time bins: 29\n",
      "\n",
      "Aggregation complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def aggregate_to_states(tweets_gdf, states_ref, hurricane_name):\n",
    "    \"\"\"\n",
    "    Aggregate tweets to state boundaries by time bin.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tweets_gdf : GeoDataFrame\n",
    "        Tweets with spatial and temporal assignments\n",
    "    states_ref : GeoDataFrame\n",
    "        State boundary polygons\n",
    "    hurricane_name : str\n",
    "        Hurricane name\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame with aggregated state-level features\n",
    "    \"\"\"\n",
    "    print(f\"Aggregating {hurricane_name} tweets to states...\")\n",
    "    \n",
    "    # Filter to tweets with state assignments\n",
    "    tweets_with_state = tweets_gdf[tweets_gdf['state_name'].notna()].copy()\n",
    "    print(f\"  {len(tweets_with_state)} tweets with state assignments\")\n",
    "    \n",
    "    # Group by state and time bin\n",
    "    agg_dict = {\n",
    "        'time': 'count',  # Count tweets\n",
    "        'GPE': lambda x: ', '.join(set([item.strip() for s in x for item in str(s).split(',') if item.strip()])),  # Collect unique GPE mentions\n",
    "    }\n",
    "    \n",
    "    state_timeseries = tweets_with_state.groupby(\n",
    "        ['state_name', 'state_abbr', 'bin_idx', 'time_start', 'time_end', 'time_mid'],\n",
    "        as_index=False\n",
    "    ).agg(agg_dict)\n",
    "    \n",
    "    # Rename aggregated columns\n",
    "    state_timeseries = state_timeseries.rename(columns={'time': 'tweet_count'})\n",
    "    state_timeseries = state_timeseries.rename(columns={'GPE': 'mentioned_entities'})\n",
    "    \n",
    "    # Join with state geometries\n",
    "    state_features = states_ref[['NAME', 'STUSPS', 'geometry', 'area_sqkm']].merge(\n",
    "        state_timeseries,\n",
    "        left_on='NAME',\n",
    "        right_on='state_name',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Calculate density\n",
    "    state_features['tweets_per_1000sqkm'] = (state_features['tweet_count'] / state_features['area_sqkm']) * 1000\n",
    "    \n",
    "    # Add hurricane label\n",
    "    state_features['hurricane'] = hurricane_name\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    state_gdf = gpd.GeoDataFrame(state_features, geometry='geometry', crs=TARGET_CRS)\n",
    "    \n",
    "    # Select final columns\n",
    "    final_cols = [\n",
    "        'state_name', 'state_abbr', 'hurricane',\n",
    "        'time_start', 'time_end', 'time_mid',\n",
    "        'tweet_count', 'tweets_per_1000sqkm', 'area_sqkm',\n",
    "        'mentioned_entities', 'geometry'\n",
    "    ]\n",
    "    state_gdf = state_gdf[final_cols]\n",
    "    \n",
    "    print(f\"  Created {len(state_gdf)} state-time features\")\n",
    "    print(f\"  States represented: {state_gdf['state_name'].nunique()}\")\n",
    "    print(f\"  Time bins: {state_gdf['time_start'].nunique()}\")\n",
    "    \n",
    "    return state_gdf\n",
    "\n",
    "\n",
    "def aggregate_to_counties(tweets_gdf, counties_ref, hurricane_name):\n",
    "    \"\"\"\n",
    "    Aggregate tweets to county boundaries by time bin.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tweets_gdf : GeoDataFrame\n",
    "        Tweets with spatial and temporal assignments\n",
    "    counties_ref : GeoDataFrame\n",
    "        County boundary polygons\n",
    "    hurricane_name : str\n",
    "        Hurricane name\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame with aggregated county-level features\n",
    "    \"\"\"\n",
    "    print(f\"Aggregating {hurricane_name} tweets to counties...\")\n",
    "    \n",
    "    # Filter to tweets with county assignments\n",
    "    tweets_with_county = tweets_gdf[tweets_gdf['county_name'].notna()].copy()\n",
    "    print(f\"  {len(tweets_with_county)} tweets with county assignments\")\n",
    "    \n",
    "    # Group by county and time bin\n",
    "    agg_dict = {\n",
    "        'time': 'count',\n",
    "        'GPE': lambda x: ', '.join(set([item.strip() for s in x for item in str(s).split(',') if item.strip()])),\n",
    "    }\n",
    "    \n",
    "    county_timeseries = tweets_with_county.groupby(\n",
    "        ['county_geoid', 'county_name', 'state_name', 'state_abbr', 'bin_idx', 'time_start', 'time_end', 'time_mid'],\n",
    "        as_index=False\n",
    "    ).agg(agg_dict)\n",
    "    \n",
    "    # Rename aggregated columns\n",
    "    county_timeseries = county_timeseries.rename(columns={'time': 'tweet_count'})\n",
    "    county_timeseries = county_timeseries.rename(columns={'GPE': 'mentioned_entities'})\n",
    "    \n",
    "    # Join with county geometries\n",
    "    county_features = counties_ref[['GEOID', 'NAME', 'STATE_NAME', 'geometry', 'area_sqkm']].merge(\n",
    "        county_timeseries,\n",
    "        left_on='GEOID',\n",
    "        right_on='county_geoid',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Calculate density\n",
    "    county_features['tweets_per_1000sqkm'] = (county_features['tweet_count'] / county_features['area_sqkm']) * 1000\n",
    "    \n",
    "    # Add hurricane label\n",
    "    county_features['hurricane'] = hurricane_name\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    county_gdf = gpd.GeoDataFrame(county_features, geometry='geometry', crs=TARGET_CRS)\n",
    "    \n",
    "    # Select final columns\n",
    "    final_cols = [\n",
    "        'county_name', 'state_name', 'state_abbr', 'hurricane',\n",
    "        'time_start', 'time_end', 'time_mid',\n",
    "        'tweet_count', 'tweets_per_1000sqkm', 'area_sqkm',\n",
    "        'mentioned_entities', 'geometry'\n",
    "    ]\n",
    "    county_gdf = county_gdf[final_cols]\n",
    "    \n",
    "    print(f\"  Created {len(county_gdf)} county-time features\")\n",
    "    print(f\"  Counties represented: {county_gdf['county_name'].nunique()}\")\n",
    "    print(f\"  Time bins: {county_gdf['time_start'].nunique()}\")\n",
    "    \n",
    "    return county_gdf\n",
    "\n",
    "\n",
    "# Execute aggregation\n",
    "print(\"=\" * 80)\n",
    "print(\"MODULE 4: AGGREGATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nAggregating Helene data...\")\n",
    "helene_states_agg = aggregate_to_states(helene_binned, states_ref, 'Helene')\n",
    "helene_counties_agg = aggregate_to_counties(helene_binned, counties_ref, 'Helene')\n",
    "\n",
    "print(\"\\nAggregating Francine data...\")\n",
    "francine_states_agg = aggregate_to_states(francine_binned, states_ref, 'Francine')\n",
    "francine_counties_agg = aggregate_to_counties(francine_binned, counties_ref, 'Francine')\n",
    "\n",
    "print(\"\\nAggregation complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output Generation Module\n",
    "\n",
    "Export aggregated features to GeoPackage format for ArcGIS Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:13:04.195297Z",
     "iopub.status.busy": "2025-10-29T20:13:04.195144Z",
     "iopub.status.idle": "2025-10-29T20:13:04.403377Z",
     "shell.execute_reply": "2025-10-29T20:13:04.402895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODULE 5: OUTPUT GENERATION\n",
      "================================================================================\n",
      "Exporting layers to C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\output\\hurricane_analysis_output.gpkg...\n",
      "  Writing layer 'helene_states_timeseries': 131 features...\n",
      "  Writing layer 'helene_counties_timeseries': 561 features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Writing layer 'francine_states_timeseries': 129 features..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Writing layer 'francine_counties_timeseries': 381 features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Successfully created C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\output\\hurricane_analysis_output.gpkg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  File size: 2.09 MB\n",
      "\n",
      "Output generation complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def export_to_geopackage(layer_dict, output_path):\n",
    "    \"\"\"\n",
    "    Export multiple layers to a single GeoPackage.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    layer_dict : dict\n",
    "        Dictionary mapping layer names to GeoDataFrames\n",
    "    output_path : Path\n",
    "        Output GeoPackage path\n",
    "    \"\"\"\n",
    "    print(f\"Exporting layers to {output_path}...\")\n",
    "    \n",
    "    # Remove existing file if present\n",
    "    if output_path.exists():\n",
    "        output_path.unlink()\n",
    "        print(f\"  Removed existing file\")\n",
    "    \n",
    "    for layer_name, gdf in layer_dict.items():\n",
    "        print(f\"  Writing layer '{layer_name}': {len(gdf)} features...\")\n",
    "        \n",
    "        # Ensure datetime columns are properly formatted\n",
    "        for col in ['time_start', 'time_end', 'time_mid']:\n",
    "            if col in gdf.columns:\n",
    "                gdf[col] = pd.to_datetime(gdf[col])\n",
    "        \n",
    "        # Write to GeoPackage\n",
    "        gdf.to_file(output_path, layer=layer_name, driver='GPKG')\n",
    "    \n",
    "    print(f\"\\n  Successfully created {output_path}\")\n",
    "    print(f\"  File size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "\n",
    "# Execute export\n",
    "print(\"=\" * 80)\n",
    "print(\"MODULE 5: OUTPUT GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "output_layers = {\n",
    "    'helene_states_timeseries': helene_states_agg,\n",
    "    'helene_counties_timeseries': helene_counties_agg,\n",
    "    'francine_states_timeseries': francine_states_agg,\n",
    "    'francine_counties_timeseries': francine_counties_agg\n",
    "}\n",
    "\n",
    "export_to_geopackage(output_layers, OUTPUT_GPKG)\n",
    "\n",
    "print(\"\\nOutput generation complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:13:04.405118Z",
     "iopub.status.busy": "2025-10-29T20:13:04.404888Z",
     "iopub.status.idle": "2025-10-29T20:13:04.419606Z",
     "shell.execute_reply": "2025-10-29T20:13:04.418982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "HELENE (Hurricane)\n",
      "========================================\n",
      "\n",
      "State-level aggregates:\n",
      "  Total features: 131\n",
      "  States affected: 10\n",
      "  Time bins: 21\n",
      "  Total tweet count: 2989\n",
      "  Top 5 states by total tweets:\n",
      "    Florida: 2274\n",
      "    Georgia: 446\n",
      "    North Carolina: 101\n",
      "    South Carolina: 70\n",
      "    Tennessee: 23\n",
      "\n",
      "County-level aggregates:\n",
      "  Total features: 561\n",
      "  Counties affected: 128\n",
      "  Time bins: 21\n",
      "  Total tweet count: 2989\n",
      "  Top 5 counties by total tweets:\n",
      "    Polk, FL: 1484\n",
      "    Dodge, GA: 147\n",
      "    Leon, FL: 142\n",
      "    Hillsborough, FL: 138\n",
      "    Bacon, GA: 133\n",
      "\n",
      "========================================\n",
      "FRANCINE (Hurricane)\n",
      "========================================\n",
      "\n",
      "State-level aggregates:\n",
      "  Total features: 129\n",
      "  States affected: 10\n",
      "  Time bins: 29\n",
      "  Total tweet count: 2303\n",
      "  Top 5 states by total tweets:\n",
      "    Louisiana: 2025\n",
      "    Mississippi: 83\n",
      "    Florida: 65\n",
      "    Texas: 47\n",
      "    Tennessee: 23\n",
      "\n",
      "County-level aggregates:\n",
      "  Total features: 381\n",
      "  Counties affected: 83\n",
      "  Time bins: 29\n",
      "  Total tweet count: 2303\n",
      "  Top 5 counties by total tweets:\n",
      "    Avoyelles, LA: 1201\n",
      "    Orleans, LA: 267\n",
      "    Terrebonne, LA: 136\n",
      "    Jefferson, LA: 117\n",
      "    St. Mary, LA: 76\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"HELENE (Hurricane)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\nState-level aggregates:\")\n",
    "print(f\"  Total features: {len(helene_states_agg)}\")\n",
    "print(f\"  States affected: {helene_states_agg['state_name'].nunique()}\")\n",
    "print(f\"  Time bins: {helene_states_agg['time_start'].nunique()}\")\n",
    "print(f\"  Total tweet count: {helene_states_agg['tweet_count'].sum()}\")\n",
    "print(f\"  Top 5 states by total tweets:\")\n",
    "top_helene_states = helene_states_agg.groupby('state_name')['tweet_count'].sum().sort_values(ascending=False).head(5)\n",
    "for state, count in top_helene_states.items():\n",
    "    print(f\"    {state}: {count}\")\n",
    "\n",
    "print(f\"\\nCounty-level aggregates:\")\n",
    "print(f\"  Total features: {len(helene_counties_agg)}\")\n",
    "print(f\"  Counties affected: {helene_counties_agg['county_name'].nunique()}\")\n",
    "print(f\"  Time bins: {helene_counties_agg['time_start'].nunique()}\")\n",
    "print(f\"  Total tweet count: {helene_counties_agg['tweet_count'].sum()}\")\n",
    "print(f\"  Top 5 counties by total tweets:\")\n",
    "top_helene_counties = helene_counties_agg.groupby(['county_name', 'state_abbr'])['tweet_count'].sum().sort_values(ascending=False).head(5)\n",
    "for (county, state), count in top_helene_counties.items():\n",
    "    print(f\"    {county}, {state}: {count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"FRANCINE (Hurricane)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\nState-level aggregates:\")\n",
    "print(f\"  Total features: {len(francine_states_agg)}\")\n",
    "print(f\"  States affected: {francine_states_agg['state_name'].nunique()}\")\n",
    "print(f\"  Time bins: {francine_states_agg['time_start'].nunique()}\")\n",
    "print(f\"  Total tweet count: {francine_states_agg['tweet_count'].sum()}\")\n",
    "print(f\"  Top 5 states by total tweets:\")\n",
    "top_francine_states = francine_states_agg.groupby('state_name')['tweet_count'].sum().sort_values(ascending=False).head(5)\n",
    "for state, count in top_francine_states.items():\n",
    "    print(f\"    {state}: {count}\")\n",
    "\n",
    "print(f\"\\nCounty-level aggregates:\")\n",
    "print(f\"  Total features: {len(francine_counties_agg)}\")\n",
    "print(f\"  Counties affected: {francine_counties_agg['county_name'].nunique()}\")\n",
    "print(f\"  Time bins: {francine_counties_agg['time_start'].nunique()}\")\n",
    "print(f\"  Total tweet count: {francine_counties_agg['tweet_count'].sum()}\")\n",
    "print(f\"  Top 5 counties by total tweets:\")\n",
    "top_francine_counties = francine_counties_agg.groupby(['county_name', 'state_abbr'])['tweet_count'].sum().sort_values(ascending=False).head(5)\n",
    "for (county, state), count in top_francine_counties.items():\n",
    "    print(f\"    {county}, {state}: {count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ArcGIS Pro Usage Instructions\n",
    "\n",
    "### Loading the Data in ArcGIS Pro\n",
    "\n",
    "1. **Add Data to Map**\n",
    "   - Open ArcGIS Pro\n",
    "   - Create new Map view\n",
    "   - Add Data → Browse to `hurricane_analysis_output.gpkg`\n",
    "   - Select desired layer (state or county level, Helene or Francine)\n",
    "\n",
    "2. **Enable Time**\n",
    "   - Right-click layer → Properties\n",
    "   - Navigate to Time tab\n",
    "   - Check \"Layer Time\" box\n",
    "   - Configure:\n",
    "     - **Time Type**: `Time Instant with Time Extent` or `Time Extent`\n",
    "     - **Start Time Field**: `time_start`\n",
    "     - **End Time Field**: `time_end`\n",
    "   - Click OK\n",
    "\n",
    "3. **Symbolize by Tweet Count**\n",
    "   - Right-click layer → Symbology\n",
    "   - Choose **Graduated Colors**\n",
    "   - Field: `tweet_count` (or `tweets_per_1000sqkm` for density)\n",
    "   - Method: Natural Breaks (Jenks) or Quantile\n",
    "   - Color Scheme: Yellow-Orange-Red (sequential)\n",
    "   - Adjust class breaks as needed\n",
    "\n",
    "4. **Activate Time Slider**\n",
    "   - View → Time Slider (or click Time Slider icon)\n",
    "   - Configure:\n",
    "     - Span: Current Time Extent\n",
    "     - Step Interval: Matches your bins (2 hours for Helene, 6 hours for Francine)\n",
    "   - Click Play to animate\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **State-level layers**: Show broad regional patterns, good for overview\n",
    "- **County-level layers**: Show detailed local hotspots\n",
    "- **tweet_count**: Absolute number of tweets (affected by population)\n",
    "- **tweets_per_1000sqkm**: Normalized density (better for comparing regions)\n",
    "- **mentioned_entities**: Text field showing all GPE entities mentioned in tweets for that polygon-time combination\n",
    "\n",
    "### Recommended Analysis Workflows\n",
    "\n",
    "1. **Identify Peak Impact Times**: Use time slider to find bins with highest counts\n",
    "2. **Compare Hurricanes**: Load both Francine and Helene layers to compare spatial footprints\n",
    "3. **Hotspot Analysis**: Use county-level data for Emerging Hot Spot Analysis tool\n",
    "4. **Export Maps**: Create map series showing evolution over key time periods\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Notes\n",
    "\n",
    "- **CRS**: EPSG:4269 (NAD83) - matches Census boundaries\n",
    "- **Temporal Binning**: Adaptive (2h for Helene, 6h for Francine)\n",
    "- **Spatial Assignment**: Primary method is spatial join (tweet coordinates), GPE field is secondary\n",
    "- **Area Calculations**: Computed in EPSG:5070 (Albers Equal Area) for accuracy\n",
    "- **Density Metric**: Tweets per 1,000 sq km (scale chosen for readability)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Complete\n",
    "\n",
    "Output file: `hurricane_analysis_output.gpkg` contains 4 layers ready for ArcGIS Pro time-enabled visualization.\n",
    "\n",
    "This notebook is self-contained and reproducible. Re-run all cells to regenerate outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
