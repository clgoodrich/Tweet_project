{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-03T21:01:20.211202Z",
     "start_time": "2025-11-03T21:01:20.209928Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:01:21.545948Z",
     "start_time": "2025-11-03T21:01:20.214495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import folium\n",
    "from click.formatting import iter_rows\n",
    "from folium import plugins\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import folium\n",
    "from folium import plugins\n",
    "import json\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "\n",
    "def get_project_root():\n",
    "    # This assumes the notebook is running from within the project's root folder.\n",
    "    return os.getcwd()\n",
    "\n",
    "def get_data_file_path(*path_segments):\n",
    "    project_root = get_project_root()\n",
    "    # Navigate to the data folder from the project root.\n",
    "    return os.path.join(project_root, *path_segments)\n",
    "\n",
    "# Data loading functions\n",
    "def get_geojson():\n",
    "    \"\"\"Get path to helene.geojson\"\"\"\n",
    "    geojson = get_data_file_path('data', 'geojson', 'helene.geojson')\n",
    "    print(geojson)\n",
    "    return gpd.read_file(geojson)\n",
    "\n",
    "def get_cities():\n",
    "    df_path = get_data_file_path('data', 'tables', 'cities1000.csv')\n",
    "    # df = pd.read_csv(df_path)\n",
    "    df = pd.read_csv(df_path, low_memory=False)\n",
    "\n",
    "    us_cities_df = df[\n",
    "        (df['country_code'] == 'US') &\n",
    "        (df['feature_class'] == 'P') &\n",
    "        (df['population'].notna()) &\n",
    "        (df['latitude'].notna()) &\n",
    "        (df['longitude'].notna())\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    us_cities_gdf = gpd.GeoDataFrame(\n",
    "        us_cities_df,\n",
    "        geometry=gpd.points_from_xy(us_cities_df.longitude, us_cities_df.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    return us_cities_gdf\n",
    "\n",
    "def get_states():\n",
    "    gdf_path = get_data_file_path('data', 'shape_files', \"cb_2023_us_state_20m.shp\")\n",
    "    return gpd.read_file(gdf_path)\n",
    "\n",
    "def get_counties():\n",
    "    gdf_path = get_data_file_path('data', 'shape_files', \"cb_2023_us_county_20m.shp\")\n",
    "    return gpd.read_file(gdf_path)\n",
    "\n",
    "tweets_gdf = get_geojson().to_crs(\"EPSG:4326\")\n",
    "us_cities_gdf = get_cities().to_crs(\"EPSG:4326\")\n",
    "us_states_gdf = get_states().to_crs(\"EPSG:4326\")\n",
    "us_counties_gdf = get_counties().to_crs(\"EPSG:4326\")"
   ],
   "id": "e17b2216ad9d0d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\data\\geojson\\helene.geojson\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:01:32.212392Z",
     "start_time": "2025-11-03T21:01:21.599023Z"
    }
   },
   "cell_type": "code",
   "source": "# ==============================================================================\n# NEW APPROACH: Count tweets by GPE/FAC mentions ONLY (ignore lat/lon)\n# ==============================================================================\n\nfrom fuzzywuzzy import fuzz, process\nimport re\n\ndef preprocess_place_name(name):\n    \"\"\"Standardize place names for matching\"\"\"\n    if pd.isna(name) or name == 'NAN' or name == '':\n        return None\n    name = str(name).upper().strip()\n    name = re.sub(r'\\bST\\.?\\b', 'SAINT', name)\n    name = re.sub(r'\\bMT\\.?\\b', 'MOUNT', name)\n    name = re.sub(r'\\bFT\\.?\\b', 'FORT', name)\n    name = re.sub(r'[^\\w\\s]', '', name)\n    name = re.sub(r'\\s+', ' ', name)\n    return name.strip()\n\ndef parse_gpe_entities(gpe_string):\n    \"\"\"Split GPE field into individual place mentions\"\"\"\n    if not gpe_string or pd.isna(gpe_string) or str(gpe_string).strip() == '':\n        return []\n    gpe_string = str(gpe_string).strip()\n    entities = []\n    for part in [p.strip() for p in gpe_string.split(',')]:\n        if not part:\n            continue\n        for sub in re.split(r'[;&|]', part):\n            sub = preprocess_place_name(sub)\n            if sub and len(sub) > 1:\n                entities.append(sub)\n    # Remove duplicates while preserving order\n    seen, clean = set(), []\n    for e in entities:\n        if e not in seen:\n            clean.append(e)\n            seen.add(e)\n    return clean\n\ndef create_lookup_dictionaries(states_gdf, counties_gdf, cities_gdf):\n    \"\"\"Build name->geometry lookup dictionaries\"\"\"\n    print(\"Building lookup dictionaries...\")\n    \n    # States\n    state_lookup = {}\n    state_abbrev_to_name = {}\n    for _, row in states_gdf.iterrows():\n        name = preprocess_place_name(row['NAME'])\n        if name:\n            state_lookup[name] = row\n        if 'STUSPS' in row:\n            abbr = str(row['STUSPS']).upper()\n            state_abbrev_to_name[abbr] = name\n            state_lookup[abbr] = row\n    \n    # Counties\n    county_lookup = {}\n    for _, row in counties_gdf.iterrows():\n        name = preprocess_place_name(row['NAME'])\n        if name:\n            county_lookup[name] = row\n    \n    # Cities\n    city_lookup = {}\n    for _, row in cities_gdf.iterrows():\n        name = preprocess_place_name(row['name'])\n        if name:\n            city_lookup[name] = row\n    \n    print(f\"  States: {len(state_lookup)}\")\n    print(f\"  Counties: {len(county_lookup)}\")\n    print(f\"  Cities: {len(city_lookup)}\")\n    \n    return state_lookup, county_lookup, city_lookup, state_abbrev_to_name\n\ndef fuzzy_match_entity(entity, lookup_dict, threshold=85):\n    \"\"\"Fuzzy match entity to lookup dictionary\"\"\"\n    if entity in lookup_dict:\n        return lookup_dict[entity], 100\n    \n    names = list(lookup_dict.keys())\n    if not names:\n        return None, 0\n    \n    match = process.extractOne(entity, names, scorer=fuzz.ratio)\n    if match and match[1] >= threshold:\n        return lookup_dict[match[0]], match[1]\n    \n    return None, 0\n\ndef count_mentions_in_tweets(tweets_gdf, state_lookup, county_lookup, city_lookup):\n    \"\"\"\n    Count tweets by what they MENTION, not where they are located.\n    \n    Rules:\n    - If tweet mentions \"Texas\" → Texas state gets +1\n    - If tweet mentions \"Houston\" → Houston city gets +1\n    - If tweet mentions \"Harris County\" → Harris County gets +1\n    - All mentions in a single tweet count independently\n    \"\"\"\n    print(\"\\nCounting tweet mentions...\")\n    \n    state_mentions = {}\n    county_mentions = {}\n    city_mentions = {}\n    \n    for idx, row in tweets_gdf.iterrows():\n        if idx % 100 == 0:\n            print(f\"  Processing tweet {idx}/{len(tweets_gdf)}\")\n        \n        # Parse GPE field\n        entities = parse_gpe_entities(row['GPE'])\n        \n        for entity in entities:\n            # Try to match to state\n            state_match, state_score = fuzzy_match_entity(entity, state_lookup, threshold=90)\n            if state_match is not None:\n                state_code = state_match['STUSPS']\n                state_mentions[state_code] = state_mentions.get(state_code, 0) + 1\n                continue\n            \n            # Try to match to county\n            county_match, county_score = fuzzy_match_entity(entity, county_lookup, threshold=85)\n            if county_match is not None:\n                county_id = county_match['GEOID']\n                county_mentions[county_id] = county_mentions.get(county_id, 0) + 1\n                continue\n            \n            # Try to match to city\n            city_match, city_score = fuzzy_match_entity(entity, city_lookup, threshold=85)\n            if city_match is not None:\n                city_id = city_match['geonameid']\n                city_mentions[city_id] = city_mentions.get(city_id, 0) + 1\n    \n    print(f\"\\n  Found mentions:\")\n    print(f\"    States: {len(state_mentions)}\")\n    print(f\"    Counties: {len(county_mentions)}\")\n    print(f\"    Cities: {len(city_mentions)}\")\n    \n    return state_mentions, county_mentions, city_mentions\n\ndef create_count_gdfs(state_mentions, county_mentions, city_mentions, \n                      us_states_gdf, us_counties_gdf, us_cities_gdf):\n    \"\"\"Create GeoDataFrames with mention counts\"\"\"\n    \n    # States\n    state_counts_df = pd.DataFrame([\n        {'STUSPS': k, 'tweet_count': v} \n        for k, v in state_mentions.items()\n    ])\n    states_with_counts = us_states_gdf.merge(\n        state_counts_df, on='STUSPS', how='left'\n    )\n    states_with_counts['tweet_count'] = states_with_counts['tweet_count'].fillna(0)\n    \n    # Counties\n    county_counts_df = pd.DataFrame([\n        {'GEOID': k, 'tweet_count': v} \n        for k, v in county_mentions.items()\n    ])\n    counties_with_counts = us_counties_gdf.merge(\n        county_counts_df, on='GEOID', how='left'\n    )\n    counties_with_counts['tweet_count'] = counties_with_counts['tweet_count'].fillna(0)\n    \n    # Cities\n    city_counts_df = pd.DataFrame([\n        {'geonameid': k, 'tweet_count': v} \n        for k, v in city_mentions.items()\n    ])\n    cities_with_counts = us_cities_gdf.merge(\n        city_counts_df, on='geonameid', how='left'\n    )\n    cities_with_counts['tweet_count'] = cities_with_counts['tweet_count'].fillna(0)\n    \n    return states_with_counts, counties_with_counts, cities_with_counts\n\n# Execute the new approach\nstate_lookup, county_lookup, city_lookup, state_abbrev_to_name = create_lookup_dictionaries(\n    us_states_gdf, us_counties_gdf, us_cities_gdf\n)\n\nstate_mentions, county_mentions, city_mentions = count_mentions_in_tweets(\n    tweets_gdf, state_lookup, county_lookup, city_lookup\n)\n\nstates_with_counts, counties_with_counts, cities_with_counts = create_count_gdfs(\n    state_mentions, county_mentions, city_mentions,\n    us_states_gdf, us_counties_gdf, us_cities_gdf\n)\n\nprint(\"\\nTop states by mentions:\")\nprint(states_with_counts[states_with_counts['tweet_count'] > 0][['NAME', 'STUSPS', 'tweet_count']].sort_values('tweet_count', ascending=False).head(10))\n\nprint(\"\\nTop counties by mentions:\")\nprint(counties_with_counts[counties_with_counts['tweet_count'] > 0][['NAME', 'GEOID', 'tweet_count']].sort_values('tweet_count', ascending=False).head(10))\n\nprint(\"\\nTop cities by mentions:\")\nprint(cities_with_counts[cities_with_counts['tweet_count'] > 0][['name', 'geonameid', 'tweet_count']].sort_values('tweet_count', ascending=False).head(10))",
   "id": "9219b041917b9c86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building lookup dictionaries...\n",
      "  States: 104\n",
      "  Counties: 1915\n",
      "  Cities: 12256\n",
      "\n",
      "Counting tweet mentions...\n",
      "  Processing tweet 0/3007\n",
      "  Processing tweet 100/3007\n",
      "  Processing tweet 200/3007\n",
      "  Processing tweet 300/3007\n",
      "  Processing tweet 400/3007\n",
      "  Processing tweet 500/3007\n",
      "  Processing tweet 600/3007\n",
      "  Processing tweet 700/3007\n",
      "  Processing tweet 800/3007\n",
      "  Processing tweet 900/3007\n",
      "  Processing tweet 1000/3007\n",
      "  Processing tweet 1100/3007\n",
      "  Processing tweet 1200/3007\n",
      "  Processing tweet 1300/3007\n",
      "  Processing tweet 1400/3007\n",
      "  Processing tweet 1500/3007\n",
      "  Processing tweet 1600/3007\n",
      "  Processing tweet 1700/3007\n",
      "  Processing tweet 1800/3007\n",
      "  Processing tweet 1900/3007\n",
      "  Processing tweet 2000/3007\n",
      "  Processing tweet 2100/3007\n",
      "  Processing tweet 2200/3007\n",
      "  Processing tweet 2300/3007\n",
      "  Processing tweet 2400/3007\n",
      "  Processing tweet 2500/3007\n",
      "  Processing tweet 2600/3007\n",
      "  Processing tweet 2700/3007\n",
      "  Processing tweet 2800/3007\n",
      "  Processing tweet 2900/3007\n",
      "  Processing tweet 3000/3007\n",
      "\n",
      "  Found mentions:\n",
      "    States: 16\n",
      "    Counties: 47\n",
      "    Cities: 108\n",
      "\n",
      "Top states by mentions:\n",
      "              NAME STUSPS  tweet_count\n",
      "12         Florida     FL       2156.0\n",
      "3          Georgia     GA        369.0\n",
      "30  North Carolina     NC         69.0\n",
      "35  South Carolina     SC         56.0\n",
      "8        Tennessee     TN         18.0\n",
      "21            Ohio     OH         18.0\n",
      "7         Virginia     VA         17.0\n",
      "38         Alabama     AL         17.0\n",
      "2         Kentucky     KY         10.0\n",
      "1       California     CA          2.0\n",
      "\n",
      "Top counties by mentions:\n",
      "            NAME  GEOID  tweet_count\n",
      "2713        Tama  19171        120.0\n",
      "3082       Perry  21193         86.0\n",
      "2499       Miami  39109         15.0\n",
      "1096    Sarasota  12115         14.0\n",
      "2431  Clearwater  27029         11.0\n",
      "360     Carolina  72031         11.0\n",
      "3197     Raleigh  54081          9.0\n",
      "1422       Macon  29121          6.0\n",
      "822    Charlotte  51037          6.0\n",
      "2235     Augusta  51015          6.0\n",
      "\n",
      "Top cities by mentions:\n",
      "                   name  geonameid  tweet_count\n",
      "1193        Tallahassee    4174715        135.0\n",
      "8817            Atlanta    4984500         79.0\n",
      "1131     St. Petersburg    4171563         21.0\n",
      "1130   Saint Pete Beach    4171522         18.0\n",
      "3721          Asheville    4453066         15.0\n",
      "766    Fort Myers Beach    4155996         15.0\n",
      "765          Fort Myers    4155995         11.0\n",
      "1031            Orlando    4167147         11.0\n",
      "1644           Valdosta    4228147         11.0\n",
      "15538      Jacksonville    5733409          9.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:01:32.244543Z",
     "start_time": "2025-11-03T21:01:32.237227Z"
    }
   },
   "cell_type": "code",
   "source": "def count_mentions_in_tweets_temporal(tweets_gdf, state_lookup, county_lookup, city_lookup):\n    \"\"\"\n    Count mentions by time bin.\n    Returns dictionaries: {time_bin: {entity_id: count}}\n    \"\"\"\n    print(\"\\nCounting tweet mentions by time bin...\")\n    \n    # Add time binning\n    tweets_gdf['time'] = pd.to_datetime(tweets_gdf['time'])\n    tweets_gdf['bin'] = tweets_gdf['time'].dt.floor('4h')\n    \n    time_bins = sorted(tweets_gdf['bin'].unique())\n    \n    # Initialize dictionaries for each time bin\n    temporal_state_mentions = {tb: {} for tb in time_bins}\n    temporal_county_mentions = {tb: {} for tb in time_bins}\n    temporal_city_mentions = {tb: {} for tb in time_bins}\n    \n    for idx, row in tweets_gdf.iterrows():\n        if idx % 100 == 0:\n            print(f\"  Processing tweet {idx}/{len(tweets_gdf)}\")\n        \n        time_bin = row['bin']\n        entities = parse_gpe_entities(row['GPE'])\n        \n        for entity in entities:\n            # Try state match\n            state_match, state_score = fuzzy_match_entity(entity, state_lookup, threshold=90)\n            if state_match is not None:\n                state_code = state_match['STUSPS']\n                temporal_state_mentions[time_bin][state_code] = temporal_state_mentions[time_bin].get(state_code, 0) + 1\n                continue\n            \n            # Try county match\n            county_match, county_score = fuzzy_match_entity(entity, county_lookup, threshold=85)\n            if county_match is not None:\n                county_id = county_match['GEOID']\n                temporal_county_mentions[time_bin][county_id] = temporal_county_mentions[time_bin].get(county_id, 0) + 1\n                continue\n            \n            # Try city match\n            city_match, city_score = fuzzy_match_entity(entity, city_lookup, threshold=85)\n            if city_match is not None:\n                city_id = city_match['geonameid']\n                temporal_city_mentions[time_bin][city_id] = temporal_city_mentions[time_bin].get(city_id, 0) + 1\n    \n    return time_bins, temporal_state_mentions, temporal_county_mentions, temporal_city_mentions\n\ndef create_temporal_aggregations(time_bins, temporal_state_mentions, temporal_county_mentions, temporal_city_mentions):\n    \"\"\"Create aggregated counts for each time bin\"\"\"\n    temporal_data = {}\n    \n    for bin_time in time_bins:\n        # Convert mention dictionaries to DataFrames\n        state_counts = pd.DataFrame([\n            {'state_code': k, 'tweet_count': v}\n            for k, v in temporal_state_mentions[bin_time].items()\n        ])\n        \n        county_counts = pd.DataFrame([\n            {'county_fips': k, 'tweet_count': v}\n            for k, v in temporal_county_mentions[bin_time].items()\n        ])\n        \n        city_counts = pd.DataFrame([\n            {'city_id': k, 'tweet_count': v}\n            for k, v in temporal_city_mentions[bin_time].items()\n        ])\n        \n        temporal_data[bin_time] = {\n            'states': state_counts,\n            'counties': county_counts,\n            'cities': city_counts\n        }\n    \n    return temporal_data\n\ndef prepare_timeslider_data_correct(temporal_data, boundary_gdf, join_left, join_right, level_name):\n    \"\"\"\n    Convert geometry to proper GeoJSON format with lists, not tuples\n    \"\"\"\n    timeslider_data = []\n\n    for bin_time, counts_data in temporal_data.items():\n        bin_gdf = boundary_gdf.merge(\n            counts_data[level_name],\n            left_on=join_left,\n            right_on=join_right,\n            how='left'\n        )\n\n        bin_gdf['tweet_count'] = bin_gdf['tweet_count'].fillna(0)\n        bin_gdf = bin_gdf[bin_gdf.geometry.notna()]\n        bin_gdf = bin_gdf[bin_gdf.geometry.is_valid]\n\n        if bin_gdf.crs is None:\n            bin_gdf = bin_gdf.set_crs(\"EPSG:4326\")\n        else:\n            bin_gdf = bin_gdf.to_crs(\"EPSG:4326\")\n\n        geojson_str = bin_gdf.to_json()\n        geojson_dict = json.loads(geojson_str)\n\n        timestamp_str = bin_time.strftime('%Y-%m-%dT%H:%M:%S')\n        for feature in geojson_dict['features']:\n            if feature['properties'] is None:\n                feature['properties'] = {}\n            feature['properties']['time'] = timestamp_str\n            if 'tweet_count' not in feature['properties']:\n                feature['properties']['tweet_count'] = 0\n            feature['properties']['tweet_count'] = float(feature['properties']['tweet_count'])\n\n        timeslider_data.append(geojson_dict)\n\n    return timeslider_data\n\ndef prepare_heatmap_with_time(temporal_data, cities_gdf):\n    \"\"\"Prepare data for HeatMapWithTime\"\"\"\n    heat_data = []\n    time_index = []\n\n    for bin_time, counts_data in temporal_data.items():\n        bin_cities = cities_gdf.merge(\n            counts_data['cities'],\n            left_on='geonameid',\n            right_on='city_id',\n            how='inner'\n        )\n\n        bin_heat_data = []\n        for _, row in bin_cities.iterrows():\n            if pd.notna(row.latitude) and pd.notna(row.longitude) and row.tweet_count > 0:\n                bin_heat_data.append([\n                    float(row.latitude),\n                    float(row.longitude),\n                    float(row.tweet_count)\n                ])\n\n        heat_data.append(bin_heat_data)\n        time_index.append(bin_time.strftime('%Y-%m-%d %H:%M'))\n\n    return heat_data, time_index",
   "id": "ec95ca5c4a822c7f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T21:03:48.697461Z",
     "start_time": "2025-11-03T21:03:35.685044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def folium_process_dynamic(tweets_gdf, us_states_gdf, us_counties_gdf, us_cities_gdf, \n",
    "                           state_lookup, county_lookup, city_lookup):\n",
    "    \"\"\"Process data for dynamic temporal visualization using mention-based counting\"\"\"\n",
    "    \n",
    "    # Count mentions by time bin\n",
    "    time_bins, temporal_state_mentions, temporal_county_mentions, temporal_city_mentions = \\\n",
    "        count_mentions_in_tweets_temporal(tweets_gdf, state_lookup, county_lookup, city_lookup)\n",
    "    \n",
    "    # Create temporal aggregations\n",
    "    temporal_data = create_temporal_aggregations(\n",
    "        time_bins, temporal_state_mentions, temporal_county_mentions, temporal_city_mentions\n",
    "    )\n",
    "    \n",
    "    # Prepare timeslider data for states and counties\n",
    "    state_timeslider_data = prepare_timeslider_data_correct(\n",
    "        temporal_data,\n",
    "        us_states_gdf,\n",
    "        'STUSPS',\n",
    "        'state_code',\n",
    "        'states'\n",
    "    )\n",
    "    \n",
    "    county_timeslider_data = prepare_timeslider_data_correct(\n",
    "        temporal_data,\n",
    "        us_counties_gdf,\n",
    "        'GEOID',\n",
    "        'county_fips',\n",
    "        'counties'\n",
    "    )\n",
    "    \n",
    "    # Prepare heatmap data for cities\n",
    "    city_heat_data, time_labels = prepare_heatmap_with_time(temporal_data, us_cities_gdf)\n",
    "    \n",
    "    return state_timeslider_data, county_timeslider_data, city_heat_data, time_labels\n",
    "\n",
    "# Execute the temporal processing\n",
    "state_timeslider_data, county_timeslider_data, city_heat_data, time_labels = folium_process_dynamic(\n",
    "    tweets_gdf, us_states_gdf, us_counties_gdf, us_cities_gdf,\n",
    "    state_lookup, county_lookup, city_lookup\n",
    ")\n",
    "\n",
    "print(f\"\\nTemporal data prepared:\")\n",
    "print(f\"  Time bins: {len(state_timeslider_data)}\")\n",
    "print(f\"  State timeslider features: {len(state_timeslider_data)}\")\n",
    "print(f\"  County timeslider features: {len(county_timeslider_data)}\")\n",
    "print(f\"  City heatmap time steps: {len(city_heat_data)}\")\n",
    "print(state_timeslider_data[:10])"
   ],
   "id": "bd54dc16cf8c21e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counting tweet mentions by time bin...\n",
      "  Processing tweet 0/3007\n",
      "  Processing tweet 100/3007\n",
      "  Processing tweet 200/3007\n",
      "  Processing tweet 300/3007\n",
      "  Processing tweet 400/3007\n",
      "  Processing tweet 500/3007\n",
      "  Processing tweet 600/3007\n",
      "  Processing tweet 700/3007\n",
      "  Processing tweet 800/3007\n",
      "  Processing tweet 900/3007\n",
      "  Processing tweet 1000/3007\n",
      "  Processing tweet 1100/3007\n",
      "  Processing tweet 1200/3007\n",
      "  Processing tweet 1300/3007\n",
      "  Processing tweet 1400/3007\n",
      "  Processing tweet 1500/3007\n",
      "  Processing tweet 1600/3007\n",
      "  Processing tweet 1700/3007\n",
      "  Processing tweet 1800/3007\n",
      "  Processing tweet 1900/3007\n",
      "  Processing tweet 2000/3007\n",
      "  Processing tweet 2100/3007\n",
      "  Processing tweet 2200/3007\n",
      "  Processing tweet 2300/3007\n",
      "  Processing tweet 2400/3007\n",
      "  Processing tweet 2500/3007\n",
      "  Processing tweet 2600/3007\n",
      "  Processing tweet 2700/3007\n",
      "  Processing tweet 2800/3007\n",
      "  Processing tweet 2900/3007\n",
      "  Processing tweet 3000/3007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "88f2e7d9074677e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
