{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-03T22:03:25.219610Z",
     "start_time": "2025-11-03T22:03:25.218245Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T22:03:26.663878Z",
     "start_time": "2025-11-03T22:03:25.222177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import folium\n",
    "from click.formatting import iter_rows\n",
    "from folium import plugins\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import folium\n",
    "from folium import plugins\n",
    "import json\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "\n",
    "def get_project_root():\n",
    "    # This assumes the notebook is running from within the project's root folder.\n",
    "    return os.getcwd()\n",
    "\n",
    "def get_data_file_path(*path_segments):\n",
    "    project_root = get_project_root()\n",
    "    # Navigate to the data folder from the project root.\n",
    "    return os.path.join(project_root, *path_segments)\n",
    "\n",
    "# Data loading functions\n",
    "def get_geojson():\n",
    "    \"\"\"Get path to helene.geojson\"\"\"\n",
    "    geojson = get_data_file_path('data', 'geojson', 'helene.geojson')\n",
    "    print(geojson)\n",
    "    return gpd.read_file(geojson)\n",
    "\n",
    "def get_cities():\n",
    "    df_path = get_data_file_path('data', 'tables', 'cities1000.csv')\n",
    "    # df = pd.read_csv(df_path)\n",
    "    df = pd.read_csv(df_path, low_memory=False)\n",
    "\n",
    "    us_cities_df = df[\n",
    "        (df['country_code'] == 'US') &\n",
    "        (df['feature_class'] == 'P') &\n",
    "        (df['population'].notna()) &\n",
    "        (df['latitude'].notna()) &\n",
    "        (df['longitude'].notna())\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    us_cities_gdf = gpd.GeoDataFrame(\n",
    "        us_cities_df,\n",
    "        geometry=gpd.points_from_xy(us_cities_df.longitude, us_cities_df.latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    return us_cities_gdf\n",
    "\n",
    "def get_states():\n",
    "    gdf_path = get_data_file_path('data', 'shape_files', \"cb_2023_us_state_20m.shp\")\n",
    "    return gpd.read_file(gdf_path)\n",
    "\n",
    "def get_counties():\n",
    "    gdf_path = get_data_file_path('data', 'shape_files', \"cb_2023_us_county_20m.shp\")\n",
    "    return gpd.read_file(gdf_path)\n",
    "\n",
    "tweets_gdf = get_geojson().to_crs(\"EPSG:4326\")\n",
    "us_cities_gdf = get_cities().to_crs(\"EPSG:4326\")\n",
    "us_states_gdf = get_states().to_crs(\"EPSG:4326\")\n",
    "us_counties_gdf = get_counties().to_crs(\"EPSG:4326\")"
   ],
   "id": "e17b2216ad9d0d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\data\\geojson\\helene.geojson\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T22:03:37.397854Z",
     "start_time": "2025-11-03T22:03:26.718129Z"
    }
   },
   "cell_type": "code",
   "source": "# ==============================================================================\n# NEW APPROACH: Count tweets by GPE/FAC mentions ONLY (ignore lat/lon)\n# ==============================================================================\n\nfrom fuzzywuzzy import fuzz, process\nimport re\n\ndef preprocess_place_name(name):\n    \"\"\"Standardize place names for matching\"\"\"\n    if pd.isna(name) or name == 'NAN' or name == '':\n        return None\n    name = str(name).upper().strip()\n    name = re.sub(r'\\bST\\.?\\b', 'SAINT', name)\n    name = re.sub(r'\\bMT\\.?\\b', 'MOUNT', name)\n    name = re.sub(r'\\bFT\\.?\\b', 'FORT', name)\n    name = re.sub(r'[^\\w\\s]', '', name)\n    name = re.sub(r'\\s+', ' ', name)\n    return name.strip()\n\ndef parse_gpe_entities(gpe_string):\n    \"\"\"Split GPE field into individual place mentions\"\"\"\n    if not gpe_string or pd.isna(gpe_string) or str(gpe_string).strip() == '':\n        return []\n    gpe_string = str(gpe_string).strip()\n    entities = []\n    for part in [p.strip() for p in gpe_string.split(',')]:\n        if not part:\n            continue\n        for sub in re.split(r'[;&|]', part):\n            sub = preprocess_place_name(sub)\n            if sub and len(sub) > 1:\n                entities.append(sub)\n    # Remove duplicates while preserving order\n    seen, clean = set(), []\n    for e in entities:\n        if e not in seen:\n            clean.append(e)\n            seen.add(e)\n    return clean\n\ndef create_lookup_dictionaries(states_gdf, counties_gdf, cities_gdf):\n    \"\"\"Build name->geometry lookup dictionaries\"\"\"\n    print(\"Building lookup dictionaries...\")\n    \n    # States\n    state_lookup = {}\n    state_abbrev_to_name = {}\n    for _, row in states_gdf.iterrows():\n        name = preprocess_place_name(row['NAME'])\n        if name:\n            state_lookup[name] = row\n        if 'STUSPS' in row:\n            abbr = str(row['STUSPS']).upper()\n            state_abbrev_to_name[abbr] = name\n            state_lookup[abbr] = row\n    \n    # Counties\n    county_lookup = {}\n    for _, row in counties_gdf.iterrows():\n        name = preprocess_place_name(row['NAME'])\n        if name:\n            county_lookup[name] = row\n    \n    # Cities\n    city_lookup = {}\n    for _, row in cities_gdf.iterrows():\n        name = preprocess_place_name(row['name'])\n        if name:\n            city_lookup[name] = row\n    \n    print(f\"  States: {len(state_lookup)}\")\n    print(f\"  Counties: {len(county_lookup)}\")\n    print(f\"  Cities: {len(city_lookup)}\")\n    \n    return state_lookup, county_lookup, city_lookup, state_abbrev_to_name\n\ndef fuzzy_match_entity(entity, lookup_dict, threshold=85):\n    \"\"\"Fuzzy match entity to lookup dictionary\"\"\"\n    if entity in lookup_dict:\n        return lookup_dict[entity], 100\n    \n    names = list(lookup_dict.keys())\n    if not names:\n        return None, 0\n    \n    match = process.extractOne(entity, names, scorer=fuzz.ratio)\n    if match and match[1] >= threshold:\n        return lookup_dict[match[0]], match[1]\n    \n    return None, 0\n\ndef count_mentions_in_tweets(tweets_gdf, state_lookup, county_lookup, city_lookup):\n    \"\"\"\n    Count tweets by what they MENTION, not where they are located.\n    Also track which tweets mentioned each entity for detailed output.\n    \n    Rules:\n    - If tweet mentions \"Texas\" → Texas state gets +1\n    - If tweet mentions \"Houston\" → Houston city gets +1\n    - If tweet mentions \"Harris County\" → Harris County gets +1\n    - All mentions in a single tweet count independently\n    \"\"\"\n    print(\"\\nCounting tweet mentions...\")\n    \n    state_mentions = {}\n    county_mentions = {}\n    city_mentions = {}\n    \n    # Track tweet details for each entity (especially for cities)\n    state_tweet_details = {}\n    county_tweet_details = {}\n    city_tweet_details = {}\n    \n    for idx, row in tweets_gdf.iterrows():\n        if idx % 100 == 0:\n            print(f\"  Processing tweet {idx}/{len(tweets_gdf)}\")\n        \n        # Parse GPE field\n        entities = parse_gpe_entities(row['GPE'])\n        original_gpe = str(row['GPE']) if pd.notna(row['GPE']) else ''\n        tweet_time = str(row['time']) if pd.notna(row['time']) else ''\n        \n        for entity in entities:\n            # Try to match to state\n            state_match, state_score = fuzzy_match_entity(entity, state_lookup, threshold=90)\n            if state_match is not None:\n                state_code = state_match['STUSPS']\n                state_mentions[state_code] = state_mentions.get(state_code, 0) + 1\n                \n                # Track tweet details\n                if state_code not in state_tweet_details:\n                    state_tweet_details[state_code] = []\n                state_tweet_details[state_code].append({\n                    'original_gpe': original_gpe,\n                    'matched_entity': entity,\n                    'time': tweet_time\n                })\n                continue\n            \n            # Try to match to county\n            county_match, county_score = fuzzy_match_entity(entity, county_lookup, threshold=85)\n            if county_match is not None:\n                county_id = county_match['GEOID']\n                county_mentions[county_id] = county_mentions.get(county_id, 0) + 1\n                \n                # Track tweet details\n                if county_id not in county_tweet_details:\n                    county_tweet_details[county_id] = []\n                county_tweet_details[county_id].append({\n                    'original_gpe': original_gpe,\n                    'matched_entity': entity,\n                    'time': tweet_time\n                })\n                continue\n            \n            # Try to match to city\n            city_match, city_score = fuzzy_match_entity(entity, city_lookup, threshold=85)\n            if city_match is not None:\n                city_id = city_match['geonameid']\n                city_mentions[city_id] = city_mentions.get(city_id, 0) + 1\n                \n                # Track tweet details\n                if city_id not in city_tweet_details:\n                    city_tweet_details[city_id] = []\n                city_tweet_details[city_id].append({\n                    'original_gpe': original_gpe,\n                    'matched_entity': entity,\n                    'time': tweet_time\n                })\n    \n    print(f\"\\n  Found mentions:\")\n    print(f\"    States: {len(state_mentions)}\")\n    print(f\"    Counties: {len(county_mentions)}\")\n    print(f\"    Cities: {len(city_mentions)}\")\n    \n    return (state_mentions, county_mentions, city_mentions,\n            state_tweet_details, county_tweet_details, city_tweet_details)\n\ndef create_count_gdfs(state_mentions, county_mentions, city_mentions,\n                      state_tweet_details, county_tweet_details, city_tweet_details,\n                      us_states_gdf, us_counties_gdf, us_cities_gdf):\n    \"\"\"Create GeoDataFrames with mention counts and tweet details\"\"\"\n    \n    # States\n    state_counts_df = pd.DataFrame([\n        {\n            'STUSPS': k, \n            'tweet_count': v,\n            'sample_mentions': '; '.join([d['matched_entity'] for d in state_tweet_details[k][:5]]),\n            'sample_gpe_text': '; '.join([d['original_gpe'][:100] for d in state_tweet_details[k][:3]])\n        } \n        for k, v in state_mentions.items()\n    ])\n    states_with_counts = us_states_gdf.merge(\n        state_counts_df, on='STUSPS', how='left'\n    )\n    states_with_counts['tweet_count'] = states_with_counts['tweet_count'].fillna(0)\n    \n    # Counties\n    county_counts_df = pd.DataFrame([\n        {\n            'GEOID': k, \n            'tweet_count': v,\n            'sample_mentions': '; '.join([d['matched_entity'] for d in county_tweet_details[k][:5]]),\n            'sample_gpe_text': '; '.join([d['original_gpe'][:100] for d in county_tweet_details[k][:3]])\n        } \n        for k, v in county_mentions.items()\n    ])\n    counties_with_counts = us_counties_gdf.merge(\n        county_counts_df, on='GEOID', how='left'\n    )\n    counties_with_counts['tweet_count'] = counties_with_counts['tweet_count'].fillna(0)\n    \n    # Cities - with full tweet text details\n    city_counts_df = pd.DataFrame([\n        {\n            'geonameid': k, \n            'tweet_count': v,\n            'matched_entities': '; '.join([d['matched_entity'] for d in city_tweet_details[k]]),\n            'original_gpe_text': ' | '.join([d['original_gpe'] for d in city_tweet_details[k]]),\n            'mention_times': '; '.join([d['time'] for d in city_tweet_details[k][:10]])\n        } \n        for k, v in city_mentions.items()\n    ])\n    cities_with_counts = us_cities_gdf.merge(\n        city_counts_df, on='geonameid', how='left'\n    )\n    cities_with_counts['tweet_count'] = cities_with_counts['tweet_count'].fillna(0)\n    \n    return states_with_counts, counties_with_counts, cities_with_counts\n\n# Execute the new approach\nstate_lookup, county_lookup, city_lookup, state_abbrev_to_name = create_lookup_dictionaries(\n    us_states_gdf, us_counties_gdf, us_cities_gdf\n)\n\n(state_mentions, county_mentions, city_mentions,\n state_tweet_details, county_tweet_details, city_tweet_details) = count_mentions_in_tweets(\n    tweets_gdf, state_lookup, county_lookup, city_lookup\n)\n\nstates_with_counts, counties_with_counts, cities_with_counts = create_count_gdfs(\n    state_mentions, county_mentions, city_mentions,\n    state_tweet_details, county_tweet_details, city_tweet_details,\n    us_states_gdf, us_counties_gdf, us_cities_gdf\n)\n\nprint(\"\\nTop states by mentions:\")\nprint(states_with_counts[states_with_counts['tweet_count'] > 0][['NAME', 'STUSPS', 'tweet_count', 'sample_mentions']].sort_values('tweet_count', ascending=False).head(10))\n\nprint(\"\\nTop counties by mentions:\")\nprint(counties_with_counts[counties_with_counts['tweet_count'] > 0][['NAME', 'GEOID', 'tweet_count', 'sample_mentions']].sort_values('tweet_count', ascending=False).head(10))\n\nprint(\"\\nTop cities by mentions:\")\nprint(cities_with_counts[cities_with_counts['tweet_count'] > 0][['name', 'geonameid', 'tweet_count', 'matched_entities']].sort_values('tweet_count', ascending=False).head(10))",
   "id": "9219b041917b9c86",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building lookup dictionaries...\n",
      "  States: 104\n",
      "  Counties: 1915\n",
      "  Cities: 12256\n",
      "\n",
      "Counting tweet mentions...\n",
      "  Processing tweet 0/3007\n",
      "  Processing tweet 100/3007\n",
      "  Processing tweet 200/3007\n",
      "  Processing tweet 300/3007\n",
      "  Processing tweet 400/3007\n",
      "  Processing tweet 500/3007\n",
      "  Processing tweet 600/3007\n",
      "  Processing tweet 700/3007\n",
      "  Processing tweet 800/3007\n",
      "  Processing tweet 900/3007\n",
      "  Processing tweet 1000/3007\n",
      "  Processing tweet 1100/3007\n",
      "  Processing tweet 1200/3007\n",
      "  Processing tweet 1300/3007\n",
      "  Processing tweet 1400/3007\n",
      "  Processing tweet 1500/3007\n",
      "  Processing tweet 1600/3007\n",
      "  Processing tweet 1700/3007\n",
      "  Processing tweet 1800/3007\n",
      "  Processing tweet 1900/3007\n",
      "  Processing tweet 2000/3007\n",
      "  Processing tweet 2100/3007\n",
      "  Processing tweet 2200/3007\n",
      "  Processing tweet 2300/3007\n",
      "  Processing tweet 2400/3007\n",
      "  Processing tweet 2500/3007\n",
      "  Processing tweet 2600/3007\n",
      "  Processing tweet 2700/3007\n",
      "  Processing tweet 2800/3007\n",
      "  Processing tweet 2900/3007\n",
      "  Processing tweet 3000/3007\n",
      "\n",
      "  Found mentions:\n",
      "    States: 16\n",
      "    Counties: 47\n",
      "    Cities: 108\n",
      "\n",
      "Top states by mentions:\n",
      "              NAME STUSPS  tweet_count  \\\n",
      "12         Florida     FL       2156.0   \n",
      "3          Georgia     GA        369.0   \n",
      "30  North Carolina     NC         69.0   \n",
      "35  South Carolina     SC         56.0   \n",
      "8        Tennessee     TN         18.0   \n",
      "21            Ohio     OH         18.0   \n",
      "7         Virginia     VA         17.0   \n",
      "38         Alabama     AL         17.0   \n",
      "2         Kentucky     KY         10.0   \n",
      "1       California     CA          2.0   \n",
      "\n",
      "                                      sample_mentions  \n",
      "12        FLORIDA; FLORIDA; FLORIDA; FLORIDA; FLORIDA  \n",
      "3              GEORGIA; GEORGIA; GA; GEORGIA; GEORGIA  \n",
      "30         NC; NC; NC; NORTH CAROLINA; NORTH CAROLINA  \n",
      "35  SC; SOUTH CAROLINA; SOUTH CAROLINA; SOUTH CARO...  \n",
      "8   TENNESSEE; TENNESSEE; TENNESSEE; TENNESSEE; TE...  \n",
      "21                       OHIO; OHIO; OHIO; OHIO; OHIO  \n",
      "7          VIRGINIA; VIRGINIA; VIRGINIA; VA; VIRGINIA  \n",
      "38        ALABAMA; ALABAMA; ALABAMA; ALABAMA; ALABAMA  \n",
      "2    KENTUCKY; KENTUCKY; KENTUCKY; KENTUCKY; KENTUCKY  \n",
      "1                              CALIFORNIA; CALIFORNIA  \n",
      "\n",
      "Top counties by mentions:\n",
      "            NAME  GEOID  tweet_count  \\\n",
      "2713        Tama  19171        120.0   \n",
      "3082       Perry  21193         86.0   \n",
      "2499       Miami  39109         15.0   \n",
      "1096    Sarasota  12115         14.0   \n",
      "2431  Clearwater  27029         11.0   \n",
      "360     Carolina  72031         11.0   \n",
      "3197     Raleigh  54081          9.0   \n",
      "1422       Macon  29121          6.0   \n",
      "822    Charlotte  51037          6.0   \n",
      "2235     Augusta  51015          6.0   \n",
      "\n",
      "                                        sample_mentions  \n",
      "2713                  TAMPA; TAMPA; TAMPA; TAMPA; TAMPA  \n",
      "3082                  PERRY; PERRY; PERRY; PERRY; PERRY  \n",
      "2499                  MIAMI; MIAMI; MIAMI; MIAMI; MIAMI  \n",
      "1096   SARASOTA; SARASOTA; SARASOTA; SARASOTA; SARASOTA  \n",
      "2431  CLEARWATER; CLEARWATER; CLEARWATER; CLEARWATER...  \n",
      "360   CAROLINA; CAROLINAS; CAROLINAS; CAROLINAS; CAR...  \n",
      "3197        RALEIGH; RALEIGH; RALEIGH; RALEIGH; RALEIGH  \n",
      "1422                  MACON; MACON; MACON; MACON; MACON  \n",
      "822   CHARLOTTE; CHARLOTTE; CHARLOTTE; CHARLOTTE; CH...  \n",
      "2235        AUGUSTA; AUGUSTA; AUGUSTA; AUGUSTA; AUGUSTA  \n",
      "\n",
      "Top cities by mentions:\n",
      "                   name  geonameid  tweet_count  \\\n",
      "1193        Tallahassee    4174715        135.0   \n",
      "8817            Atlanta    4984500         79.0   \n",
      "1131     St. Petersburg    4171563         21.0   \n",
      "1130   Saint Pete Beach    4171522         18.0   \n",
      "3721          Asheville    4453066         15.0   \n",
      "766    Fort Myers Beach    4155996         15.0   \n",
      "765          Fort Myers    4155995         11.0   \n",
      "1031            Orlando    4167147         11.0   \n",
      "1644           Valdosta    4228147         11.0   \n",
      "15538      Jacksonville    5733409          9.0   \n",
      "\n",
      "                                        matched_entities  \n",
      "1193   TALLAHASSEE; TALLAHASSEE; TALLAHASSEE; TALLAHA...  \n",
      "8817   ATLANTA; ATLANTA; ATLANTA; ATLANTA; ATLANTA; A...  \n",
      "1131   SAINT PETERSBURG; SAINT PETERSBURG; SAINT PETE...  \n",
      "1130   SAINT PETE BEACH; SAINT PETE BEACH; SAINT PETE...  \n",
      "3721   ASHEVILLE; ASHEVILLE NC; ASHEVILLE; ASHEVILLE;...  \n",
      "766    FORT MYERS BEACH; FORT MYERS BEACH; FORT MYERS...  \n",
      "765    FORT MYERS; FORT MYERS; FORT MYERS; FORT MYERS...  \n",
      "1031   ORLANDO; ORLANDO; ORLANDO; ORLANDO; ORLANDO; O...  \n",
      "1644   VALDOSTA; VALDOSTA; VALDOSTA; VALDOSTA; VALDOS...  \n",
      "15538  JACKSONVILLE; JACKSONVILLE; JACKSONVILLE; JACK...  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T22:03:47.665297Z",
     "start_time": "2025-11-03T22:03:37.424082Z"
    }
   },
   "cell_type": "code",
   "source": "def count_mentions_in_tweets_temporal(tweets_gdf, state_lookup, county_lookup, city_lookup):\n    \"\"\"\n    Count mentions by time bin.\n    Returns dictionaries: {time_bin: {entity_id: count}}\n    \"\"\"\n    print(\"\\nCounting tweet mentions by time bin...\")\n    \n    # Add time binning\n    tweets_gdf['time'] = pd.to_datetime(tweets_gdf['time'])\n    tweets_gdf['bin'] = tweets_gdf['time'].dt.floor('4h')\n    \n    time_bins = sorted(tweets_gdf['bin'].unique())\n    \n    # Initialize dictionaries for each time bin\n    temporal_state_mentions = {tb: {} for tb in time_bins}\n    temporal_county_mentions = {tb: {} for tb in time_bins}\n    temporal_city_mentions = {tb: {} for tb in time_bins}\n    \n    # Track tweet details for each entity at each time bin\n    temporal_state_details = {tb: {} for tb in time_bins}\n    temporal_county_details = {tb: {} for tb in time_bins}\n    temporal_city_details = {tb: {} for tb in time_bins}\n    \n    for idx, row in tweets_gdf.iterrows():\n        if idx % 100 == 0:\n            print(f\"  Processing tweet {idx}/{len(tweets_gdf)}\")\n        \n        time_bin = row['bin']\n        entities = parse_gpe_entities(row['GPE'])\n        original_gpe = str(row['GPE']) if pd.notna(row['GPE']) else ''\n        tweet_time = str(row['time']) if pd.notna(row['time']) else ''\n        \n        for entity in entities:\n            # Try state match\n            state_match, state_score = fuzzy_match_entity(entity, state_lookup, threshold=90)\n            if state_match is not None:\n                state_code = state_match['STUSPS']\n                temporal_state_mentions[time_bin][state_code] = temporal_state_mentions[time_bin].get(state_code, 0) + 1\n                \n                # Track details\n                if state_code not in temporal_state_details[time_bin]:\n                    temporal_state_details[time_bin][state_code] = []\n                temporal_state_details[time_bin][state_code].append({\n                    'original_gpe': original_gpe,\n                    'matched_entity': entity,\n                    'time': tweet_time\n                })\n                continue\n            \n            # Try county match\n            county_match, county_score = fuzzy_match_entity(entity, county_lookup, threshold=85)\n            if county_match is not None:\n                county_id = county_match['GEOID']\n                temporal_county_mentions[time_bin][county_id] = temporal_county_mentions[time_bin].get(county_id, 0) + 1\n                \n                # Track details\n                if county_id not in temporal_county_details[time_bin]:\n                    temporal_county_details[time_bin][county_id] = []\n                temporal_county_details[time_bin][county_id].append({\n                    'original_gpe': original_gpe,\n                    'matched_entity': entity,\n                    'time': tweet_time\n                })\n                continue\n            \n            # Try city match\n            city_match, city_score = fuzzy_match_entity(entity, city_lookup, threshold=85)\n            if city_match is not None:\n                city_id = city_match['geonameid']\n                temporal_city_mentions[time_bin][city_id] = temporal_city_mentions[time_bin].get(city_id, 0) + 1\n                \n                # Track details\n                if city_id not in temporal_city_details[time_bin]:\n                    temporal_city_details[time_bin][city_id] = []\n                temporal_city_details[time_bin][city_id].append({\n                    'original_gpe': original_gpe,\n                    'matched_entity': entity,\n                    'time': tweet_time\n                })\n    \n    return (time_bins, temporal_state_mentions, temporal_county_mentions, temporal_city_mentions,\n            temporal_state_details, temporal_county_details, temporal_city_details)\n\ndef create_temporal_aggregations(time_bins, temporal_state_mentions, temporal_county_mentions, \n                                 temporal_city_mentions, temporal_state_details, temporal_county_details, \n                                 temporal_city_details):\n    \"\"\"Create aggregated counts for each time bin with tweet details\"\"\"\n    temporal_data = {}\n    \n    for bin_time in time_bins:\n        # Convert mention dictionaries to DataFrames with details\n        state_counts = pd.DataFrame([\n            {\n                'state_code': k, \n                'tweet_count': v,\n                'sample_gpe_text': ' | '.join([d['original_gpe'][:100] for d in temporal_state_details[bin_time][k][:3]])\n            }\n            for k, v in temporal_state_mentions[bin_time].items()\n        ])\n        \n        county_counts = pd.DataFrame([\n            {\n                'county_fips': k, \n                'tweet_count': v,\n                'sample_gpe_text': ' | '.join([d['original_gpe'][:100] for d in temporal_county_details[bin_time][k][:3]])\n            }\n            for k, v in temporal_county_mentions[bin_time].items()\n        ])\n        \n        city_counts = pd.DataFrame([\n            {\n                'city_id': k, \n                'tweet_count': v,\n                'original_gpe_text': ' | '.join([d['original_gpe'] for d in temporal_city_details[bin_time][k]]),\n                'matched_entities': '; '.join([d['matched_entity'] for d in temporal_city_details[bin_time][k]])\n            }\n            for k, v in temporal_city_mentions[bin_time].items()\n        ])\n        \n        temporal_data[bin_time] = {\n            'states': state_counts,\n            'counties': county_counts,\n            'cities': city_counts\n        }\n    \n    return temporal_data\n\ndef prepare_timeslider_data_correct(temporal_data, boundary_gdf, join_left, join_right, level_name):\n    \"\"\"\n    Convert geometry to proper GeoJSON format with lists, not tuples\n    \"\"\"\n    timeslider_data = []\n\n    for bin_time, counts_data in temporal_data.items():\n        bin_gdf = boundary_gdf.merge(\n            counts_data[level_name],\n            left_on=join_left,\n            right_on=join_right,\n            how='left'\n        )\n\n        bin_gdf['tweet_count'] = bin_gdf['tweet_count'].fillna(0)\n        bin_gdf = bin_gdf[bin_gdf.geometry.notna()]\n        bin_gdf = bin_gdf[bin_gdf.geometry.is_valid]\n\n        if bin_gdf.crs is None:\n            bin_gdf = bin_gdf.set_crs(\"EPSG:4326\")\n        else:\n            bin_gdf = bin_gdf.to_crs(\"EPSG:4326\")\n\n        geojson_str = bin_gdf.to_json()\n        geojson_dict = json.loads(geojson_str)\n\n        timestamp_str = bin_time.strftime('%Y-%m-%dT%H:%M:%S')\n        for feature in geojson_dict['features']:\n            if feature['properties'] is None:\n                feature['properties'] = {}\n            feature['properties']['time'] = timestamp_str\n            if 'tweet_count' not in feature['properties']:\n                feature['properties']['tweet_count'] = 0\n            feature['properties']['tweet_count'] = float(feature['properties']['tweet_count'])\n\n        timeslider_data.append(geojson_dict)\n\n    return timeslider_data\n\ndef prepare_heatmap_with_time(temporal_data, cities_gdf):\n    \"\"\"Prepare data for HeatMapWithTime\"\"\"\n    heat_data = []\n    time_index = []\n\n    for bin_time, counts_data in temporal_data.items():\n        bin_cities = cities_gdf.merge(\n            counts_data['cities'],\n            left_on='geonameid',\n            right_on='city_id',\n            how='inner'\n        )\n\n        bin_heat_data = []\n        for _, row in bin_cities.iterrows():\n            if pd.notna(row.latitude) and pd.notna(row.longitude) and row.tweet_count > 0:\n                bin_heat_data.append([\n                    float(row.latitude),\n                    float(row.longitude),\n                    float(row.tweet_count)\n                ])\n\n        heat_data.append(bin_heat_data)\n        time_index.append(bin_time.strftime('%Y-%m-%d %H:%M'))\n\n    return heat_data, time_index\n\n# Execute temporal counting\n(time_bins, temporal_state_mentions, temporal_county_mentions, temporal_city_mentions,\n temporal_state_details, temporal_county_details, temporal_city_details) = \\\n    count_mentions_in_tweets_temporal(tweets_gdf, state_lookup, county_lookup, city_lookup)\n\n# Create temporal aggregations\ntemporal_data = create_temporal_aggregations(\n    time_bins, temporal_state_mentions, temporal_county_mentions, temporal_city_mentions,\n    temporal_state_details, temporal_county_details, temporal_city_details\n)\n\nprint(f\"\\nTemporal bins created: {len(time_bins)}\")\nprint(f\"Time range: {time_bins[0]} to {time_bins[-1]}\")",
   "id": "ec95ca5c4a822c7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counting tweet mentions by time bin...\n",
      "  Processing tweet 0/3007\n",
      "  Processing tweet 100/3007\n",
      "  Processing tweet 200/3007\n",
      "  Processing tweet 300/3007\n",
      "  Processing tweet 400/3007\n",
      "  Processing tweet 500/3007\n",
      "  Processing tweet 600/3007\n",
      "  Processing tweet 700/3007\n",
      "  Processing tweet 800/3007\n",
      "  Processing tweet 900/3007\n",
      "  Processing tweet 1000/3007\n",
      "  Processing tweet 1100/3007\n",
      "  Processing tweet 1200/3007\n",
      "  Processing tweet 1300/3007\n",
      "  Processing tweet 1400/3007\n",
      "  Processing tweet 1500/3007\n",
      "  Processing tweet 1600/3007\n",
      "  Processing tweet 1700/3007\n",
      "  Processing tweet 1800/3007\n",
      "  Processing tweet 1900/3007\n",
      "  Processing tweet 2000/3007\n",
      "  Processing tweet 2100/3007\n",
      "  Processing tweet 2200/3007\n",
      "  Processing tweet 2300/3007\n",
      "  Processing tweet 2400/3007\n",
      "  Processing tweet 2500/3007\n",
      "  Processing tweet 2600/3007\n",
      "  Processing tweet 2700/3007\n",
      "  Processing tweet 2800/3007\n",
      "  Processing tweet 2900/3007\n",
      "  Processing tweet 3000/3007\n",
      "\n",
      "Temporal bins created: 11\n",
      "Time range: 2024-09-26 00:00:00+00:00 to 2024-09-27 16:00:00+00:00\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T22:03:48.871349Z",
     "start_time": "2025-11-03T22:03:47.693167Z"
    }
   },
   "cell_type": "code",
   "source": "# ==============================================================================\n# EXPORT TEMPORAL (4-HOUR BINNED) DATA TO ARCGIS\n# Both INCREMENTAL (per bin) and CUMULATIVE (running total) counts\n# ==============================================================================\n\ndef export_temporal_to_arcgis(temporal_data, time_bins, us_states_gdf, us_counties_gdf, us_cities_gdf,\n                               output_dir='arcgis_outputs'):\n    \"\"\"\n    Export temporal (4-hour binned) data for states, counties, and cities.\n    Creates BOTH incremental and cumulative count files.\n\n    - Incremental: Count for just that 4-hour bin\n    - Cumulative: Running total up to and including that bin (persists even if bin has 0 new mentions)\n    \"\"\"\n    import os\n\n    # Create temporal output directories\n    temporal_dir = os.path.join(output_dir, 'temporal_4hour_bins')\n    incremental_dir = os.path.join(temporal_dir, 'incremental')\n    cumulative_dir = os.path.join(temporal_dir, 'cumulative')\n\n    os.makedirs(incremental_dir, exist_ok=True)\n    os.makedirs(cumulative_dir, exist_ok=True)\n\n    print(f\"\\n{'='*60}\")\n    print(\"EXPORTING TEMPORAL DATA - INCREMENTAL & CUMULATIVE\")\n    print(\"=\"*60)\n    print(f\"\\nTime bins: {len(time_bins)}\")\n    print(f\"Output directory: {temporal_dir}\")\n\n    # Master lists for both incremental and cumulative\n    all_states_incremental = []\n    all_counties_incremental = []\n    all_cities_incremental = []\n\n    all_states_cumulative = []\n    all_counties_cumulative = []\n    all_cities_cumulative = []\n\n    # Cumulative tracking dictionaries\n    cumulative_state_counts = {}\n    cumulative_county_counts = {}\n    cumulative_city_counts = {}\n\n    # Track all entities that have ever been mentioned\n    all_mentioned_states = set()\n    all_mentioned_counties = set()\n    all_mentioned_cities = set()\n\n    # First pass: collect all entities that ever get mentioned\n    for bin_time in time_bins:\n        counts_data = temporal_data[bin_time]\n        if len(counts_data['states']) > 0:\n            all_mentioned_states.update(counts_data['states']['state_code'].values)\n        if len(counts_data['counties']) > 0:\n            all_mentioned_counties.update(counts_data['counties']['county_fips'].values)\n        if len(counts_data['cities']) > 0:\n            all_mentioned_cities.update(counts_data['cities']['city_id'].values)\n\n    print(f\"\\nEntities ever mentioned:\")\n    print(f\"  States: {len(all_mentioned_states)}\")\n    print(f\"  Counties: {len(all_mentioned_counties)}\")\n    print(f\"  Cities: {len(all_mentioned_cities)}\")\n\n    # Process each time bin\n    for idx, bin_time in enumerate(time_bins):\n        bin_str = bin_time.strftime('%Y%m%d_%H%M')\n        bin_label = bin_time.strftime('%Y-%m-%d %H:%M:%S')\n\n        print(f\"\\n  Processing time bin {idx+1}/{len(time_bins)}: {bin_label}\")\n\n        counts_data = temporal_data[bin_time]\n\n        # === STATES ===\n        # Update cumulative counts for states with new mentions\n        if len(counts_data['states']) > 0:\n            for _, row in counts_data['states'].iterrows():\n                state_code = row['state_code']\n                cumulative_state_counts[state_code] = cumulative_state_counts.get(state_code, 0) + row['tweet_count']\n\n        # INCREMENTAL: Only states with mentions in THIS bin\n        if len(counts_data['states']) > 0:\n            states_inc = us_states_gdf.merge(counts_data['states'], left_on='STUSPS', right_on='state_code', how='inner')\n            states_inc['time_bin'] = bin_label\n            states_inc['bin_start'] = bin_time\n            states_inc['count_type'] = 'incremental'\n\n            # Save individual shapefile\n            states_shp = states_inc[['NAME', 'STUSPS', 'tweet_count', 'sample_gpe_text', 'time_bin', 'geometry']].copy()\n            states_shp.columns = ['state_name', 'state_code', 'tweet_cnt', 'smpl_gpe', 'time_bin', 'geometry']\n            states_shp.to_file(os.path.join(incremental_dir, f'states_inc_{bin_str}.shp'))\n\n            all_states_incremental.append(states_inc[['NAME', 'STUSPS', 'tweet_count', 'sample_gpe_text', 'time_bin', 'bin_start', 'geometry']])\n            print(f\"    States incremental: {len(states_inc)} features\")\n\n        # CUMULATIVE: ALL states that have ever been mentioned (even if 0 this bin)\n        cumulative_states_data = pd.DataFrame([\n            {'state_code': code, 'cumulative_count': count}\n            for code, count in cumulative_state_counts.items()\n        ])\n\n        states_cum = us_states_gdf.merge(cumulative_states_data, left_on='STUSPS', right_on='state_code', how='inner')\n        states_cum['time_bin'] = bin_label\n        states_cum['bin_start'] = bin_time\n        states_cum['count_type'] = 'cumulative'\n\n        # Save individual shapefile\n        states_cum_shp = states_cum[['NAME', 'STUSPS', 'cumulative_count', 'time_bin', 'geometry']].copy()\n        states_cum_shp.columns = ['state_name', 'state_code', 'cumul_cnt', 'time_bin', 'geometry']\n        states_cum_shp.to_file(os.path.join(cumulative_dir, f'states_cum_{bin_str}.shp'))\n\n        all_states_cumulative.append(states_cum[['NAME', 'STUSPS', 'cumulative_count', 'time_bin', 'bin_start', 'geometry']])\n        print(f\"    States cumulative: {len(states_cum)} features (total mentions so far)\")\n\n        # === COUNTIES ===\n        # Update cumulative counts\n        if len(counts_data['counties']) > 0:\n            for _, row in counts_data['counties'].iterrows():\n                county_id = row['county_fips']\n                cumulative_county_counts[county_id] = cumulative_county_counts.get(county_id, 0) + row['tweet_count']\n\n        # INCREMENTAL\n        if len(counts_data['counties']) > 0:\n            counties_inc = us_counties_gdf.merge(counts_data['counties'], left_on='GEOID', right_on='county_fips', how='inner')\n            counties_inc['time_bin'] = bin_label\n            counties_inc['bin_start'] = bin_time\n            counties_inc['count_type'] = 'incremental'\n\n            counties_shp = counties_inc[['NAME', 'GEOID', 'STATEFP', 'tweet_count', 'sample_gpe_text', 'time_bin', 'geometry']].copy()\n            counties_shp.columns = ['cnty_name', 'cnty_id', 'state_fp', 'tweet_cnt', 'smpl_gpe', 'time_bin', 'geometry']\n            counties_shp.to_file(os.path.join(incremental_dir, f'counties_inc_{bin_str}.shp'))\n\n            all_counties_incremental.append(counties_inc[['NAME', 'GEOID', 'STATEFP', 'tweet_count', 'sample_gpe_text', 'time_bin', 'bin_start', 'geometry']])\n            print(f\"    Counties incremental: {len(counties_inc)} features\")\n\n        # CUMULATIVE\n        cumulative_counties_data = pd.DataFrame([\n            {'county_fips': code, 'cumulative_count': count}\n            for code, count in cumulative_county_counts.items()\n        ])\n\n        counties_cum = us_counties_gdf.merge(cumulative_counties_data, left_on='GEOID', right_on='county_fips', how='inner')\n        counties_cum['time_bin'] = bin_label\n        counties_cum['bin_start'] = bin_time\n        counties_cum['count_type'] = 'cumulative'\n\n        counties_cum_shp = counties_cum[['NAME', 'GEOID', 'STATEFP', 'cumulative_count', 'time_bin', 'geometry']].copy()\n        counties_cum_shp.columns = ['cnty_name', 'cnty_id', 'state_fp', 'cumul_cnt', 'time_bin', 'geometry']\n        counties_cum_shp.to_file(os.path.join(cumulative_dir, f'counties_cum_{bin_str}.shp'))\n\n        all_counties_cumulative.append(counties_cum[['NAME', 'GEOID', 'STATEFP', 'cumulative_count', 'time_bin', 'bin_start', 'geometry']])\n        print(f\"    Counties cumulative: {len(counties_cum)} features\")\n\n        # === CITIES ===\n        # Update cumulative counts\n        if len(counts_data['cities']) > 0:\n            for _, row in counts_data['cities'].iterrows():\n                city_id = row['city_id']\n                cumulative_city_counts[city_id] = cumulative_city_counts.get(city_id, 0) + row['tweet_count']\n\n        # INCREMENTAL\n        if len(counts_data['cities']) > 0:\n            cities_inc = us_cities_gdf.merge(counts_data['cities'], left_on='geonameid', right_on='city_id', how='inner')\n            cities_inc['time_bin'] = bin_label\n            cities_inc['bin_start'] = bin_time\n            cities_inc['count_type'] = 'incremental'\n\n            cities_shp = cities_inc[['name', 'geonameid', 'population', 'tweet_count', 'matched_entities', 'original_gpe_text', 'time_bin', 'geometry']].copy()\n            cities_shp['orig_gpe'] = cities_shp['original_gpe_text'].str[:254]\n            cities_shp['mtch_ent'] = cities_shp['matched_entities'].str[:254]\n            cities_shp = cities_shp[['name', 'geonameid', 'population', 'tweet_count', 'mtch_ent', 'orig_gpe', 'time_bin', 'geometry']].copy()\n            cities_shp.columns = ['city_name', 'city_id', 'population', 'tweet_cnt', 'mtchd_ent', 'orig_gpe', 'time_bin', 'geometry']\n            cities_shp.to_file(os.path.join(incremental_dir, f'cities_inc_{bin_str}.shp'))\n\n            all_cities_incremental.append(cities_inc[['name', 'geonameid', 'latitude', 'longitude', 'population', 'tweet_count', 'matched_entities', 'original_gpe_text', 'time_bin', 'bin_start', 'geometry']])\n            print(f\"    Cities incremental: {len(cities_inc)} features\")\n\n        # CUMULATIVE\n        cumulative_cities_data = pd.DataFrame([\n            {'city_id': code, 'cumulative_count': count}\n            for code, count in cumulative_city_counts.items()\n        ])\n\n        cities_cum = us_cities_gdf.merge(cumulative_cities_data, left_on='geonameid', right_on='city_id', how='inner')\n        cities_cum['time_bin'] = bin_label\n        cities_cum['bin_start'] = bin_time\n        cities_cum['count_type'] = 'cumulative'\n\n        cities_cum_shp = cities_cum[['name', 'geonameid', 'population', 'cumulative_count', 'time_bin', 'geometry']].copy()\n        cities_cum_shp.columns = ['city_name', 'city_id', 'population', 'cumul_cnt', 'time_bin', 'geometry']\n        cities_cum_shp.to_file(os.path.join(cumulative_dir, f'cities_cum_{bin_str}.shp'))\n\n        all_cities_cumulative.append(cities_cum[['name', 'geonameid', 'latitude', 'longitude', 'population', 'cumulative_count', 'time_bin', 'bin_start', 'geometry']])\n        print(f\"    Cities cumulative: {len(cities_cum)} features\")\n\n    # === CREATE MASTER FILES ===\n    print(f\"\\n  Creating master files...\")\n\n    # INCREMENTAL MASTERS\n    if all_states_incremental:\n        states_inc_master = gpd.GeoDataFrame(pd.concat(all_states_incremental, ignore_index=True))\n        states_inc_master.to_file(os.path.join(incremental_dir, 'states_INCREMENTAL_ALL.shp'))\n        states_inc_master.to_file(os.path.join(incremental_dir, 'states_INCREMENTAL_ALL.geojson'), driver='GeoJSON')\n        print(f\"    ✓ States incremental master: {len(states_inc_master)} records\")\n\n    if all_counties_incremental:\n        counties_inc_master = gpd.GeoDataFrame(pd.concat(all_counties_incremental, ignore_index=True))\n        counties_inc_master.to_file(os.path.join(incremental_dir, 'counties_INCREMENTAL_ALL.shp'))\n        counties_inc_master.to_file(os.path.join(incremental_dir, 'counties_INCREMENTAL_ALL.geojson'), driver='GeoJSON')\n        print(f\"    ✓ Counties incremental master: {len(counties_inc_master)} records\")\n\n    if all_cities_incremental:\n        cities_inc_master = gpd.GeoDataFrame(pd.concat(all_cities_incremental, ignore_index=True))\n        cities_inc_master_shp = cities_inc_master[['name', 'geonameid', 'population', 'tweet_count', 'matched_entities', 'original_gpe_text', 'time_bin', 'bin_start', 'geometry']].copy()\n        cities_inc_master_shp['orig_gpe'] = cities_inc_master_shp['original_gpe_text'].str[:254]\n        cities_inc_master_shp['mtch_ent'] = cities_inc_master_shp['matched_entities'].str[:254]\n        cities_inc_master_shp = cities_inc_master_shp[['name', 'geonameid', 'population', 'tweet_count', 'mtch_ent', 'orig_gpe', 'time_bin', 'bin_start', 'geometry']].copy()\n        cities_inc_master_shp.to_file(os.path.join(incremental_dir, 'cities_INCREMENTAL_ALL.shp'))\n        cities_inc_master.to_file(os.path.join(incremental_dir, 'cities_INCREMENTAL_ALL.geojson'), driver='GeoJSON')\n        cities_inc_master[['name', 'geonameid', 'population', 'latitude', 'longitude', 'tweet_count', 'matched_entities', 'original_gpe_text', 'time_bin', 'bin_start']].to_csv(\n            os.path.join(incremental_dir, 'cities_INCREMENTAL_ALL.csv'), index=False)\n        print(f\"    ✓ Cities incremental master: {len(cities_inc_master)} records\")\n\n    # CUMULATIVE MASTERS\n    if all_states_cumulative:\n        states_cum_master = gpd.GeoDataFrame(pd.concat(all_states_cumulative, ignore_index=True))\n        states_cum_master.to_file(os.path.join(cumulative_dir, 'states_CUMULATIVE_ALL.shp'))\n        states_cum_master.to_file(os.path.join(cumulative_dir, 'states_CUMULATIVE_ALL.geojson'), driver='GeoJSON')\n        print(f\"    ✓ States cumulative master: {len(states_cum_master)} records\")\n\n    if all_counties_cumulative:\n        counties_cum_master = gpd.GeoDataFrame(pd.concat(all_counties_cumulative, ignore_index=True))\n        counties_cum_master.to_file(os.path.join(cumulative_dir, 'counties_CUMULATIVE_ALL.shp'))\n        counties_cum_master.to_file(os.path.join(cumulative_dir, 'counties_CUMULATIVE_ALL.geojson'), driver='GeoJSON')\n        print(f\"    ✓ Counties cumulative master: {len(counties_cum_master)} records\")\n\n    if all_cities_cumulative:\n        cities_cum_master = gpd.GeoDataFrame(pd.concat(all_cities_cumulative, ignore_index=True))\n        cities_cum_master.to_file(os.path.join(cumulative_dir, 'cities_CUMULATIVE_ALL.shp'))\n        cities_cum_master.to_file(os.path.join(cumulative_dir, 'cities_CUMULATIVE_ALL.geojson'), driver='GeoJSON')\n        cities_cum_master[['name', 'geonameid', 'population', 'latitude', 'longitude', 'cumulative_count', 'time_bin', 'bin_start']].to_csv(\n            os.path.join(cumulative_dir, 'cities_CUMULATIVE_ALL.csv'), index=False)\n        print(f\"    ✓ Cities cumulative master: {len(cities_cum_master)} records\")\n\n    print(f\"\\n{'='*60}\")\n    print(\"TEMPORAL EXPORT COMPLETE!\")\n    print(\"=\"*60)\n    print(f\"\\nFiles saved to: {os.path.abspath(temporal_dir)}\")\n    print(f\"\\nOutput structure:\")\n    print(f\"  incremental/ - Counts for just that 4-hour bin\")\n    print(f\"  cumulative/  - Running total (persists even if bin has 0 new mentions)\")\n    print(f\"\\nTo use in ArcGIS Pro:\")\n    print(f\"  1. Add *_INCREMENTAL_ALL.shp or *_CUMULATIVE_ALL.shp\")\n    print(f\"  2. Enable time using 'bin_start' field\")\n    print(f\"  3. Set time step to 4 hours\")\n\n# Execute temporal export\nexport_temporal_to_arcgis(\n    temporal_data, time_bins, \n    us_states_gdf, us_counties_gdf, us_cities_gdf\n)\n\nprint(\"\\n\\nSummary:\")\nprint(f\"Total time bins: {len(time_bins)}\")\nprint(f\"Time range: {time_bins[0].strftime('%Y-%m-%d %H:%M:%S')} to {time_bins[-1].strftime('%Y-%m-%d %H:%M:%S')}\")",
   "id": "bd54dc16cf8c21e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPORTING TEMPORAL DATA - INCREMENTAL & CUMULATIVE\n",
      "============================================================\n",
      "\n",
      "Time bins: 11\n",
      "Output directory: arcgis_outputs\\temporal_4hour_bins\n",
      "\n",
      "Entities ever mentioned:\n",
      "  States: 16\n",
      "  Counties: 47\n",
      "  Cities: 108\n",
      "\n",
      "  Processing time bin 1/11: 2024-09-26 00:00:00\n",
      "    States incremental: 7 features\n",
      "    States cumulative: 7 features (total mentions so far)\n",
      "    Counties incremental: 5 features\n",
      "    Counties cumulative: 5 features\n",
      "    Cities incremental: 8 features\n",
      "    Cities cumulative: 8 features\n",
      "\n",
      "  Processing time bin 2/11: 2024-09-26 04:00:00\n",
      "    States incremental: 7 features\n",
      "    States cumulative: 9 features (total mentions so far)\n",
      "    Counties incremental: 9 features\n",
      "    Counties cumulative: 13 features\n",
      "    Cities incremental: 16 features\n",
      "    Cities cumulative: 21 features\n",
      "\n",
      "  Processing time bin 3/11: 2024-09-26 08:00:00\n",
      "    States incremental: 8 features\n",
      "    States cumulative: 11 features (total mentions so far)\n",
      "    Counties incremental: 5 features\n",
      "    Counties cumulative: 15 features\n",
      "    Cities incremental: 12 features\n",
      "    Cities cumulative: 28 features\n",
      "\n",
      "  Processing time bin 4/11: 2024-09-26 12:00:00\n",
      "    States incremental: 10 features\n",
      "    States cumulative: 13 features (total mentions so far)\n",
      "    Counties incremental: 16 features\n",
      "    Counties cumulative: 26 features\n",
      "    Cities incremental: 12 features\n",
      "    Cities cumulative: 34 features\n",
      "\n",
      "  Processing time bin 5/11: 2024-09-26 16:00:00\n",
      "    States incremental: 9 features\n",
      "    States cumulative: 14 features (total mentions so far)\n",
      "    Counties incremental: 9 features\n",
      "    Counties cumulative: 28 features\n",
      "    Cities incremental: 25 features\n",
      "    Cities cumulative: 49 features\n",
      "\n",
      "  Processing time bin 6/11: 2024-09-26 20:00:00\n",
      "    States incremental: 7 features\n",
      "    States cumulative: 15 features (total mentions so far)\n",
      "    Counties incremental: 13 features\n",
      "    Counties cumulative: 33 features\n",
      "    Cities incremental: 26 features\n",
      "    Cities cumulative: 61 features\n",
      "\n",
      "  Processing time bin 7/11: 2024-09-27 00:00:00\n",
      "    States incremental: 7 features\n",
      "    States cumulative: 15 features (total mentions so far)\n",
      "    Counties incremental: 8 features\n",
      "    Counties cumulative: 34 features\n",
      "    Cities incremental: 28 features\n",
      "    Cities cumulative: 74 features\n",
      "\n",
      "  Processing time bin 8/11: 2024-09-27 04:00:00\n",
      "    States incremental: 8 features\n",
      "    States cumulative: 16 features (total mentions so far)\n",
      "    Counties incremental: 11 features\n",
      "    Counties cumulative: 38 features\n",
      "    Cities incremental: 23 features\n",
      "    Cities cumulative: 85 features\n",
      "\n",
      "  Processing time bin 9/11: 2024-09-27 08:00:00\n",
      "    States incremental: 8 features\n",
      "    States cumulative: 16 features (total mentions so far)\n",
      "    Counties incremental: 10 features\n",
      "    Counties cumulative: 40 features\n",
      "    Cities incremental: 20 features\n",
      "    Cities cumulative: 88 features\n",
      "\n",
      "  Processing time bin 10/11: 2024-09-27 12:00:00\n",
      "    States incremental: 9 features\n",
      "    States cumulative: 16 features (total mentions so far)\n",
      "    Counties incremental: 11 features\n",
      "    Counties cumulative: 42 features\n",
      "    Cities incremental: 27 features\n",
      "    Cities cumulative: 96 features\n",
      "\n",
      "  Processing time bin 11/11: 2024-09-27 16:00:00\n",
      "    States incremental: 7 features\n",
      "    States cumulative: 16 features (total mentions so far)\n",
      "    Counties incremental: 15 features\n",
      "    Counties cumulative: 47 features\n",
      "    Cities incremental: 24 features\n",
      "    Cities cumulative: 108 features\n",
      "\n",
      "  Creating master files...\n",
      "    ✓ States incremental master: 87 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\AppData\\Local\\Temp\\ipykernel_57348\\3187880310.py:202: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  states_inc_master.to_file(os.path.join(incremental_dir, 'states_INCREMENTAL_ALL.shp'))\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'tweet_count' to 'tweet_coun'\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'sample_gpe_text' to 'sample_gpe'\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field bin_start create as date field, though DateTime requested.\n",
      "  ogr_write(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Counties incremental master: 112 records\n",
      "    ✓ Cities incremental master: 221 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\AppData\\Local\\Temp\\ipykernel_57348\\3187880310.py:208: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  counties_inc_master.to_file(os.path.join(incremental_dir, 'counties_INCREMENTAL_ALL.shp'))\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'tweet_count' to 'tweet_coun'\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'sample_gpe_text' to 'sample_gpe'\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field bin_start create as date field, though DateTime requested.\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\AppData\\Local\\Temp\\ipykernel_57348\\3187880310.py:218: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  cities_inc_master_shp.to_file(os.path.join(incremental_dir, 'cities_INCREMENTAL_ALL.shp'))\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'tweet_count' to 'tweet_coun'\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field bin_start create as date field, though DateTime requested.\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\AppData\\Local\\Temp\\ipykernel_57348\\3187880310.py:227: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  states_cum_master.to_file(os.path.join(cumulative_dir, 'states_CUMULATIVE_ALL.shp'))\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'cumulative_count' to 'cumulative'\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field bin_start create as date field, though DateTime requested.\n",
      "  ogr_write(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ States cumulative master: 148 records\n",
      "    ✓ Counties cumulative master: 321 records\n",
      "    ✓ Cities cumulative master: 652 records\n",
      "\n",
      "============================================================\n",
      "TEMPORAL EXPORT COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Files saved to: C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\arcgis_outputs\\temporal_4hour_bins\n",
      "\n",
      "Output structure:\n",
      "  incremental/ - Counts for just that 4-hour bin\n",
      "  cumulative/  - Running total (persists even if bin has 0 new mentions)\n",
      "\n",
      "To use in ArcGIS Pro:\n",
      "  1. Add *_INCREMENTAL_ALL.shp or *_CUMULATIVE_ALL.shp\n",
      "  2. Enable time using 'bin_start' field\n",
      "  3. Set time step to 4 hours\n",
      "\n",
      "\n",
      "Summary:\n",
      "Total time bins: 11\n",
      "Time range: 2024-09-26 00:00:00 to 2024-09-27 16:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colto\\AppData\\Local\\Temp\\ipykernel_57348\\3187880310.py:233: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  counties_cum_master.to_file(os.path.join(cumulative_dir, 'counties_CUMULATIVE_ALL.shp'))\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'cumulative_count' to 'cumulative'\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field bin_start create as date field, though DateTime requested.\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\AppData\\Local\\Temp\\ipykernel_57348\\3187880310.py:239: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  cities_cum_master.to_file(os.path.join(cumulative_dir, 'cities_CUMULATIVE_ALL.shp'))\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Normalized/laundered field name: 'cumulative_count' to 'cumulative'\n",
      "  ogr_write(\n",
      "C:\\Users\\colto\\Documents\\GitHub\\Tweet_project\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:723: RuntimeWarning: Field bin_start create as date field, though DateTime requested.\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T22:05:49.503571Z",
     "start_time": "2025-11-03T22:05:49.498072Z"
    }
   },
   "cell_type": "code",
   "source": "print(90/140)",
   "id": "88f2e7d9074677e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6428571428571429\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "de25190dfaade614"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
